{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Class Text-Vision Analysis with Controlled Visual Properties\n",
    "\n",
    "This notebook runs class discrimination tests using text encodings with controlled visual properties.\n",
    "Distractors are matched for size, color, and texture to ensure the model must rely on class identity.\n",
    "Multiple seeds are run to obtain confidence intervals for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\Textvision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import time\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# SyntheticKonkle paths - Using 224x224 resized images\n",
    "DATA_DIR = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224')\n",
    "RESULTS_DIR = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'Textvision')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup with proper attribute tracking\n",
    "def build_synthetic_dataset():\n",
    "    \"\"\"Load the master labels CSV with all visual properties.\"\"\"\n",
    "    # Use the master_labels.csv which has all the attribute information\n",
    "    master_csv = os.path.join(DATA_DIR, 'master_labels.csv')\n",
    "    \n",
    "    if not os.path.exists(master_csv):\n",
    "        print(f\"Warning: {master_csv} not found, trying alternative path...\")\n",
    "        # Try the original SyntheticKonkle folder\n",
    "        master_csv = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "    \n",
    "    print(f\"Loading master labels from: {master_csv}\")\n",
    "    df = pd.read_csv(master_csv)\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    required_cols = ['folder', 'filename', 'class', 'color', 'size', 'texture']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Clean the data\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images\")\n",
    "    print(f\"Classes: {df['class'].nunique()} unique\")\n",
    "    print(f\"Colors: {df['color'].nunique()} unique\")\n",
    "    print(f\"Sizes: {df['size'].nunique()} unique\")\n",
    "    print(f\"Textures: {df['texture'].nunique()} unique\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "class SyntheticImageDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform):\n",
    "        self.df = df\n",
    "        # For SyntheticKonkle_224, images are in nested structure\n",
    "        self.data_dir = os.path.join(data_dir, 'SyntheticKonkle')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.data_dir, row['folder'], row['filename'])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "        except Exception as e:\n",
    "            # Return a black image if file not found\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    classes = [b[1] for b in batch]\n",
    "    colors = [b[2] for b in batch]\n",
    "    sizes = [b[3] for b in batch]\n",
    "    textures = [b[4] for b in batch]\n",
    "    idxs = [b[5] for b in batch]\n",
    "    return imgs, classes, colors, sizes, textures, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_vision_class_test_per_class(model_name, seed=0, device='cuda' if torch.cuda.is_available() else 'cpu', \n",
    "                                         batch_size=32, trials_per_class=250):\n",
    "    \"\"\"\n",
    "    Run text-vision class test with controlled visual properties.\n",
    "    Returns per-class accuracy results.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Check if model supports text encoding\n",
    "    if model_name in ['resnext', 'dino_s_resnext50']:\n",
    "        print(f\"[WARNING] {model_name} has no text encoder, skipping\")\n",
    "        return {}\n",
    "\n",
    "    # Load model & transform\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    \n",
    "    # Build dataset and extract image embeddings\n",
    "    df = build_synthetic_dataset()\n",
    "    ds = SyntheticImageDataset(df, DATA_DIR, transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(\"Extracting image embeddings...\")\n",
    "    all_img_embs, all_classes, all_colors, all_sizes, all_textures, all_idxs = [], [], [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, classes, colors, sizes, textures, idxs in tqdm(loader, desc=\"Processing images\"):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = extractor.get_img_feature(imgs)\n",
    "            feats = extractor.norm_features(feats).cpu().float()\n",
    "            all_img_embs.append(feats)\n",
    "            all_classes.extend(classes)\n",
    "            all_colors.extend(colors)\n",
    "            all_sizes.extend(sizes)\n",
    "            all_textures.extend(textures)\n",
    "            all_idxs.extend(idxs)\n",
    "    \n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    print(f\"Extracted {len(all_img_embs)} image embeddings\")\n",
    "\n",
    "    # Encode text labels for all unique classes\n",
    "    unique_classes = list(set(all_classes))\n",
    "    print(f\"Encoding {len(unique_classes)} class text labels...\")\n",
    "    \n",
    "    class_text_features = {}\n",
    "    with torch.no_grad():\n",
    "        if \"clip\" in model_name:\n",
    "            # CLIP text encoding\n",
    "            tokens = clip.tokenize(unique_classes, truncate=True).to(device)\n",
    "            txt_features = model.encode_text(tokens)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu().float()  # Convert to float32\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "        else:  # CVCL\n",
    "            # CVCL text encoding with token length\n",
    "            tokens, token_len = model.tokenize(unique_classes)\n",
    "            tokens = tokens.to(device)\n",
    "            if isinstance(token_len, torch.Tensor):\n",
    "                token_len = token_len.to(device)\n",
    "            txt_features = model.encode_text(tokens, token_len)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu().float()  # Convert to float32\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "\n",
    "    # Group images by class and visual attributes\n",
    "    class_attr_idxs = defaultdict(lambda: defaultdict(list))\n",
    "    idx_to_row = {idx: i for i, idx in enumerate(all_idxs)}\n",
    "    \n",
    "    for i, (idx, cls, col, size, texture) in enumerate(zip(all_idxs, all_classes, all_colors, all_sizes, all_textures)):\n",
    "        class_attr_idxs[cls][(col, size, texture)].append(idx)\n",
    "\n",
    "    # Track per-class performance\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    \n",
    "    print(f\"Running {trials_per_class} trials per class...\")\n",
    "    \n",
    "    # Run trials for each class\n",
    "    for target_class in tqdm(unique_classes, desc=f\"Testing {model_name}\"):\n",
    "        trials_done = 0\n",
    "        \n",
    "        # Get text feature for target class (ensure float32)\n",
    "        target_text_feature = class_text_features[target_class].float()\n",
    "        \n",
    "        # Get all attribute combinations for this class\n",
    "        for (color, size, texture), idx_list in class_attr_idxs[target_class].items():\n",
    "            if trials_done >= trials_per_class:\n",
    "                break\n",
    "                \n",
    "            # Find distractors from other classes with SAME visual attributes\n",
    "            other_idxs = []\n",
    "            for other_cls in unique_classes:\n",
    "                if other_cls != target_class:\n",
    "                    if (color, size, texture) in class_attr_idxs[other_cls]:\n",
    "                        other_idxs.extend(class_attr_idxs[other_cls][(color, size, texture)])\n",
    "            \n",
    "            # Need at least 1 query image and 3 distractors with matching attributes\n",
    "            if len(idx_list) >= 1 and len(other_idxs) >= 3:\n",
    "                # Run multiple trials for this attribute combination\n",
    "                n_trials = min(10, trials_per_class - trials_done)\n",
    "                \n",
    "                for _ in range(n_trials):\n",
    "                    # Pick query image from target class\n",
    "                    query_idx = random.choice(idx_list)\n",
    "                    \n",
    "                    # Pick 3 distractors with matching visual properties but different classes\n",
    "                    distractor_idxs = random.sample(other_idxs, min(3, len(other_idxs)))\n",
    "                    \n",
    "                    # Create 4-way choice: query + 3 distractors\n",
    "                    candidates = [query_idx] + distractor_idxs\n",
    "                    \n",
    "                    # Get image features for all candidates (ensure float32)\n",
    "                    cand_features = torch.stack([all_img_embs[idx_to_row[idx]] for idx in candidates]).float()\n",
    "                    \n",
    "                    # Compute similarity with text encoding\n",
    "                    similarities = cand_features @ target_text_feature\n",
    "                    \n",
    "                    # Check if model correctly identifies query (index 0)\n",
    "                    prediction = similarities.argmax().item()\n",
    "                    \n",
    "                    # Update counts\n",
    "                    class_correct[target_class] += int(prediction == 0)\n",
    "                    class_total[target_class] += 1\n",
    "                    trials_done += 1\n",
    "                    \n",
    "                    if trials_done >= trials_per_class:\n",
    "                        break\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for cls in unique_classes:\n",
    "        if class_total[cls] > 0:\n",
    "            class_accuracies[cls] = class_correct[cls] / class_total[cls]\n",
    "        else:\n",
    "            class_accuracies[cls] = 0.0\n",
    "    \n",
    "    # Print summary\n",
    "    overall_correct = sum(class_correct.values())\n",
    "    overall_total = sum(class_total.values())\n",
    "    overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nOverall: {overall_correct}/{overall_total} = {overall_acc:.3f}\")\n",
    "    print(f\"Classes tested: {len([c for c in class_accuracies if class_total[c] > 0])}\")\n",
    "    \n",
    "    return class_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Found 67 unique classes in the dataset\n",
      "\n",
      "Starting Text-Vision evaluation with controlled visual properties:\n",
      "Configuration: 3 seeds × 500 trials/class × 67 classes\n",
      "Key difference: Distractors have MATCHING size, color, and texture\n",
      "This ensures models must use class identity, not visual properties\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing cvcl-resnext with text-vision approach\n",
      "============================================================\n",
      "\n",
      "Seed 1/3 for cvcl-resnext\n",
      "Loading cvcl-resnext...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 247/247 [00:20<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext: 100%|██████████| 67/67 [00:00<00:00, 128.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 8680/33100 = 0.262\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.264\n",
      "  Classes successfully tested: 67\n",
      "  Waiting 30 seconds before next seed...\n",
      "\n",
      "Seed 2/3 for cvcl-resnext\n",
      "Loading cvcl-resnext...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_1.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 247/247 [00:19<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext: 100%|██████████| 67/67 [00:00<00:00, 127.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 8549/33100 = 0.258\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.260\n",
      "  Classes successfully tested: 67\n",
      "  Waiting 30 seconds before next seed...\n",
      "\n",
      "Seed 3/3 for cvcl-resnext\n",
      "Loading cvcl-resnext...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_2.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 247/247 [00:20<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext: 100%|██████████| 67/67 [00:00<00:00, 126.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 8725/33100 = 0.264\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.265\n",
      "  Classes successfully tested: 67\n",
      "\n",
      "============================================================\n",
      "Testing clip-res with text-vision approach\n",
      "============================================================\n",
      "\n",
      "Seed 1/3 for clip-res\n",
      "Loading clip-res...\n",
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/247 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Processing images: 100%|██████████| 247/247 [00:18<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing clip-res: 100%|██████████| 67/67 [00:00<00:00, 117.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 27859/33100 = 0.842\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.843\n",
      "  Classes successfully tested: 67\n",
      "\n",
      "Seed 2/3 for clip-res\n",
      "Loading clip-res...\n",
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 247/247 [00:18<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing clip-res: 100%|██████████| 67/67 [00:00<00:00, 112.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 27994/33100 = 0.846\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.847\n",
      "  Classes successfully tested: 67\n",
      "\n",
      "Seed 3/3 for clip-res\n",
      "Loading clip-res...\n",
      "Loading master labels from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\master_labels.csv\n",
      "Loaded 7881 images\n",
      "Classes: 67 unique\n",
      "Colors: 11 unique\n",
      "Sizes: 4 unique\n",
      "Textures: 2 unique\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 247/247 [00:18<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7881 image embeddings\n",
      "Encoding 67 class text labels...\n",
      "Running 500 trials per class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing clip-res: 100%|██████████| 67/67 [00:00<00:00, 117.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: 27957/33100 = 0.845\n",
      "Classes tested: 67\n",
      "  Mean accuracy across classes: 0.846\n",
      "  Classes successfully tested: 67\n",
      "\n",
      "============================================================\n",
      "TEXT-VISION EVALUATION COMPLETE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run multiple seeds for statistical analysis\n",
    "n_seeds = 3  # Limited seeds due to potential rate limiting\n",
    "trials_per_class = 500  # More trials per seed for robust statistics\n",
    "models_to_test = ['cvcl-resnext', 'clip-res']\n",
    "\n",
    "# Check dataset first\n",
    "test_df = build_synthetic_dataset()\n",
    "n_classes = len(test_df['class'].unique())\n",
    "print(f\"Found {n_classes} unique classes in the dataset\")\n",
    "\n",
    "print(f\"\\nStarting Text-Vision evaluation with controlled visual properties:\")\n",
    "print(f\"Configuration: {n_seeds} seeds × {trials_per_class} trials/class × {n_classes} classes\")\n",
    "print(f\"Key difference: Distractors have MATCHING size, color, and texture\")\n",
    "print(f\"This ensures models must use class identity, not visual properties\\n\")\n",
    "\n",
    "all_results = {model: defaultdict(list) for model in models_to_test}\n",
    "\n",
    "# Run evaluation\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {model_name} with text-vision approach\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        print(f\"\\nSeed {seed+1}/{n_seeds} for {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            class_acc = run_text_vision_class_test_per_class(\n",
    "                model_name, \n",
    "                seed=seed, \n",
    "                trials_per_class=trials_per_class\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            for cls, acc in class_acc.items():\n",
    "                all_results[model_name][cls].append(acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if len(class_acc) > 0:\n",
    "                mean_acc = np.mean(list(class_acc.values()))\n",
    "                print(f\"  Mean accuracy across classes: {mean_acc:.3f}\")\n",
    "                print(f\"  Classes successfully tested: {len(class_acc)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            if \"404\" in str(e) or \"rate\" in str(e).lower():\n",
    "                print(f\"  Rate limit hit - waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                # Retry once\n",
    "                try:\n",
    "                    class_acc = run_text_vision_class_test_per_class(\n",
    "                        model_name, seed=seed, trials_per_class=trials_per_class\n",
    "                    )\n",
    "                    for cls, acc in class_acc.items():\n",
    "                        all_results[model_name][cls].append(acc)\n",
    "                    print(f\"  Retry successful!\")\n",
    "                except:\n",
    "                    print(f\"  Retry failed - skipping seed {seed}\")\n",
    "                    continue\n",
    "        \n",
    "        # Add delay between seeds for CVCL\n",
    "        if 'cvcl' in model_name and seed < n_seeds - 1:\n",
    "            print(\"  Waiting 30 seconds before next seed...\")\n",
    "            time.sleep(30)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_results = {}\n",
    "for model_name in models_to_test:\n",
    "    stats_results[model_name] = {}\n",
    "    for cls, accs in all_results[model_name].items():\n",
    "        if len(accs) > 0:\n",
    "            n_samples = len(accs)\n",
    "            stats_results[model_name][cls] = {\n",
    "                'mean': np.mean(accs),\n",
    "                'std': np.std(accs, ddof=1) if n_samples > 1 else 0,\n",
    "                'se': np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'ci95': 1.96 * np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'n_samples': n_samples,\n",
    "                'total_trials': n_samples * trials_per_class,\n",
    "                'raw': accs\n",
    "            }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEXT-VISION EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved detailed results to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\Textvision\\class_textvision_perclass_results.csv\n",
      "Saved summary statistics to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\Textvision\\class_textvision_perclass_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results\n",
    "detailed_df = []\n",
    "for model_name in models_to_test:\n",
    "    for cls, stats in stats_results[model_name].items():\n",
    "        for seed_idx, acc in enumerate(stats['raw']):\n",
    "            detailed_df.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'seed': seed_idx,\n",
    "                'accuracy': acc,\n",
    "                'n_trials': trials_per_class,\n",
    "                'test_type': 'text_vision_controlled'\n",
    "            })\n",
    "\n",
    "if len(detailed_df) > 0:\n",
    "    detailed_df = pd.DataFrame(detailed_df)\n",
    "    output_path = os.path.join(RESULTS_DIR, 'class_textvision_perclass_results.csv')\n",
    "    detailed_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved detailed results to {output_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary_stats = []\n",
    "    for model_name in models_to_test:\n",
    "        for cls, stats in stats_results[model_name].items():\n",
    "            summary_stats.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'mean_accuracy': stats['mean'],\n",
    "                'std': stats['std'],\n",
    "                'se': stats['se'],\n",
    "                'ci95': stats['ci95'],\n",
    "                'n_seeds': stats['n_samples'],\n",
    "                'total_trials': stats['total_trials']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_path = os.path.join(RESULTS_DIR, 'class_textvision_perclass_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Saved summary statistics to {summary_path}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "if len(stats_results[models_to_test[0]]) > 0:\n",
    "    fig = plt.figure(figsize=(14, 11))\n",
    "    \n",
    "    # Create subplots\n",
    "    ax1 = plt.subplot2grid((20, 1), (0, 0), rowspan=8)\n",
    "    ax2 = plt.subplot2grid((20, 1), (12, 0), rowspan=8)\n",
    "    \n",
    "    # Prepare data\n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    mid_point = len(classes) // 2\n",
    "    classes_first_half = classes[:mid_point]\n",
    "    classes_second_half = classes[mid_point:]\n",
    "    \n",
    "    # Define colors and markers\n",
    "    colors = {\n",
    "        'cvcl-resnext': '#2a9d8f',  # Teal for CVCL\n",
    "        'clip-res': '#e63946'  # Red for CLIP\n",
    "    }\n",
    "    markers = {\n",
    "        'cvcl-resnext': 'o',\n",
    "        'clip-res': 's'\n",
    "    }\n",
    "    avg_line_styles = {\n",
    "        'cvcl-resnext': '--',\n",
    "        'clip-res': '-.'\n",
    "    }\n",
    "    \n",
    "    legend_elements = []\n",
    "    \n",
    "    def plot_on_axis(ax, class_subset, is_first=False):\n",
    "        x_pos = np.arange(len(class_subset))\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            means = [stats_results[model_name][cls]['mean'] * 100 for cls in class_subset]\n",
    "            errors = [stats_results[model_name][cls]['ci95'] * 100 for cls in class_subset]\n",
    "            \n",
    "            ax.errorbar(x_pos, means, yerr=errors,\n",
    "                       label=model_name.upper().replace('-', ' '),\n",
    "                       color=colors[model_name],\n",
    "                       marker=markers[model_name],\n",
    "                       markersize=7,\n",
    "                       linewidth=0,\n",
    "                       capsize=4,\n",
    "                       capthick=1.5,\n",
    "                       alpha=0.9,\n",
    "                       markeredgecolor='black',\n",
    "                       markeredgewidth=0.5)\n",
    "        \n",
    "        # Add chance level\n",
    "        ax.axhline(y=25, color='#ffa500', linestyle=':', alpha=0.8, linewidth=1.5)\n",
    "        \n",
    "        # Calculate overall averages\n",
    "        all_classes_means = {}\n",
    "        for model_name in models_to_test:\n",
    "            all_means = [stats_results[model_name][cls]['mean'] * 100 for cls in classes]\n",
    "            all_classes_means[model_name] = np.mean(all_means)\n",
    "        \n",
    "        # Add average lines\n",
    "        for model_name in models_to_test:\n",
    "            avg_performance = all_classes_means[model_name]\n",
    "            ax.axhline(y=avg_performance,\n",
    "                      color=colors[model_name],\n",
    "                      linestyle=avg_line_styles[model_name],\n",
    "                      alpha=0.7,\n",
    "                      linewidth=2)\n",
    "            \n",
    "            if is_first:\n",
    "                ax.text(len(class_subset) + 0.8, avg_performance,\n",
    "                       f'{avg_performance:.1f}%',\n",
    "                       fontsize=9,\n",
    "                       color=colors[model_name],\n",
    "                       va='center',\n",
    "                       fontweight='bold')\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_ylabel('Text-Vision Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(class_subset, rotation=45, ha='right', fontsize=10)\n",
    "        ax.set_ylim(0, 105)\n",
    "        ax.set_yticks([0, 25, 50, 75, 100])\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.set_facecolor('#fafafa')\n",
    "        \n",
    "        # Create legend elements\n",
    "        global legend_elements\n",
    "        if is_first:\n",
    "            from matplotlib.lines import Line2D\n",
    "            legend_elements = []\n",
    "            \n",
    "            for model_name in models_to_test:\n",
    "                legend_elements.append(\n",
    "                    Line2D([0], [0], marker=markers[model_name], color='w',\n",
    "                          markerfacecolor=colors[model_name], markeredgecolor='black',\n",
    "                          markersize=8, label=model_name.upper().replace('-', ' '))\n",
    "                )\n",
    "            \n",
    "            for model_name in models_to_test:\n",
    "                avg_val = all_classes_means[model_name]\n",
    "                legend_elements.append(\n",
    "                    Line2D([0], [0], color=colors[model_name],\n",
    "                          linestyle=avg_line_styles[model_name], linewidth=2,\n",
    "                          label=f'{model_name.upper().split(\"-\")[0]} Average ({avg_val:.1f}%)')\n",
    "                )\n",
    "            \n",
    "            legend_elements.append(\n",
    "                Line2D([0], [0], color='#ffa500', linestyle=':', linewidth=1.5,\n",
    "                      label='Chance Level (25%)')\n",
    "            )\n",
    "    \n",
    "    # Plot both halves\n",
    "    plot_on_axis(ax1, classes_first_half, is_first=True)\n",
    "    ax1.set_title('Text-Vision Per-Class Performance - Part 1\\nControlled Visual Properties (Size, Color, Texture)',\n",
    "                 fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    plot_on_axis(ax2, classes_second_half, is_first=False)\n",
    "    ax2.set_title('Text-Vision Per-Class Performance - Part 2',\n",
    "                 fontsize=13, fontweight='bold', pad=10)\n",
    "    ax2.set_xlabel('Target Category', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_ax = fig.add_axes([0.125, 0.44, 0.775, 0.08])\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    legend = legend_ax.legend(handles=legend_elements,\n",
    "                             loc='center',\n",
    "                             ncol=3,\n",
    "                             fontsize=10,\n",
    "                             frameon=True,\n",
    "                             fancybox=True,\n",
    "                             shadow=True,\n",
    "                             framealpha=0.95,\n",
    "                             columnspacing=2.5,\n",
    "                             handlelength=3)\n",
    "    \n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('gray')\n",
    "    legend.get_frame().set_linewidth(1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.35)\n",
    "    \n",
    "    # Save plots\n",
    "    png_path = os.path.join(RESULTS_DIR, 'class_textvision_perclass_controlled.png')\n",
    "    pdf_path = os.path.join(RESULTS_DIR, 'class_textvision_perclass_controlled.pdf')\n",
    "    \n",
    "    plt.savefig(png_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(pdf_path, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved plots to:\")\n",
    "    print(f\"  - {png_path}\")\n",
    "    print(f\"  - {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "if len(stats_results) > 0 and len(stats_results[models_to_test[0]]) > 0:\n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    \n",
    "    summary_data = []\n",
    "    for cls in classes:\n",
    "        row = {'Class': cls}\n",
    "        for model in models_to_test:\n",
    "            if cls in stats_results[model]:\n",
    "                stats = stats_results[model][cls]\n",
    "                row[f\"{model}_mean\"] = f\"{stats['mean']:.3f}\"\n",
    "                row[f\"{model}_ci95\"] = f\"±{stats['ci95']:.3f}\"\n",
    "                row[f\"{model}_trials\"] = stats['total_trials']\n",
    "        \n",
    "        # Add difference if both models have results\n",
    "        if cls in stats_results['clip-res'] and cls in stats_results['cvcl-resnext']:\n",
    "            diff = stats_results['clip-res'][cls]['mean'] - stats_results['cvcl-resnext'][cls]['mean']\n",
    "            row['difference'] = f\"{diff:+.3f}\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT-VISION PER-CLASS PERFORMANCE SUMMARY (Controlled Attributes)\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERALL TEXT-VISION PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        all_accs = []\n",
    "        for cls in classes:\n",
    "            if cls in stats_results[model]:\n",
    "                all_accs.extend(stats_results[model][cls]['raw'])\n",
    "        \n",
    "        if len(all_accs) > 0:\n",
    "            mean = np.mean(all_accs)\n",
    "            std = np.std(all_accs)\n",
    "            se = std / np.sqrt(len(all_accs))\n",
    "            ci95 = 1.96 * se\n",
    "            print(f\"{model}: {mean:.3f} ± {ci95:.3f} (SE: {se:.3f}, n={len(all_accs)} samples)\")\n",
    "    \n",
    "    # Statistical test\n",
    "    from scipy import stats as scipy_stats\n",
    "    \n",
    "    cvcl_all = []\n",
    "    clip_all = []\n",
    "    for cls in classes:\n",
    "        if cls in stats_results['cvcl-resnext']:\n",
    "            cvcl_all.extend(stats_results['cvcl-resnext'][cls]['raw'])\n",
    "        if cls in stats_results['clip-res']:\n",
    "            clip_all.extend(stats_results['clip-res'][cls]['raw'])\n",
    "    \n",
    "    if len(cvcl_all) > 0 and len(clip_all) > 0:\n",
    "        t_stat, p_value = scipy_stats.ttest_ind(cvcl_all, clip_all)\n",
    "        print(f\"\\nt-test: t={t_stat:.3f}, p={p_value:.6f}\")\n",
    "        if p_value < 0.001:\n",
    "            print(\"Result: Highly significant difference (p < 0.001)\")\n",
    "        elif p_value < 0.01:\n",
    "            print(\"Result: Significant difference (p < 0.01)\")\n",
    "        elif p_value < 0.05:\n",
    "            print(\"Result: Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"Result: No significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create difference plot\n",
    "if len(stats_results) > 0 and len(stats_results[models_to_test[0]]) > 0:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    differences = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        if cls in stats_results['clip-res'] and cls in stats_results['cvcl-resnext']:\n",
    "            diff = stats_results['clip-res'][cls]['mean'] - stats_results['cvcl-resnext'][cls]['mean']\n",
    "            differences.append(diff)\n",
    "        else:\n",
    "            differences.append(0)\n",
    "    \n",
    "    colors_diff = ['#2ecc71' if d > 0 else '#e74c3c' for d in differences]\n",
    "    bars = plt.bar(range(len(classes)), differences, color=colors_diff, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (cls, diff) in enumerate(zip(classes, differences)):\n",
    "        if diff != 0:\n",
    "            plt.text(i, diff + (0.01 if diff > 0 else -0.02), f'{diff:.2f}',\n",
    "                    ha='center', va='bottom' if diff > 0 else 'top', fontsize=8)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel('Object Class', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Performance Difference\\n(CLIP - CVCL)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Text-Vision Model Performance Differences by Class\\nWith Controlled Visual Properties',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#2ecc71', alpha=0.7, label='CLIP Better'),\n",
    "        Patch(facecolor='#e74c3c', alpha=0.7, label='CVCL Better')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    diff_plot_path = os.path.join(RESULTS_DIR, 'class_textvision_difference_controlled.png')\n",
    "    plt.savefig(diff_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved difference plot to {diff_plot_path}\")\n",
    "    \n",
    "    # Print summary of differences\n",
    "    clip_better = sum(1 for d in differences if d > 0)\n",
    "    cvcl_better = sum(1 for d in differences if d < 0)\n",
    "    tied = sum(1 for d in differences if d == 0)\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  CLIP performs better: {clip_better}/{len(classes)} classes\")\n",
    "    print(f\"  CVCL performs better: {cvcl_better}/{len(classes)} classes\")\n",
    "    if tied > 0:\n",
    "        print(f\"  No difference: {tied}/{len(classes)} classes\")\n",
    "    \n",
    "    avg_diff = np.mean([d for d in differences if d != 0])\n",
    "    print(f\"  Average difference: {avg_diff:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
