{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDT Text-Vision Test - CVCL Training Classes Only\\n\\nSame Class Different Texture\\n\\n**This version only tests on the 25 classes that appear in CVCL's training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "CVCL classes file: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\CVCL_Konkle_Overlap\\CVCLKonkMatches.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "CVCL_CLASSES_PATH = os.path.join(REPO_ROOT, 'data', 'CVCL_Konkle_Overlap', 'CVCLKonkMatches.csv')\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'cvcl_training_text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"CVCL classes file: {CVCL_CLASSES_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVCL Training Classes (24):\n",
      "  ball\n",
      "  butterfly\n",
      "  phone\n",
      "  bagel\n",
      "  basket\n",
      "  bell\n",
      "  fan\n",
      "  seashell\n",
      "  bird\n",
      "  stool\n",
      "  train\n",
      "  ring\n",
      "  tricycle\n",
      "  toothpaste\n",
      "  pen\n",
      "  tree\n",
      "  apple\n",
      "  cookie\n",
      "  bread\n",
      "  pumpkin\n",
      "  camera\n",
      "  rabbit\n",
      "  pillow\n",
      "  horse\n"
     ]
    }
   ],
   "source": [
    "# Load CVCL training classes\n",
    "cvcl_df = pd.read_csv(CVCL_CLASSES_PATH)\n",
    "CVCL_TRAINING_CLASSES = cvcl_df['Class'].str.strip().tolist()\n",
    "\n",
    "print(f\"CVCL Training Classes ({len(CVCL_TRAINING_CLASSES)}):\")\n",
    "for cls in CVCL_TRAINING_CLASSES:\n",
    "    print(f\"  {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2817 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 3 - ['large', 'medium', 'small']\n",
      "Unique textures: 2 - ['bumpy', 'smooth']\n",
      "\n",
      "Sample data:\n",
      "   class   color   size texture\n",
      "0  apple     red  large   bumpy\n",
      "1  apple   green  large   bumpy\n",
      "2  apple    blue  large   bumpy\n",
      "3  apple  yellow  large   bumpy\n",
      "4  apple  orange  large   bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data - FILTERED TO CVCL TRAINING CLASSES\n",
    "def load_cvcl_synthetickonkle_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset filtered to CVCL training classes\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Filter to only CVCL training classes\n",
    "    df = df[df['class'].isin(CVCL_TRAINING_CLASSES)].copy()\n",
    "    \n",
    "    # Handle missing ball and bread\n",
    "    missing_classes = set(CVCL_TRAINING_CLASSES) - set(df['class'].unique())\n",
    "    if missing_classes:\n",
    "        print(f\"Adding missing classes from folders: {missing_classes}\")\n",
    "        for cls in missing_classes:\n",
    "            folder = f\"{cls}_color\"\n",
    "            folder_path = os.path.join(DATA_PATH, folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "                for img_file in image_files:\n",
    "                    # Parse filename to extract metadata\n",
    "                    # Format: ball_large_bumpy_01_black.png\n",
    "                    # Parts: [class, size, texture, variant, color]\n",
    "                    parts = img_file.replace('.png', '').split('_')\n",
    "                    if len(parts) >= 5:\n",
    "                        new_row = {\n",
    "                            'folder': folder,\n",
    "                            'filename': img_file,\n",
    "                            'class': cls,\n",
    "                            'size': parts[1],  # This is the size (large/medium/small)\n",
    "                            'texture': parts[2],  # This is the texture (bumpy/smooth)\n",
    "                            'variant': parts[3],  # This is the variant (01/02)\n",
    "                            'color': '_'.join(parts[4:])  # Rest is color\n",
    "                        }\n",
    "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid metadata\n",
    "    df = df[df['color'].notna() & (df['color'] != '')].copy()\n",
    "    df = df[df['size'].notna() & (df['size'] != '')].copy()\n",
    "    df = df[df['texture'].notna() & (df['texture'] != '')].copy()\n",
    "    \n",
    "    # Standardize names (lowercase)\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid texture values\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    df = df[df['texture'].isin(valid_textures) & df['size'].isin(valid_sizes)].copy()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images from {df['class'].nunique()} CVCL training classes\")\n",
    "    print(f\"Classes: {sorted(df['class'].unique())}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()} - {sorted(df['size'].unique())}\")\n",
    "    print(f\"Unique textures: {df['texture'].nunique()} - {sorted(df['texture'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data_df = load_cvcl_synthetickonkle_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'size', 'texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_scdt_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n    \"\"\"Run SCDT text-vision test on CVCL training classes only\n    Same Class Different Texture - allows different colors/sizes for variety\n    \n    Test design:\n    - Query: One image of a specific texture\n    - Distractors: 3 images of the SAME CLASS but with different texture distribution\n    - Colors and sizes can vary for image diversity\n    - Text format: \"{texture} {class}\" (e.g., \"smooth apple\", \"bumpy apple\")\n    - Uses 3-1 split: 3 images of one texture, 1 of the other\n    \n    Args:\n        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n        seed: Random seed for reproducibility\n        device: Device to use (None for auto-detect)\n        num_trials: Total number of trials to run\n    \"\"\"\n    # Set seeds\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Running SCDT Text-Vision Test with {model_name}\")\n    print(f\"CVCL Training Classes Only\")\n    print(f\"(Same Class Different Texture - Varied Colors/Sizes)\")\n    print(f\"Text format: {{texture}} {{class}}\")\n    print(f\"Note: Using 3-1 split (3 of one texture, 1 of the other)\")\n    print(f\"{'='*60}\")\n    \n    # Device selection\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    print(f\"Using device: {device}\")\n    \n    # Load model\n    print(f\"[INFO] Loading {model_name} on {device}...\")\n    model, transform = load_model(model_name, seed=seed, device=device)\n    extractor = FeatureExtractor(model_name, model, device)\n    model.eval()\n    \n    # Load filtered data - CVCL training classes only\n    df = load_cvcl_synthetickonkle_data()\n    \n    # Filter to only valid texture values\n    valid_textures = ['smooth', 'bumpy']\n    df = df[df['texture'].isin(valid_textures)].copy()\n    \n    print(f\"Loaded {len(df)} images with texture annotations\")\n    print(f\"Unique classes: {df['class'].nunique()}\")\n    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n    \n    # Group by class to ensure we have both textures for each class\n    class_groups = df.groupby('class')\n    valid_classes = []\n    for class_name, group in class_groups:\n        unique_textures = group['texture'].unique()\n        if len(unique_textures) == 2:  # Has both smooth and bumpy\n            # Also check we have enough images per texture for variety\n            texture_counts = group.groupby('texture').size()\n            if texture_counts.min() >= 4:  # At least 4 images per texture (for 3-1 split)\n                valid_classes.append(class_name)\n    \n    if len(valid_classes) == 0:\n        print(\"ERROR: No CVCL training classes have both textures with enough images.\")\n        return [], 0.0\n    \n    print(f\"\\nFound {len(valid_classes)} CVCL classes with both textures\")\n    print(f\"Classes: {sorted(valid_classes)}\")\n    \n    # Pre-compute image embeddings\n    print(\"\\nExtracting image embeddings...\")\n    image_embeddings = {}\n    skipped_images = []\n    \n    # Get all relevant images from valid classes\n    df_valid = df[df['class'].isin(valid_classes)]\n    all_image_paths = df_valid['image_path'].unique().tolist()\n    batch_size = 16\n    \n    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n        batch_paths = all_image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for img_path in batch_paths:\n            try:\n                img = Image.open(img_path).convert('RGB')\n                img_processed = transform(img).unsqueeze(0).to(device)\n                batch_images.append((img_path, img_processed))\n            except Exception as e:\n                skipped_images.append(img_path)\n                continue\n        \n        if batch_images:\n            paths = [p for p, _ in batch_images]\n            imgs = torch.cat([img for _, img in batch_images], dim=0)\n            \n            with torch.no_grad():\n                embeddings = extractor.get_img_feature(imgs)\n                embeddings = extractor.norm_features(embeddings)\n            \n            for path, emb in zip(paths, embeddings):\n                image_embeddings[path] = emb.cpu().float()\n    \n    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n    if skipped_images:\n        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n    \n    # Prepare for trials\n    correct_count = 0\n    trial_results = []\n    \n    # Calculate trials per class\n    trials_per_class = num_trials // len(valid_classes)\n    remaining_trials = num_trials % len(valid_classes)\n    \n    print(f\"\\nRunning {num_trials} trials across {len(valid_classes)} classes...\")\n    print(f\"Trials per class: {trials_per_class}, with {remaining_trials} getting 1 extra\")\n    \n    # Run trials\n    for class_idx, class_name in enumerate(tqdm(valid_classes, desc=\"Processing classes\")):\n        # Get all images for this class\n        class_data = df_valid[df_valid['class'] == class_name]\n        \n        # Group by texture\n        smooth_images = class_data[class_data['texture'] == 'smooth']['image_path'].tolist()\n        bumpy_images = class_data[class_data['texture'] == 'bumpy']['image_path'].tolist()\n        \n        # Filter to valid embeddings\n        smooth_images = [p for p in smooth_images if p in image_embeddings]\n        bumpy_images = [p for p in bumpy_images if p in image_embeddings]\n        \n        # Determine number of trials for this class\n        n_trials = trials_per_class + (1 if class_idx < remaining_trials else 0)\n        \n        for trial in range(n_trials):\n            if len(trial_results) >= num_trials:\n                break\n            \n            # For 4-way choice with 2 textures, use 3-1 split for unambiguous mapping\n            # Randomly choose which texture gets 3 images vs 1\n            if random.random() < 0.5:\n                majority_texture = 'smooth'\n                minority_texture = 'bumpy'\n                majority_images = smooth_images\n                minority_images = bumpy_images\n            else:\n                majority_texture = 'bumpy'\n                minority_texture = 'smooth'\n                majority_images = bumpy_images\n                minority_images = smooth_images\n            \n            # Need at least 3 majority and 1 minority\n            if len(majority_images) < 3 or len(minority_images) < 1:\n                continue\n            \n            # Select 3 different images from majority texture (can be different colors/sizes)\n            selected_majority = random.sample(majority_images, 3)\n            # Select 1 from minority texture\n            selected_minority = random.sample(minority_images, 1)\n            \n            # Build candidates list with (image_path, texture) tuples\n            candidates = []\n            for img_path in selected_majority:\n                candidates.append((img_path, majority_texture))\n            for img_path in selected_minority:\n                candidates.append((img_path, minority_texture))\n            \n            # Select query from candidates\n            query_idx = random.randint(0, 3)\n            query_img_path, query_texture = candidates[query_idx]\n            \n            # Create text prompts\n            candidate_texts = [f\"{texture} {class_name.lower()}\" for _, texture in candidates]\n            \n            # Shuffle for random presentation\n            shuffled_order = list(range(4))\n            random.shuffle(shuffled_order)\n            shuffled_candidates = [candidates[i] for i in shuffled_order]\n            shuffled_texts = [candidate_texts[i] for i in shuffled_order]\n            correct_idx = shuffled_order.index(query_idx)\n            \n            # Encode text prompts\n            with torch.no_grad():\n                if \"clip\" in model_name:\n                    tokens = clip.tokenize(shuffled_texts, truncate=True).to(device)\n                    txt_features = model.encode_text(tokens)\n                    txt_features = extractor.norm_features(txt_features)\n                else:  # CVCL\n                    tokens, token_len = model.tokenize(shuffled_texts)\n                    tokens = tokens.to(device)\n                    if isinstance(token_len, torch.Tensor):\n                        token_len = token_len.to(device)\n                    txt_features = model.encode_text(tokens, token_len)\n                    txt_features = extractor.norm_features(txt_features)\n            \n            # Get query image embedding\n            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n            \n            # Calculate similarity\n            query_embedding = query_embedding.float()\n            txt_features = txt_features.float()\n            \n            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n            \n            # Get prediction\n            pred_idx = similarity.argmax(dim=1).item()\n            \n            # Check if correct\n            is_correct = (pred_idx == correct_idx)\n            if is_correct:\n                correct_count += 1\n            \n            # Store trial result\n            trial_results.append({\n                'trial': len(trial_results) + 1,\n                'query_class': class_name,\n                'query_texture': query_texture,\n                'query_img': os.path.basename(query_img_path),\n                'correct_idx': correct_idx,\n                'predicted_idx': pred_idx,\n                'correct': is_correct,\n                'candidate_texts': shuffled_texts,\n                'similarity_scores': similarity.cpu().numpy().tolist()\n            })\n    \n    # Calculate accuracy\n    accuracy = correct_count / len(trial_results) if trial_results else 0\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Results for {model_name} - SCDT Text-Vision Test:\")\n    print(f\"Total trials: {len(trial_results)}\")\n    print(f\"Correct: {correct_count}\")\n    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"{'='*60}\")\n    \n    # Save results\n    results_row = {\n        'Model': model_name,\n        'Test': 'SCDT-TextVision-CVCLTraining',\n        'Dataset': 'SyntheticKonkle_224',\n        'Correct': correct_count,\n        'Trials': len(trial_results),\n        'Accuracy': accuracy\n    }\n    \n    # Append to results file\n    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n    if os.path.exists(RESULTS_PATH):\n        results_df = pd.read_csv(RESULTS_PATH)\n    else:\n        results_df = pd.DataFrame()\n    \n    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n    print(f\"\\nResults saved to {RESULTS_PATH}\")\n    \n    return trial_results, accuracy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run CVCL test\ncvcl_trials, cvcl_accuracy = run_scdt_test('cvcl-resnext', seed=0, num_trials=4000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run CLIP test\nclip_trials, clip_accuracy = run_scdt_test('clip-resnext', seed=0, num_trials=4000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"SCDT TEXT-VISION TEST COMPARISON - CVCL TRAINING CLASSES\")\nprint(\"=\"*60)\nprint(f\"\\nTest: Same Class Different Texture (4-way forced choice)\")\nprint(f\"Control: Colors and sizes can vary (not mentioned in text)\")\nprint(f\"Text format: '{{texture}} {{class}}'\")\nprint(f\"\\nResults:\")\nprint(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\nprint(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\nprint(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\nif cvcl_accuracy > clip_accuracy:\n    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}% on its training classes\")\nelif clip_accuracy > cvcl_accuracy:\n    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}% even on CVCL's training classes\")\nelse:\n    print(\"Both models perform equally\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}