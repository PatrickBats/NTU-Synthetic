{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Class Different Color (SCDC) Text-Vision Comparison Test\n",
    "\n",
    "This notebook tests model performance on distinguishing objects of the SAME class that have DIFFERENT colors using text-vision zero-shot classification.\n",
    "\n",
    "Test format:\n",
    "- 4-way forced choice\n",
    "- Query: Image with specific class and color (e.g., red apple)\n",
    "- Text prompts: Include color + class (e.g., \"red apple\", \"green apple\", \"yellow apple\", \"blue apple\")\n",
    "- Distractors: Same class but different colors\n",
    "- 4000 trials total per model\n",
    "\n",
    "This is typically harder than DCDC since only color differs, not class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Image path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "IMG_PATH = os.path.join(DATA_PATH, )\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Image path: {IMG_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7882 images with color annotations\n",
      "Unique classes: 67\n",
      "Unique colors: 12\n",
      "Unique class-color combinations: 671\n",
      "\n",
      "Classes with 4+ colors (suitable for SCDC): 67\n",
      "Examples: ['abacus', 'apple', 'axe', 'babushkadolls', 'bagel']\n",
      "\n",
      "Sample data:\n",
      "    class   color    class_color\n",
      "0  abacus     red     abacus_red\n",
      "1  abacus   green   abacus_green\n",
      "2  abacus    blue    abacus_blue\n",
      "3  abacus  yellow  abacus_yellow\n",
      "4  abacus  orange  abacus_orange\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_konklab_data():\n",
    "    \"\"\"Load KonkLab dataset with metadata\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names (handle both 'color' and 'colour')\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid color information\n",
    "    df = df[df['color'].notna() & (df['color'] != '')].copy()\n",
    "    \n",
    "    # Standardize color names (lowercase)\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    \n",
    "    # Create class-color combination column\n",
    "    df['class_color'] = df['class'] + '_' + df['color']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with color annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique class-color combinations: {df['class_color'].nunique()}\")\n",
    "    \n",
    "    # Find classes that have multiple colors (needed for SCDC test)\n",
    "    class_color_counts = df.groupby('class')['color'].nunique()\n",
    "    multi_color_classes = class_color_counts[class_color_counts >= 4].index.tolist()\n",
    "    \n",
    "    print(f\"\\nClasses with 4+ colors (suitable for SCDC): {len(multi_color_classes)}\")\n",
    "    if len(multi_color_classes) > 0:\n",
    "        print(f\"Examples: {multi_color_classes[:5]}\")\n",
    "    \n",
    "    return df, multi_color_classes\n",
    "\n",
    "# Load data\n",
    "data_df, multi_color_classes = load_konklab_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'class_color']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scdc_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Same Class Different Color text-vision test\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-res')\n",
    "        seed: Random seed for reproducibility (matches original Class test)\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDC Text-Vision Test with {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model using same loader as original\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names (handle both 'color' and 'colour')\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid color information\n",
    "    df = df[df['color'].notna() & (df['color'] != '')].copy()\n",
    "    \n",
    "    # Standardize color names (lowercase)\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    \n",
    "    # Create class-color combination column\n",
    "    df['class_color'] = df['class'] + '_' + df['color']\n",
    "    \n",
    "    # Find classes that have at least 4 different colors (needed for 4-way choice)\n",
    "    class_colors = df.groupby('class')['color'].unique()\n",
    "    valid_classes = [cls for cls, colors in class_colors.items() if len(colors) >= 4]\n",
    "    \n",
    "    if len(valid_classes) == 0:\n",
    "        print(\"ERROR: No classes have 4+ different colors. Cannot run SCDC test.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_classes)} classes with 4+ colors\")\n",
    "    print(f\"Examples: {valid_classes[:5]}\")\n",
    "    \n",
    "    # Group data by class-color combinations for valid classes\n",
    "    df_valid = df[df['class'].isin(valid_classes)]\n",
    "    grouped = df_valid.groupby('class_color').agg({\n",
    "        'image_path': list,\n",
    "        'class': 'first',\n",
    "        'color': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"\\nUsing {len(grouped)} class-color combinations from {len(valid_classes)} classes\")\n",
    "    \n",
    "    # Pre-compute all image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Process in batches\n",
    "    all_image_paths = [img for imgs in grouped['image_path'] for img in imgs]\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            if img_path not in image_embeddings:  # Skip if already processed\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                    batch_images.append((img_path, img_processed))\n",
    "        \n",
    "                except Exception as e:\n",
    "                    # Skip corrupted/invalid images\n",
    "                    skipped_images.append(img_path)\n",
    "                    continue\n",
    "        if batch_images:\n",
    "            # Stack batch\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            # Store\n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()  # Ensure float32\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per class to get exactly num_trials\n",
    "    trials_per_class = num_trials // len(valid_classes)\n",
    "    remaining_trials = num_trials % len(valid_classes)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_classes)} classes...\")\n",
    "    print(f\"Trials per class: {trials_per_class}, with {remaining_trials} classes getting 1 extra\")\n",
    "    \n",
    "    # Run trials for each valid class\n",
    "    for class_idx, target_class in enumerate(tqdm(valid_classes, desc=\"Processing classes\")):\n",
    "        # Get all color combinations for this class\n",
    "        class_combos = grouped[grouped['class'] == target_class]\n",
    "        available_colors = class_combos['color'].unique()\n",
    "        \n",
    "        if len(available_colors) < 4:\n",
    "            continue  # Skip if not enough colors\n",
    "        \n",
    "        # Determine number of trials for this class\n",
    "        n_trials = trials_per_class + (1 if class_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Select 4 different colors for this class\n",
    "            selected_colors = random.sample(list(available_colors), 4)\n",
    "            \n",
    "            # First color is the query\n",
    "            query_color = selected_colors[0]\n",
    "            query_combo = f\"{target_class}_{query_color}\"\n",
    "            query_data = class_combos[class_combos['class_color'] == query_combo].iloc[0]\n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            \n",
    "            # Create candidate list (all 4 colors of same class)\n",
    "            candidate_combos = [f\"{target_class}_{color}\" for color in selected_colors]\n",
    "            random.shuffle(candidate_combos)\n",
    "            \n",
    "            # Get correct index\n",
    "            correct_idx = candidate_combos.index(query_combo)\n",
    "            \n",
    "            # Create text prompts for each candidate\n",
    "            candidate_texts = []\n",
    "            for combo in candidate_combos:\n",
    "                combo_data = class_combos[class_combos['class_color'] == combo].iloc[0]\n",
    "                # Create text prompt with color + class\n",
    "                text_prompt = f\"{combo_data['color']} {combo_data['class'].lower()}\"\n",
    "                candidate_texts.append(text_prompt)\n",
    "            \n",
    "            # Encode text prompts (batch encoding)\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity following original predict() method\n",
    "            # Ensure both are float32\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': target_class,\n",
    "                'query_color': query_color,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': candidate_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDC Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDC-TextVision',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    # Append to results file\n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDC Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDC Text-Vision Test with cvcl-resnext\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 67 classes with 4+ colors\n",
      "Examples: ['abacus', 'apple', 'axe', 'babushkadolls', 'bagel']\n",
      "\n",
      "Using 671 class-color combinations from 67 classes\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 493/493 [00:24<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7841 images\n",
      "Skipped 32 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 67 classes...\n",
      "Trials per class: 59, with 47 classes getting 1 extra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 67/67 [00:40<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDC Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 982\n",
      "Accuracy: 0.2455 (24.55%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original Class test)\n",
    "cvcl_trials, cvcl_accuracy = run_scdc_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDC Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDC Text-Vision Test with clip-resnext\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 67 classes with 4+ colors\n",
      "Examples: ['abacus', 'apple', 'axe', 'babushkadolls', 'bagel']\n",
      "\n",
      "Using 671 class-color combinations from 67 classes\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/493 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 493/493 [00:17<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7841 images\n",
      "Skipped 32 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 67 classes...\n",
      "Trials per class: 59, with 47 classes getting 1 extra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 67/67 [00:24<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDC Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3927\n",
      "Accuracy: 0.9818 (98.17%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original Class test)\n",
    "clip_trials, clip_accuracy = run_scdc_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCDC TEXT-VISION TEST COMPARISON\n",
      "============================================================\n",
      "\n",
      "Test: Same Class Different Color (4-way forced choice)\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2455 (24.55%)\n",
      "  CLIP Accuracy: 0.9818 (98.17%)\n",
      "\n",
      "Difference: 0.7363 (73.62%)\n",
      "CLIP performs better by 73.62%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- SCDC is typically harder than DCDC since only color differs\n",
      "- Models must rely on color understanding (both visual and textual)\n",
      "- Lower accuracy than DCDC would confirm color discrimination is challenging\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDC TEXT-VISION TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Same Class Different Color (4-way forced choice)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- SCDC is typically harder than DCDC since only color differs\")\n",
    "print(\"- Models must rely on color understanding (both visual and textual)\")\n",
    "print(\"- Lower accuracy than DCDC would confirm color discrimination is challenging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDC Text-Vision Test Characteristics:\n",
    "- Tests discrimination when class is SAME but colors DIFFER\n",
    "- All 4 candidates are the same object class (e.g., all apples)\n",
    "- Text prompts include color information (e.g., \"red apple\", \"green apple\")\n",
    "- Generally harder than DCDC since class provides no discriminative signal\n",
    "- Pure test of color understanding (both visual and textual)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
