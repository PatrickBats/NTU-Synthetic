{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Class Different Color and Texture (SCDCT) Text-Vision Comparison - SyntheticKonkle\n",
    "\n",
    "This notebook compares CVCL and CLIP models on text-vision matching using the SyntheticKonkle dataset.\n",
    "The task is 4-way classification where distractors are from the SAME class but DIFFERENT colors AND textures.\n",
    "\n",
    "## Text Ordering Considerations\n",
    "\n",
    "**We use natural English adjective ordering**: `\"{color} {texture} {class}\"`\n",
    "- Example: \"red smooth apple\", \"blue bumpy apple\", \"green smooth apple\"\n",
    "- This follows standard English grammar rules where color comes before texture\n",
    "- Both CVCL (trained on child-directed speech) and CLIP (trained on internet text) expect this natural ordering\n",
    "- Parents typically say \"red bumpy ball\" not \"bumpy red ball\"\n",
    "\n",
    "## Test Characteristics\n",
    "- **Visual**: All candidates from SAME class, with controlled size\n",
    "- **Variation**: Both color AND texture differ between candidates\n",
    "- **Control**: Size held constant visually but NOT mentioned in text\n",
    "- **Difficulty**: Medium - harder than DCDCT (where class also varies) but easier than pure color or texture tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7865 images with size, color, and texture annotations\n",
      "Unique classes: 67\n",
      "Unique colors: 11\n",
      "Unique textures: ['bumpy', 'smooth']\n",
      "Unique sizes: ['large', 'medium', 'small']\n",
      "\n",
      "Class-Size combinations with 3+ color-texture pairs: 200\n",
      "Examples: ['abacus_large', 'abacus_medium', 'abacus_small']\n",
      "  abacus_large has color-texture pairs like: ['red_bumpy' 'green_bumpy' 'blue_bumpy' 'yellow_bumpy']\n",
      "\n",
      "Sample data:\n",
      "    class   color texture   size color_texture\n",
      "0  abacus     red   bumpy  large     red_bumpy\n",
      "1  abacus   green   bumpy  large   green_bumpy\n",
      "2  abacus    blue   bumpy  large    blue_bumpy\n",
      "3  abacus  yellow   bumpy  large  yellow_bumpy\n",
      "4  abacus  orange   bumpy  large  orange_bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_synthetic_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset with metadata for color-texture testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid size and texture values\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['size'].isin(valid_sizes) & df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_size'] = df['class'] + '_' + df['size']\n",
    "    df['color_texture'] = df['color'] + '_' + df['texture']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['size'] + '_' + df['texture']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with size, color, and texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n",
    "    print(f\"Unique sizes: {sorted(df['size'].unique())}\")\n",
    "    \n",
    "    # Find class-size combinations that have multiple color-texture pairs\n",
    "    cs_groups = df.groupby('class_size')['color_texture'].nunique()\n",
    "    valid_cs = cs_groups[cs_groups >= 3].index.tolist()\n",
    "    \n",
    "    print(f\"\\nClass-Size combinations with 3+ color-texture pairs: {len(valid_cs)}\")\n",
    "    if len(valid_cs) > 0:\n",
    "        print(f\"Examples: {valid_cs[:3]}\")\n",
    "        # Show color-texture distribution for first example\n",
    "        if len(valid_cs) > 0:\n",
    "            example = valid_cs[0]\n",
    "            ct_pairs = df[df['class_size'] == example]['color_texture'].unique()[:4]\n",
    "            print(f\"  {example} has color-texture pairs like: {ct_pairs}\")\n",
    "    \n",
    "    return df, valid_cs\n",
    "\n",
    "# Load data\n",
    "data_df, valid_combinations = load_synthetic_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'texture', 'size', 'color_texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scdct_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Same Class Different Color and Texture text-vision test with controlled size\n",
    "    \n",
    "    Text format uses natural English ordering: \"{color} {texture} {class}\"\n",
    "    Example: \"red smooth apple\", \"blue bumpy apple\"\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDCT Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Same Class Different Color & Texture - Controlled Size)\")\n",
    "    print(f\"Text format: {{color}} {{texture}} {{class}} (natural English order)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid size and texture values\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['size'].isin(valid_sizes) & df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_size'] = df['class'] + '_' + df['size']\n",
    "    df['color_texture'] = df['color'] + '_' + df['texture']\n",
    "    \n",
    "    # Find class-size combinations with at least 3 different color-texture pairs\n",
    "    cs_groups = df.groupby('class_size')\n",
    "    valid_cs = []\n",
    "    for cs, group in cs_groups:\n",
    "        unique_ct = group['color_texture'].unique()\n",
    "        # With many colors and 2 textures, we should have many combinations\n",
    "        if len(unique_ct) >= 3:\n",
    "            valid_cs.append(cs)\n",
    "    \n",
    "    if len(valid_cs) == 0:\n",
    "        print(\"ERROR: No class-size combinations have 3+ different color-texture pairs.\")\n",
    "        print(\"Cannot run SCDCT test with strict controls.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_cs)} class-size combinations with 3+ color-texture pairs\")\n",
    "    \n",
    "    # Pre-compute image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['class_size'].isin(valid_cs)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per combination\n",
    "    trials_per_cs = num_trials // len(valid_cs)\n",
    "    remaining_trials = num_trials % len(valid_cs)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_cs)} combinations...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for cs_idx, cs in enumerate(tqdm(valid_cs, desc=\"Processing combinations\")):\n",
    "        # Get all images for this class-size combination\n",
    "        cs_data = df_valid[df_valid['class_size'] == cs]\n",
    "        \n",
    "        # Group by color-texture\n",
    "        ct_groups = cs_data.groupby('color_texture').agg({\n",
    "            'image_path': list,\n",
    "            'color': 'first',\n",
    "            'texture': 'first'\n",
    "        }).to_dict('index')\n",
    "        \n",
    "        available_ct = list(ct_groups.keys())\n",
    "        \n",
    "        if len(available_ct) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Parse class from combination string\n",
    "        class_name = cs.split('_')[0]\n",
    "        \n",
    "        # Determine number of trials for this combination\n",
    "        n_trials = trials_per_cs + (1 if cs_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Select color-texture pairs for 4-way choice\n",
    "            if len(available_ct) == 3:\n",
    "                # Use all 3 pairs plus duplicate one for 4-way choice\n",
    "                selected_pairs = available_ct.copy()\n",
    "                selected_pairs.append(random.choice(available_ct))\n",
    "            else:\n",
    "                # Select 4 different pairs if possible\n",
    "                selected_pairs = random.sample(available_ct, min(4, len(available_ct)))\n",
    "            \n",
    "            # First pair is the query\n",
    "            query_pair = selected_pairs[0]\n",
    "            query_data = ct_groups[query_pair]\n",
    "            \n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            query_color = query_data['color']\n",
    "            query_texture = query_data['texture']\n",
    "            \n",
    "            # Shuffle for candidate order\n",
    "            random.shuffle(selected_pairs)\n",
    "            correct_idx = selected_pairs.index(query_pair)\n",
    "            \n",
    "            # Create text prompts - NATURAL ENGLISH ORDER: {color} {texture} {class}\n",
    "            candidate_texts = []\n",
    "            for pair in selected_pairs:\n",
    "                pair_data = ct_groups[pair]\n",
    "                # Natural English order: color before texture before noun\n",
    "                text_prompt = f\"{pair_data['color']} {pair_data['texture']} {class_name.lower()}\"\n",
    "                candidate_texts.append(text_prompt)\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': class_name,\n",
    "                'query_color': query_color,\n",
    "                'query_texture': query_texture,\n",
    "                'class_size': cs,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': candidate_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDCT Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDCT-TextVision',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDCT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCT Text-Vision Test with cvcl-resnext\n",
      "(Same Class Different Color & Texture - Controlled Size)\n",
      "Text format: {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 200 class-size combinations with 3+ color-texture pairs\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 492/492 [00:22<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7835 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 200 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 200/200 [00:46<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDCT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1094\n",
      "Accuracy: 0.2735 (27.35%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_scdct_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDCT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCT Text-Vision Test with clip-resnext\n",
      "(Same Class Different Color & Texture - Controlled Size)\n",
      "Text format: {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 200 class-size combinations with 3+ color-texture pairs\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 492/492 [00:18<00:00, 26.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7835 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 200 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 200/200 [00:19<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDCT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3746\n",
      "Accuracy: 0.9365 (93.65%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_scdct_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCDCT TEXT-VISION TEST COMPARISON\n",
      "============================================================\n",
      "\n",
      "Test: Same Class Different Color & Texture (4-way forced choice)\n",
      "Control: Size held constant (not mentioned in text)\n",
      "Text format: '{color} {texture} {class}' (natural English order)\n",
      "Example: 'red smooth apple' vs 'blue bumpy apple'\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2735 (27.35%)\n",
      "  CLIP Accuracy: 0.9365 (93.65%)\n",
      "\n",
      "Difference: 0.6630 (66.30%)\n",
      "CLIP performs better by 66.30%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- Tests multi-attribute discrimination within same class\n",
      "- Both color AND texture provide discriminative signals\n",
      "- Natural English ordering (color before texture) helps both models\n",
      "- Should perform better than single-attribute tests (SCDC or texture-only)\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDCT TEXT-VISION TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Same Class Different Color & Texture (4-way forced choice)\")\n",
    "print(f\"Control: Size held constant (not mentioned in text)\")\n",
    "print(f\"Text format: '{{color}} {{texture}} {{class}}' (natural English order)\")\n",
    "print(f\"Example: 'red smooth apple' vs 'blue bumpy apple'\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Tests multi-attribute discrimination within same class\")\n",
    "print(\"- Both color AND texture provide discriminative signals\")\n",
    "print(\"- Natural English ordering (color before texture) helps both models\")\n",
    "print(\"- Should perform better than single-attribute tests (SCDC or texture-only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDCT Text-Vision Test Characteristics:\n",
    "- **Visual Control**: All 4 candidates have same class and size\n",
    "- **Variation**: Both color AND texture differ between candidates\n",
    "- **Text Prompts**: Natural order \"{color} {texture} {class}\" (e.g., \"red smooth apple\")\n",
    "- **NOT mentioned**: Size is controlled but excluded from text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
