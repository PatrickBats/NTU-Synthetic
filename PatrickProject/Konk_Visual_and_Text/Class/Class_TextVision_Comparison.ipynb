{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Text-Vision Comparison - KonkLab\n",
    "\n",
    "This notebook compares CVCL and CLIP models using text-based prototypes for class discrimination.\n",
    "Instead of averaging image features to create prototypes, we use text descriptions.\n",
    "The task remains 4-way forced choice classification with 4000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport random\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom collections import defaultdict\nimport clip\n\n# Path setup - Use absolute paths to avoid any confusion\nREPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n\n# Add discover-hidden-visual-concepts to path\nDISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\nsys.path.insert(0, DISCOVER_ROOT)\nsys.path.insert(0, REPO_ROOT)\n\n# Import from discover-hidden-visual-concepts repo\nsys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\nfrom utils.model_loader import load_model\nfrom models.feature_extractor import FeatureExtractor\n\n# KonkLab paths - Use absolute paths\nCSV_PATH = os.path.join(REPO_ROOT, 'data', 'KonkLab', 'testdata.csv')\nIMG_DIR = os.path.join(REPO_ROOT, 'data', 'KonkLab', '17-objects')\nMASTER_CSV = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')"
  },
  {
   "cell_type": "code",
   "source": "# Quick test to check if GPU is available and model loading speed\nimport torch\nimport time\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    \n    # Test GPU speed\n    x = torch.randn(32, 3, 224, 224).cuda()\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(10):\n        _ = x * 2\n    torch.cuda.synchronize()\n    print(f\"GPU test time: {time.time() - start:.3f}s\")\nelse:\n    print(\"WARNING: Running on CPU will be VERY slow!\")\n    print(\"If you have a GPU, make sure CUDA is properly installed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Dataset class with optimized loading\nclass ClassImageDataset(Dataset):\n    def __init__(self, csv_path, img_dir, transform):\n        self.df = pd.read_csv(csv_path)\n        assert 'Filename' in self.df and 'Class' in self.df, \\\n            \"CSV needs Filename and Class columns\"\n        self.img_dir = img_dir\n        self.transform = transform\n        # Pre-compute paths to avoid repeated joins\n        self.paths = [os.path.join(img_dir, row['Class'], row['Filename']) \n                      for _, row in self.df.iterrows()]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        cls = row['Class']\n        # Use pre-computed path\n        path = self.paths[idx]\n        img = Image.open(path).convert('RGB')\n        return self.transform(img), cls, idx\n\ndef collate_fn(batch):\n    imgs = torch.stack([b[0] for b in batch])\n    classes = [b[1] for b in batch]\n    idxs = [b[2] for b in batch]\n    return imgs, classes, idxs\n\ndef run_class_text_vision_test(model_name, seed=0, device=None, batch_size=16, \n                                trials_per_class=None, max_trials=4000):\n    \"\"\"\n    Run 4-way classification test using the EXACT methodology from discover-hidden-visual-concepts predict() method.\n    \"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    # Device selection\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    if device == 'cuda' and not torch.cuda.is_available():\n        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n        device = 'cpu'\n    \n    if device == 'cpu':\n        print(\"[WARNING] Running on CPU - this will be SLOW!\")\n        print(\"Reducing batch size to 4 for CPU\")\n        batch_size = 4\n    else:\n        print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n\n    # Check if model supports text encoding\n    if model_name in ['resnext', 'dino_s_resnext50']:\n        print(f\"[WARNING] {model_name} has no text encoder, skipping\")\n        return {}, 0.0\n\n    # 1) Load model\n    print(f\"[INFO] Loading {model_name} on {device}...\")\n    import time\n    start_time = time.time()\n    model, transform = load_model(model_name, seed=seed, device=device)\n    extractor = FeatureExtractor(model_name, model, device)\n    print(f\"[INFO] Model loaded in {time.time() - start_time:.1f}s\")\n\n    # 2) Load dataset\n    ds = ClassImageDataset(CSV_PATH, IMG_DIR, transform)\n    print(f\"[INFO] Dataset: {len(ds)} images\")\n    \n    # Use multiple workers only if not on Windows or if on Linux/Mac\n    num_workers = 0 if os.name == 'nt' else 2\n    \n    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n                   num_workers=num_workers, collate_fn=collate_fn, \n                   pin_memory=(device=='cuda'))\n    \n    # 3) Extract image embeddings\n    print(f\"[INFO] Extracting embeddings (batch_size={batch_size})...\")\n    all_img_embs, all_classes, all_idxs = [], [], []\n    \n    from tqdm import tqdm\n    start_time = time.time()\n    \n    with torch.no_grad():\n        for imgs, classes, idxs in tqdm(dl, desc=\"Extracting embeddings\"):\n            imgs = imgs.to(device, non_blocking=True)\n            feats = extractor.get_img_feature(imgs)\n            feats = extractor.norm_features(feats).cpu()\n            \n            all_img_embs.append(feats)\n            all_classes.extend(classes)\n            all_idxs.extend(idxs)\n                \n    all_img_embs = torch.cat(all_img_embs, dim=0)\n    print(f\"[INFO] Extracted {len(all_idxs)} embeddings in {time.time() - start_time:.1f}s\")\n\n    # 4) Encode text labels\n    unique_classes = list(set(all_classes))\n    print(f\"[INFO] Encoding {len(unique_classes)} class labels...\")\n    \n    class_text_features = {}\n    with torch.no_grad():\n        if \"clip\" in model_name:\n            tokens = clip.tokenize(unique_classes, truncate=True).to(device)\n            txt_features = model.encode_text(tokens)\n            txt_features = extractor.norm_features(txt_features).cpu()\n            for i, cls in enumerate(unique_classes):\n                class_text_features[cls] = txt_features[i]\n        else:  # CVCL\n            tokens, token_len = model.tokenize(unique_classes)\n            tokens = tokens.to(device)\n            if isinstance(token_len, torch.Tensor):\n                token_len = token_len.to(device)\n            txt_features = model.encode_text(tokens, token_len)\n            txt_features = extractor.norm_features(txt_features).cpu()\n            for i, cls in enumerate(unique_classes):\n                class_text_features[cls] = txt_features[i]\n    \n    print(f\"[INFO] Text encoding complete\")\n    \n    # 5) Build mappings\n    idx2class = {i:c for i,c in zip(all_idxs, all_classes)}\n    idx2row = {i:r for r,i in enumerate(all_idxs)}\n    class2idxs = defaultdict(list)\n    for i,c in idx2class.items():\n        class2idxs[c].append(i)\n\n    # 6) Run trials - FIXED to run exactly max_trials\n    class_results = {}\n    total_correct = 0\n    total_trials = 0\n    \n    # Get valid classes (those with at least 1 image)\n    valid_classes = [c for c in class2idxs if len(class2idxs[c]) >= 1]\n    n_classes = len(valid_classes)\n    \n    # Calculate trials per class if not specified\n    if trials_per_class is None:\n        # Distribute trials evenly across classes, with some getting extra\n        trials_per_class = max_trials // n_classes\n        extra_trials = max_trials % n_classes\n    else:\n        extra_trials = 0\n    \n    print(f\"[INFO] Running {max_trials} total trials across {n_classes} classes\")\n    print(f\"[INFO] Base trials per class: {trials_per_class}, extra trials for first {extra_trials} classes\")\n    \n    # Run trials for each class\n    for class_idx, cls in enumerate(valid_classes):\n        if total_trials >= max_trials:\n            break\n            \n        idxs = class2idxs[cls]\n        \n        # Add extra trial for first few classes to reach exactly max_trials\n        current_trials = trials_per_class + (1 if class_idx < extra_trials else 0)\n        # Don't exceed max_trials\n        current_trials = min(current_trials, max_trials - total_trials)\n        \n        correct = 0\n        txt_feature = class_text_features[cls].unsqueeze(0)\n        \n        for trial in range(current_trials):\n            # Pick query and distractors\n            q = random.choice(idxs)\n            others = [i for i in all_idxs if idx2class[i] != cls]\n            if len(others) < 3:\n                continue\n            distractors = random.sample(others, 3)\n            \n            # 4-way classification\n            candidates = [q] + distractors\n            cand_features = torch.stack([all_img_embs[idx2row[i]] for i in candidates])\n            cand_features = cand_features.unsqueeze(0)\n            \n            # Compute similarity\n            txt_feature_expanded = txt_feature.unsqueeze(1)\n            similarity = (100.0 * cand_features @ txt_feature_expanded.transpose(-2, -1)).softmax(dim=1)\n            similarity = similarity.squeeze()\n            \n            # Predict (query is at index 0)\n            if similarity.argmax().item() == 0:\n                correct += 1\n                total_correct += 1\n            total_trials += 1\n\n        acc = correct / current_trials if current_trials > 0 else 0\n        class_results[cls] = {'correct': correct, 'trials': current_trials, 'accuracy': acc}\n        \n        if total_trials % 500 == 0:\n            print(f\"[PROGRESS] {total_trials}/{max_trials} trials completed\")\n\n    # Final progress update\n    print(f\"[FINAL] Completed {total_trials} trials\")\n    \n    overall_acc = total_correct / total_trials if total_trials else 0.0\n    \n    print(f\"\\n[RESULTS] Accuracy: {total_correct}/{total_trials} ({overall_acc:.1%})\")\n    \n    # Save results\n    summary_df = pd.DataFrame([{\n        'Model': model_name,\n        'Test': 'Class-TextVision-Original',\n        'Dataset': 'KonkLab',\n        'Correct': total_correct,\n        'Trials': total_trials,\n        'Accuracy': overall_acc\n    }])\n    \n    os.makedirs(os.path.dirname(MASTER_CSV), exist_ok=True)\n    if os.path.exists(MASTER_CSV):\n        summary_df.to_csv(MASTER_CSV, mode='a', header=False, index=False, float_format='%.4f')\n    else:\n        summary_df.to_csv(MASTER_CSV, index=False, float_format='%.4f')\n\n    return class_results, overall_acc",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVCL Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CVCL text-vision classification\n",
    "cvcl_results, cvcl_overall = run_class_text_vision_test('cvcl-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCVCL Results per Class:\")\n",
    "for cls, res in sorted(cvcl_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCVCL Overall Accuracy: {cvcl_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CLIP text-vision classification\n",
    "clip_results, clip_overall = run_class_text_vision_test('clip-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCLIP Results per Class:\")\n",
    "for cls, res in sorted(clip_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCLIP Overall Accuracy: {clip_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEXT-VISION vs VISUAL PROTOTYPE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nText-Vision Classification (using text descriptions as prototypes):\")\n",
    "print(f\"  CVCL: {cvcl_overall:.1%}\")\n",
    "print(f\"  CLIP: {clip_overall:.1%}\")\n",
    "print(f\"  CLIP advantage: {(clip_overall - cvcl_overall)*100:.1f} percentage points\")\n",
    "print(f\"\\nNote: Compare these results with the visual prototype version\")\n",
    "print(f\"to see how text-based prototypes perform vs image-based prototypes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}