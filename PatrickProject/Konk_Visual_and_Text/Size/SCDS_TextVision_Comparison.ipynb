{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Image path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "IMG_PATH = os.path.join(DATA_PATH, )\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Image path: {IMG_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7882 images with size, color, and texture annotations\n",
      "Unique classes: 67\n",
      "Unique sizes: 4\n",
      "Size values: ['bumpy', 'large', 'medium', 'small']\n",
      "\n",
      "Class-Color-Texture combinations with 3 sizes: 1286\n",
      "Examples: ['abacus_black_bumpy', 'abacus_black_smooth', 'abacus_blue_bumpy']\n",
      "  abacus_black_bumpy has sizes: ['large', 'medium', 'small']\n",
      "\n",
      "Sample data:\n",
      "    class   color texture   size  class_color_texture\n",
      "0  abacus     red   bumpy  large     abacus_red_bumpy\n",
      "1  abacus   green   bumpy  large   abacus_green_bumpy\n",
      "2  abacus    blue   bumpy  large    abacus_blue_bumpy\n",
      "3  abacus  yellow   bumpy  large  abacus_yellow_bumpy\n",
      "4  abacus  orange   bumpy  large  abacus_orange_bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_konklab_data():\n",
    "    \"\"\"Load KonkLab dataset with metadata for size testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_texture'] = df['class'] + '_' + df['color'] + '_' + df['texture']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with size, color, and texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    print(f\"Size values: {sorted(df['size'].unique())}\")\n",
    "    \n",
    "    # Find class-color-texture combinations that have multiple sizes (needed for SCDS test)\n",
    "    cct_size_counts = df.groupby('class_color_texture')['size'].nunique()\n",
    "    valid_cct = cct_size_counts[cct_size_counts >= 3].index.tolist()  # Changed from 4 to 3\n",
    "    \n",
    "    print(f\"\\nClass-Color-Texture combinations with 3 sizes: {len(valid_cct)}\")\n",
    "    if len(valid_cct) > 0:\n",
    "        print(f\"Examples: {valid_cct[:3]}\")\n",
    "        # Show size distribution for first example\n",
    "        if len(valid_cct) > 0:\n",
    "            example = valid_cct[0]\n",
    "            sizes = df[df['class_color_texture'] == example]['size'].unique()\n",
    "            print(f\"  {example} has sizes: {sorted(sizes)}\")\n",
    "    \n",
    "    return df, valid_cct\n",
    "\n",
    "# Load data\n",
    "data_df, valid_combinations = load_konklab_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'texture', 'size', 'class_color_texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scds_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Same Class Different Size text-vision test with controlled color/texture\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-res')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDS Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Same Class Different Size - Controlled Color/Texture)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_texture'] = df['class'] + '_' + df['color'] + '_' + df['texture']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    # Find class-color-texture combinations with at least 3 different sizes (small, medium, large)\n",
    "    cct_groups = df.groupby('class_color_texture')\n",
    "    valid_cct = []\n",
    "    for cct, group in cct_groups:\n",
    "        unique_sizes = group['size'].unique()\n",
    "        if len(unique_sizes) >= 3:  # Changed from 4 to 3\n",
    "            valid_cct.append(cct)\n",
    "    \n",
    "    if len(valid_cct) == 0:\n",
    "        print(\"ERROR: No class-color-texture combinations have 3+ different sizes.\")\n",
    "        print(\"Cannot run SCDS test with strict controls.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_cct)} class-color-texture combinations with 3+ sizes\")\n",
    "    \n",
    "    # Pre-compute image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['class_color_texture'].isin(valid_cct)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                # Skip corrupted/invalid images\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per combination\n",
    "    trials_per_cct = num_trials // len(valid_cct)\n",
    "    remaining_trials = num_trials % len(valid_cct)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_cct)} combinations...\")\n",
    "    print(f\"Trials per combination: {trials_per_cct}, with {remaining_trials} getting 1 extra\")\n",
    "    \n",
    "    # Run trials\n",
    "    for cct_idx, cct in enumerate(tqdm(valid_cct, desc=\"Processing combinations\")):\n",
    "        # Get all images for this class-color-texture combination\n",
    "        cct_data = df_valid[df_valid['class_color_texture'] == cct]\n",
    "        \n",
    "        # Get available sizes for this combination\n",
    "        size_groups = cct_data.groupby('size')['image_path'].apply(list).to_dict()\n",
    "        available_sizes = list(size_groups.keys())\n",
    "        \n",
    "        if len(available_sizes) < 3:  # Changed from 4 to 3\n",
    "            continue\n",
    "        \n",
    "        # Parse class from combination string\n",
    "        class_name = cct.split('_')[0]\n",
    "        \n",
    "        # Determine number of trials for this combination\n",
    "        n_trials = trials_per_cct + (1 if cct_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # For 4-way choice, we'll use 3 sizes + duplicate one\n",
    "            if len(available_sizes) == 3:\n",
    "                # Use all 3 sizes plus duplicate one for 4-way choice\n",
    "                selected_sizes = available_sizes.copy()\n",
    "                # Add a duplicate of a random size to make 4 options\n",
    "                selected_sizes.append(random.choice(available_sizes))\n",
    "            else:\n",
    "                # If somehow we have more than 3 sizes, select 4\n",
    "                selected_sizes = random.sample(available_sizes, min(4, len(available_sizes)))\n",
    "            \n",
    "            # First size is the query\n",
    "            query_size = selected_sizes[0]\n",
    "            \n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in size_groups[query_size] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            \n",
    "            # Shuffle for candidate order\n",
    "            random.shuffle(selected_sizes)\n",
    "            correct_idx = selected_sizes.index(query_size)\n",
    "            \n",
    "            # Create text prompts - ONLY size + class, no color/texture\n",
    "            candidate_texts = [f\"{size} {class_name.lower()}\" for size in selected_sizes]\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': class_name,\n",
    "                'query_size': query_size,\n",
    "                'class_color_texture': cct,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': candidate_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDS Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDS-TextVision-Controlled',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDS Text-Vision Test with cvcl-resnext\n",
      "(Same Class Different Size - Controlled Color/Texture)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1286 class-color-texture combinations with 3+ sizes\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 480/480 [00:21<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7657 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 1286 combinations...\n",
      "Trials per combination: 3, with 142 getting 1 extra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 1286/1286 [00:38<00:00, 33.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDS Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 2103\n",
      "Accuracy: 0.5258 (52.58%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_scds_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDS Text-Vision Test with clip-resnext\n",
      "(Same Class Different Size - Controlled Color/Texture)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 1286 class-color-texture combinations with 3+ sizes\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/480 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 480/480 [00:16<00:00, 28.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7657 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 1286 combinations...\n",
      "Trials per combination: 3, with 142 getting 1 extra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 1286/1286 [00:21<00:00, 60.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDS Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 387\n",
      "Accuracy: 0.0968 (9.68%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_scds_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCDS TEXT-VISION TEST COMPARISON (CONTROLLED)\n",
      "============================================================\n",
      "\n",
      "Test: Same Class Different Size (4-way forced choice)\n",
      "Control: Color and Texture held constant (not mentioned in text)\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.5258 (52.58%)\n",
      "  CLIP Accuracy: 0.0968 (9.68%)\n",
      "\n",
      "Difference: 0.4290 (42.90%)\n",
      "CVCL performs better by 42.90%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- This is a PURE size discrimination test\n",
      "- Color and texture are visually controlled but not mentioned in text\n",
      "- Tests whether models can map size concepts from vision to language\n",
      "- Lower accuracy indicates difficulty with size understanding\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDS TEXT-VISION TEST COMPARISON (CONTROLLED)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Same Class Different Size (4-way forced choice)\")\n",
    "print(f\"Control: Color and Texture held constant (not mentioned in text)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- This is a PURE size discrimination test\")\n",
    "print(\"- Color and texture are visually controlled but not mentioned in text\")\n",
    "print(\"- Tests whether models can map size concepts from vision to language\")\n",
    "print(\"- Lower accuracy indicates difficulty with size understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDS Text-Vision Test Characteristics (Controlled Version):\n",
    "- **Visual Control**: All 4 candidates have same class, color, and texture\n",
    "- **Size Variation**: ONLY size differs between candidates\n",
    "- **Text Prompts**: Only mention size + class (e.g., \"large apple\", \"small apple\")\n",
    "- **NOT mentioned**: Color and texture are controlled but excluded from text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
