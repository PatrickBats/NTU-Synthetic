{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------  12.6/12.8 MB 78.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 66.7 MB/s  0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Text-Vision Comparison - CVCL Training Classes Only\n",
    "\n",
    "This notebook compares CVCL and CLIP models using text-based prototypes for class discrimination.\n",
    "**Only tests on the 25 classes that appear in CVCL's training data.**\n",
    "Instead of averaging image features to create prototypes, we use text descriptions.\n",
    "The task remains 4-way forced choice classification with 4000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import clip\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "CVCL_CLASSES_PATH = os.path.join(REPO_ROOT, 'data', 'CVCL_Konkle_Overlap', 'CVCLKonkMatches.csv')\n",
    "CSV_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "IMG_DIR = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "MASTER_CSV = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'cvcl_training_text_vision_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVCL Training Classes (24):\n",
      "  ball\n",
      "  butterfly\n",
      "  phone\n",
      "  bagel\n",
      "  basket\n",
      "  bell\n",
      "  fan\n",
      "  seashell\n",
      "  bird\n",
      "  stool\n",
      "  train\n",
      "  ring\n",
      "  tricycle\n",
      "  toothpaste\n",
      "  pen\n",
      "  tree\n",
      "  apple\n",
      "  cookie\n",
      "  bread\n",
      "  pumpkin\n",
      "  camera\n",
      "  rabbit\n",
      "  pillow\n",
      "  horse\n"
     ]
    }
   ],
   "source": [
    "# Load CVCL training classes\n",
    "cvcl_df = pd.read_csv(CVCL_CLASSES_PATH)\n",
    "CVCL_TRAINING_CLASSES = cvcl_df['Class'].str.strip().tolist()\n",
    "\n",
    "print(f\"CVCL Training Classes ({len(CVCL_TRAINING_CLASSES)}):\")\n",
    "for cls in CVCL_TRAINING_CLASSES:\n",
    "    print(f\"  {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "CUDA version: 11.8\n",
      "GPU test time: 0.019s\n"
     ]
    }
   ],
   "source": [
    "# Quick test to check if GPU is available\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Test GPU speed\n",
    "    x = torch.randn(32, 3, 224, 224).cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = x * 2\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU test time: {time.time() - start:.3f}s\")\n",
    "else:\n",
    "    print(\"WARNING: Running on CPU will be VERY slow!\")\n",
    "    print(\"If you have a GPU, make sure CUDA is properly installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class that filters to CVCL training classes only\n",
    "class CVCLClassImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform, cvcl_classes):\n",
    "        print(f\"[DEBUG] Loading CSV from: {csv_path}\")\n",
    "        print(f\"[DEBUG] Image directory: {img_dir}\")\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        print(f\"[DEBUG] CSV loaded with {len(self.df)} rows\")\n",
    "        \n",
    "        # Filter to only CVCL training classes\n",
    "        self.df = self.df[self.df['class'].isin(cvcl_classes)].reset_index(drop=True)\n",
    "        print(f\"[DEBUG] Filtered to {len(self.df)} rows for CVCL training classes\")\n",
    "        \n",
    "        # Handle missing ball and bread in master_labels.csv\n",
    "        missing_classes = set(cvcl_classes) - set(self.df['class'].unique())\n",
    "        if missing_classes:\n",
    "            print(f\"[DEBUG] Adding missing classes: {missing_classes}\")\n",
    "            for cls in missing_classes:\n",
    "                # Add images from ball_color and bread_color folders\n",
    "                folder = f\"{cls}_color\"\n",
    "                folder_path = os.path.join(img_dir, folder)\n",
    "                if os.path.exists(folder_path):\n",
    "                    image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "                    for img_file in image_files:\n",
    "                        # Parse filename to extract metadata\n",
    "                        parts = img_file.replace('.png', '').split('_')\n",
    "                        if len(parts) >= 5:\n",
    "                            new_row = {\n",
    "                                'folder': folder,\n",
    "                                'filename': img_file,\n",
    "                                'class': cls,\n",
    "                                'color': '_'.join(parts[4:]),\n",
    "                                'size': parts[1],\n",
    "                                'texture': parts[2],\n",
    "                                'variant': parts[3]\n",
    "                            }\n",
    "                            self.df = pd.concat([self.df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "        print(f\"[DEBUG] Final dataset has {len(self.df)} rows\")\n",
    "        print(f\"[DEBUG] Classes in dataset: {sorted(self.df['class'].unique())}\")\n",
    "        \n",
    "        assert 'filename' in self.df and 'class' in self.df and 'folder' in self.df, \\\n",
    "            \"CSV needs filename, class, and folder columns\"\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Pre-compute paths\n",
    "        self.paths = [os.path.join(img_dir, row['folder'], row['filename']) \n",
    "                      for _, row in self.df.iterrows()]\n",
    "        \n",
    "        # Pre-filter to only valid images\n",
    "        valid_indices = []\n",
    "        for idx, path in enumerate(self.paths):\n",
    "            if os.path.exists(path):\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        print(f\"[DEBUG] Found {len(valid_indices)} valid images\")\n",
    "        \n",
    "        # Filter dataframe and paths to only valid images\n",
    "        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.paths = [self.paths[i] for i in valid_indices]\n",
    "        print(f\"Dataset initialized with {len(self.paths)} valid images from {len(self.df['class'].unique())} CVCL classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        cls = row['class']\n",
    "        path = self.paths[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load image at index {idx}: {path}\")\n",
    "            img = Image.new('RGB', (224, 224), color='white')\n",
    "        return self.transform(img), cls, idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    classes = [b[1] for b in batch]\n",
    "    idxs = [b[2] for b in batch]\n",
    "    return imgs, classes, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cvcl_class_text_vision_test(model_name, seed=0, device=None, batch_size=16, \n",
    "                                    trials_per_class=None, max_trials=4000):\n",
    "    \"\"\"\n",
    "    Run 4-way classification test using text encoders on CVCL training classes only.\n",
    "    Uses text descriptions as prototypes instead of averaging image features.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    if device == 'cpu':\n",
    "        print(\"[WARNING] Running on CPU - this will be SLOW!\")\n",
    "        print(\"Reducing batch size to 4 for CPU\")\n",
    "        batch_size = 4\n",
    "    else:\n",
    "        print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Check if model supports text encoding\n",
    "    if model_name in ['resnext', 'dino_s_resnext50']:\n",
    "        print(f\"[WARNING] {model_name} has no text encoder, skipping\")\n",
    "        return {}, 0.0\n",
    "\n",
    "    # 1) Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    print(f\"[INFO] Model loaded in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    # 2) Load dataset filtered to CVCL training classes\n",
    "    print(f\"[DEBUG] Creating dataset with CVCL training classes only\")\n",
    "    ds = CVCLClassImageDataset(CSV_PATH, IMG_DIR, transform, CVCL_TRAINING_CLASSES)\n",
    "    print(f\"[INFO] Dataset: {len(ds)} images from CVCL training classes\")\n",
    "    \n",
    "    if len(ds) == 0:\n",
    "        print(\"[ERROR] Dataset is empty! No valid images found.\")\n",
    "        return {}, 0.0\n",
    "    \n",
    "    # Use multiple workers only if not on Windows\n",
    "    num_workers = 0 if os.name == 'nt' else 2\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                   num_workers=num_workers, collate_fn=collate_fn, \n",
    "                   pin_memory=(device=='cuda'))\n",
    "    \n",
    "    # 3) Extract image embeddings\n",
    "    print(f\"[INFO] Extracting embeddings (batch_size={batch_size})...\")\n",
    "    all_img_embs, all_classes, all_idxs = [], [], []\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, classes, idxs in tqdm(dl, desc=\"Extracting embeddings\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            feats = extractor.get_img_feature(imgs)\n",
    "            feats = extractor.norm_features(feats).cpu()\n",
    "            \n",
    "            all_img_embs.append(feats)\n",
    "            all_classes.extend(classes)\n",
    "            all_idxs.extend(idxs)\n",
    "    \n",
    "    if not all_img_embs:\n",
    "        print(\"[ERROR] No embeddings extracted!\")\n",
    "        return {}, 0.0\n",
    "                \n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    print(f\"[INFO] Extracted {len(all_idxs)} embeddings in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    # 4) Encode text labels for CVCL training classes\n",
    "    unique_classes = list(set(all_classes))\n",
    "    print(f\"[INFO] Encoding {len(unique_classes)} CVCL training class labels...\")\n",
    "    \n",
    "    class_text_features = {}\n",
    "    with torch.no_grad():\n",
    "        if \"clip\" in model_name:\n",
    "            tokens = clip.tokenize(unique_classes, truncate=True).to(device)\n",
    "            txt_features = model.encode_text(tokens)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu()\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "        else:  # CVCL\n",
    "            tokens, token_len = model.tokenize(unique_classes)\n",
    "            tokens = tokens.to(device)\n",
    "            if isinstance(token_len, torch.Tensor):\n",
    "                token_len = token_len.to(device)\n",
    "            txt_features = model.encode_text(tokens, token_len)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu()\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "    \n",
    "    print(f\"[INFO] Text encoding complete\")\n",
    "    \n",
    "    # 5) Build mappings\n",
    "    idx2class = {i:c for i,c in zip(all_idxs, all_classes)}\n",
    "    idx2row = {i:r for r,i in enumerate(all_idxs)}\n",
    "    class2idxs = defaultdict(list)\n",
    "    for i,c in idx2class.items():\n",
    "        class2idxs[c].append(i)\n",
    "\n",
    "    # 6) Run trials - exactly max_trials\n",
    "    class_results = {}\n",
    "    total_correct = 0\n",
    "    total_trials = 0\n",
    "    \n",
    "    # Get valid classes (those with at least 1 image)\n",
    "    valid_classes = [c for c in class2idxs if len(class2idxs[c]) >= 1]\n",
    "    n_classes = len(valid_classes)\n",
    "    \n",
    "    # Calculate trials per class\n",
    "    if trials_per_class is None:\n",
    "        trials_per_class = max_trials // n_classes\n",
    "        extra_trials = max_trials % n_classes\n",
    "    else:\n",
    "        extra_trials = 0\n",
    "    \n",
    "    print(f\"[INFO] Running {max_trials} total trials across {n_classes} CVCL training classes\")\n",
    "    print(f\"[INFO] Base trials per class: {trials_per_class}, extra trials for first {extra_trials} classes\")\n",
    "    print(f\"[INFO] Using text-based prototypes (not visual averaging)\")\n",
    "    \n",
    "    # Run trials for each class\n",
    "    for class_idx, cls in enumerate(valid_classes):\n",
    "        if total_trials >= max_trials:\n",
    "            break\n",
    "            \n",
    "        idxs = class2idxs[cls]\n",
    "        \n",
    "        # Add extra trial for first few classes to reach exactly max_trials\n",
    "        current_trials = trials_per_class + (1 if class_idx < extra_trials else 0)\n",
    "        current_trials = min(current_trials, max_trials - total_trials)\n",
    "        \n",
    "        correct = 0\n",
    "        txt_feature = class_text_features[cls].unsqueeze(0)\n",
    "        \n",
    "        for trial in range(current_trials):\n",
    "            # Pick query from this class\n",
    "            q = random.choice(idxs)\n",
    "            \n",
    "            # Pick distractors from other classes\n",
    "            others = [i for i in all_idxs if idx2class[i] != cls]\n",
    "            if len(others) < 3:\n",
    "                continue\n",
    "            distractors = random.sample(others, 3)\n",
    "            \n",
    "            # 4-way classification\n",
    "            candidates = [q] + distractors\n",
    "            cand_features = torch.stack([all_img_embs[idx2row[i]] for i in candidates])\n",
    "            cand_features = cand_features.unsqueeze(0)\n",
    "            \n",
    "            # Compute similarity using text prototype\n",
    "            txt_feature_expanded = txt_feature.unsqueeze(1)\n",
    "            similarity = (100.0 * cand_features @ txt_feature_expanded.transpose(-2, -1)).softmax(dim=1)\n",
    "            similarity = similarity.squeeze()\n",
    "            \n",
    "            # Predict (query is at index 0)\n",
    "            if similarity.argmax().item() == 0:\n",
    "                correct += 1\n",
    "                total_correct += 1\n",
    "            total_trials += 1\n",
    "\n",
    "        acc = correct / current_trials if current_trials > 0 else 0\n",
    "        class_results[cls] = {'correct': correct, 'trials': current_trials, 'accuracy': acc}\n",
    "        \n",
    "        if total_trials % 500 == 0:\n",
    "            print(f\"[PROGRESS] {total_trials}/{max_trials} trials completed\")\n",
    "\n",
    "    # Final progress update\n",
    "    print(f\"[FINAL] Completed {total_trials} trials\")\n",
    "    \n",
    "    overall_acc = total_correct / total_trials if total_trials else 0.0\n",
    "    \n",
    "    print(f\"\\n[RESULTS] Accuracy: {total_correct}/{total_trials} ({overall_acc:.1%})\")\n",
    "    \n",
    "    # Save results\n",
    "    summary_df = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Test': 'Class-TextVision-CVCLTraining',\n",
    "        'Dataset': 'SyntheticKonkle_224',\n",
    "        'Num_Classes': n_classes,\n",
    "        'Correct': total_correct,\n",
    "        'Trials': total_trials,\n",
    "        'Accuracy': overall_acc\n",
    "    }])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(MASTER_CSV), exist_ok=True)\n",
    "    if os.path.exists(MASTER_CSV):\n",
    "        summary_df.to_csv(MASTER_CSV, mode='a', header=False, index=False, float_format='%.4f')\n",
    "    else:\n",
    "        summary_df.to_csv(MASTER_CSV, index=False, float_format='%.4f')\n",
    "\n",
    "    return class_results, overall_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVCL Text-Vision Test (CVCL Training Classes Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded in 2.4s\n",
      "[DEBUG] Creating dataset with CVCL training classes only\n",
      "[DEBUG] Loading CSV from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Image directory: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] CSV loaded with 7882 rows\n",
      "[DEBUG] Filtered to 2699 rows for CVCL training classes\n",
      "[DEBUG] Adding missing classes: {'ball'}\n",
      "[DEBUG] Final dataset has 2832 rows\n",
      "[DEBUG] Classes in dataset: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "[DEBUG] Found 2809 valid images\n",
      "Dataset initialized with 2809 valid images from 24 CVCL classes\n",
      "[INFO] Dataset: 2809 images from CVCL training classes\n",
      "[INFO] Extracting embeddings (batch_size=16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:08<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracted 2809 embeddings in 8.2s\n",
      "[INFO] Encoding 24 CVCL training class labels...\n",
      "[INFO] Text encoding complete\n",
      "[INFO] Running 4000 total trials across 24 CVCL training classes\n",
      "[INFO] Base trials per class: 166, extra trials for first 16 classes\n",
      "[INFO] Using text-based prototypes (not visual averaging)\n",
      "[PROGRESS] 4000/4000 trials completed\n",
      "[FINAL] Completed 4000 trials\n",
      "\n",
      "[RESULTS] Accuracy: 1198/4000 (29.9%)\n",
      "\n",
      "CVCL Results per Class (Top 20):\n",
      "ball                : 137/166 (82.5%)\n",
      "butterfly           : 101/167 (60.5%)\n",
      "phone               : 94/167 (56.3%)\n",
      "ring                : 85/167 (50.9%)\n",
      "basket              : 81/167 (48.5%)\n",
      "apple               : 61/167 (36.5%)\n",
      "bell                : 60/167 (35.9%)\n",
      "pillow              : 51/167 (30.5%)\n",
      "seashell            : 46/167 (27.5%)\n",
      "horse               : 45/166 (27.1%)\n",
      "tree                : 44/166 (26.5%)\n",
      "toothpaste          : 42/166 (25.3%)\n",
      "bird                : 40/167 (24.0%)\n",
      "rabbit              : 39/166 (23.5%)\n",
      "tricycle            : 38/166 (22.9%)\n",
      "bread               : 36/167 (21.6%)\n",
      "pumpkin             : 36/167 (21.6%)\n",
      "train               : 33/166 (19.9%)\n",
      "fan                 : 32/167 (19.2%)\n",
      "cookie              : 27/167 (16.2%)\n",
      "\n",
      "CVCL Overall Accuracy on Training Classes: 29.9%\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL text-vision classification on its training classes\n",
    "cvcl_results, cvcl_overall = run_cvcl_class_text_vision_test('cvcl-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCVCL Results per Class (Top 20):\")\n",
    "for cls, res in sorted(cvcl_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCVCL Overall Accuracy on Training Classes: {cvcl_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Text-Vision Test (CVCL Training Classes Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "[INFO] Model loaded in 1.3s\n",
      "[DEBUG] Creating dataset with CVCL training classes only\n",
      "[DEBUG] Loading CSV from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Image directory: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] CSV loaded with 7882 rows\n",
      "[DEBUG] Filtered to 2699 rows for CVCL training classes\n",
      "[DEBUG] Adding missing classes: {'ball'}\n",
      "[DEBUG] Final dataset has 2832 rows\n",
      "[DEBUG] Classes in dataset: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "[DEBUG] Found 2809 valid images\n",
      "Dataset initialized with 2809 valid images from 24 CVCL classes\n",
      "[INFO] Dataset: 2809 images from CVCL training classes\n",
      "[INFO] Extracting embeddings (batch_size=16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/176 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:06<00:00, 27.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracted 2809 embeddings in 6.5s\n",
      "[INFO] Encoding 24 CVCL training class labels...\n",
      "[INFO] Text encoding complete\n",
      "[INFO] Running 4000 total trials across 24 CVCL training classes\n",
      "[INFO] Base trials per class: 166, extra trials for first 16 classes\n",
      "[INFO] Using text-based prototypes (not visual averaging)\n",
      "[PROGRESS] 4000/4000 trials completed\n",
      "[FINAL] Completed 4000 trials\n",
      "\n",
      "[RESULTS] Accuracy: 3425/4000 (85.6%)\n",
      "\n",
      "CLIP Results per Class (Top 20):\n",
      "bird                : 167/167 (100.0%)\n",
      "phone               : 167/167 (100.0%)\n",
      "rabbit              : 166/166 (100.0%)\n",
      "tree                : 166/166 (100.0%)\n",
      "apple               : 165/167 (98.8%)\n",
      "ball                : 164/166 (98.8%)\n",
      "butterfly           : 163/167 (97.6%)\n",
      "horse               : 162/166 (97.6%)\n",
      "pumpkin             : 162/167 (97.0%)\n",
      "pen                 : 161/167 (96.4%)\n",
      "tricycle            : 158/166 (95.2%)\n",
      "bell                : 154/167 (92.2%)\n",
      "pillow              : 154/167 (92.2%)\n",
      "ring                : 153/167 (91.6%)\n",
      "bagel               : 148/167 (88.6%)\n",
      "stool               : 145/166 (87.3%)\n",
      "camera              : 138/167 (82.6%)\n",
      "seashell            : 134/167 (80.2%)\n",
      "train               : 129/166 (77.7%)\n",
      "bread               : 106/167 (63.5%)\n",
      "\n",
      "CLIP Overall Accuracy on CVCL Training Classes: 85.6%\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP text-vision classification on CVCL training classes\n",
    "clip_results, clip_overall = run_cvcl_class_text_vision_test('clip-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCLIP Results per Class (Top 20):\")\n",
    "for cls, res in sorted(clip_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCLIP Overall Accuracy on CVCL Training Classes: {clip_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED CLASS-BY-CLASS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Classes where CVCL text encoder performs better:\n",
      "  None\n",
      "\n",
      "Classes where CLIP text encoder performs better:\n",
      "  ball                : CLIP 98.8% vs CVCL 82.5% (+16.3%)\n",
      "  cookie              : CLIP 49.1% vs CVCL 16.2% (+32.9%)\n",
      "  toothpaste          : CLIP 60.2% vs CVCL 25.3% (+34.9%)\n",
      "  butterfly           : CLIP 97.6% vs CVCL 60.5% (+37.1%)\n",
      "  fan                 : CLIP 59.9% vs CVCL 19.2% (+40.7%)\n",
      "  ring                : CLIP 91.6% vs CVCL 50.9% (+40.7%)\n",
      "  bread               : CLIP 63.5% vs CVCL 21.6% (+41.9%)\n",
      "  phone               : CLIP 100.0% vs CVCL 56.3% (+43.7%)\n",
      "  seashell            : CLIP 80.2% vs CVCL 27.5% (+52.7%)\n",
      "  bell                : CLIP 92.2% vs CVCL 35.9% (+56.3%)\n",
      "\n",
      "Summary:\n",
      "  CVCL text encoder better on 0 classes\n",
      "  CLIP text encoder better on 23 classes\n",
      "  Similar performance on 1 classes\n"
     ]
    }
   ],
   "source": [
    "# Detailed per-class comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASS-BY-CLASS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "for cls in CVCL_TRAINING_CLASSES:\n",
    "    if cls in cvcl_results and cls in clip_results:\n",
    "        cvcl_acc = cvcl_results[cls]['accuracy']\n",
    "        clip_acc = clip_results[cls]['accuracy']\n",
    "        diff = cvcl_acc - clip_acc\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Class': cls,\n",
    "            'CVCL_Accuracy': cvcl_acc,\n",
    "            'CLIP_Accuracy': clip_acc,\n",
    "            'Difference': diff,\n",
    "            'Better_Model': 'CVCL' if diff > 0 else ('CLIP' if diff < 0 else 'Tie')\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Difference', ascending=False)\n",
    "\n",
    "print(\"\\nClasses where CVCL text encoder performs better:\")\n",
    "cvcl_better = comparison_df[comparison_df['Difference'] > 0.05]\n",
    "if len(cvcl_better) > 0:\n",
    "    for _, row in cvcl_better.iterrows():\n",
    "        print(f\"  {row['Class']:20s}: CVCL {row['CVCL_Accuracy']:.1%} vs CLIP {row['CLIP_Accuracy']:.1%} (+{row['Difference']:.1%})\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print(\"\\nClasses where CLIP text encoder performs better:\")\n",
    "clip_better = comparison_df[comparison_df['Difference'] < -0.05]\n",
    "if len(clip_better) > 0:\n",
    "    for _, row in clip_better.head(10).iterrows():  # Show top 10\n",
    "        print(f\"  {row['Class']:20s}: CLIP {row['CLIP_Accuracy']:.1%} vs CVCL {row['CVCL_Accuracy']:.1%} (+{abs(row['Difference']):.1%})\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  CVCL text encoder better on {len(cvcl_better)} classes\")\n",
    "print(f\"  CLIP text encoder better on {len(clip_better)} classes\")\n",
    "print(f\"  Similar performance on {len(comparison_df) - len(cvcl_better) - len(clip_better)} classes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
