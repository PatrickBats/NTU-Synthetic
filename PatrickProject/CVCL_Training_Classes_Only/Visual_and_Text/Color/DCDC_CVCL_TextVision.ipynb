{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCDC Text-Vision Test - CVCL Training Classes Only\\n\\nDifferent Class Different Color\\n\\n**This version only tests on the 25 classes that appear in CVCL's training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "CVCL classes file: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\CVCL_Konkle_Overlap\\CVCLKonkMatches.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "CVCL_CLASSES_PATH = os.path.join(REPO_ROOT, 'data', 'CVCL_Konkle_Overlap', 'CVCLKonkMatches.csv')\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'cvcl_training_text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"CVCL classes file: {CVCL_CLASSES_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVCL Training Classes (24):\n",
      "  ball\n",
      "  butterfly\n",
      "  phone\n",
      "  bagel\n",
      "  basket\n",
      "  bell\n",
      "  fan\n",
      "  seashell\n",
      "  bird\n",
      "  stool\n",
      "  train\n",
      "  ring\n",
      "  tricycle\n",
      "  toothpaste\n",
      "  pen\n",
      "  tree\n",
      "  apple\n",
      "  cookie\n",
      "  bread\n",
      "  pumpkin\n",
      "  camera\n",
      "  rabbit\n",
      "  pillow\n",
      "  horse\n"
     ]
    }
   ],
   "source": [
    "# Load CVCL training classes\n",
    "cvcl_df = pd.read_csv(CVCL_CLASSES_PATH)\n",
    "CVCL_TRAINING_CLASSES = cvcl_df['Class'].str.strip().tolist()\n",
    "\n",
    "print(f\"CVCL Training Classes ({len(CVCL_TRAINING_CLASSES)}):\")\n",
    "for cls in CVCL_TRAINING_CLASSES:\n",
    "    print(f\"  {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "\n",
      "Sample data:\n",
      "   class   color   size texture\n",
      "0  apple     red  large   bumpy\n",
      "1  apple   green  large   bumpy\n",
      "2  apple    blue  large   bumpy\n",
      "3  apple  yellow  large   bumpy\n",
      "4  apple  orange  large   bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data - FILTERED TO CVCL TRAINING CLASSES\n",
    "def load_cvcl_synthetickonkle_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset filtered to CVCL training classes\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Filter to only CVCL training classes\n",
    "    df = df[df['class'].isin(CVCL_TRAINING_CLASSES)].copy()\n",
    "    \n",
    "    # Handle missing ball and bread\n",
    "    missing_classes = set(CVCL_TRAINING_CLASSES) - set(df['class'].unique())\n",
    "    if missing_classes:\n",
    "        print(f\"Adding missing classes from folders: {missing_classes}\")\n",
    "        for cls in missing_classes:\n",
    "            folder = f\"{cls}_color\"\n",
    "            folder_path = os.path.join(DATA_PATH, folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "                for img_file in image_files:\n",
    "                    # Parse filename to extract metadata\n",
    "                    parts = img_file.replace('.png', '').split('_')\n",
    "                    if len(parts) >= 5:\n",
    "                        new_row = {\n",
    "                            'folder': folder,\n",
    "                            'filename': img_file,\n",
    "                            'class': cls,\n",
    "                            'color': '_'.join(parts[4:]),\n",
    "                            'size': parts[1],\n",
    "                            'texture': parts[2],\n",
    "                            'variant': parts[3]\n",
    "                        }\n",
    "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid metadata\n",
    "    df = df[df['color'].notna() & (df['color'] != '')].copy()\n",
    "    df = df[df['size'].notna() & (df['size'] != '')].copy()\n",
    "    df = df[df['texture'].notna() & (df['texture'] != '')].copy()\n",
    "    \n",
    "    # Standardize names (lowercase)\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images from {df['class'].nunique()} CVCL training classes\")\n",
    "    print(f\"Classes: {sorted(df['class'].unique())}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    print(f\"Unique textures: {df['texture'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data_df = load_cvcl_synthetickonkle_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'size', 'texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dcdc_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run DCDC text-vision test on CVCL training classes only\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running DCDC Text-Vision Test with {model_name}\")\n",
    "    print(f\"CVCL Training Classes Only\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data (already filtered to CVCL training classes)\n",
    "    df = load_cvcl_synthetickonkle_data()\n",
    "    \n",
    "    # Create class-color combination column\n",
    "    df['class_color'] = df['class'] + '_' + df['color']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with color annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique class-color combinations: {df['class_color'].nunique()}\")\n",
    "    \n",
    "    # Group data by class-color combinations\n",
    "    grouped = df.groupby('class_color').agg({\n",
    "        'image_path': list,\n",
    "        'class': 'first',\n",
    "        'color': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Filter to combinations with at least 1 image\n",
    "    grouped = grouped[grouped['image_path'].apply(len) >= 1].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nUsing {len(grouped)} unique class-color combinations\")\n",
    "    \n",
    "    # Pre-compute all image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Process in batches\n",
    "    all_image_paths = [img for imgs in grouped['image_path'] for img in imgs]\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            if img_path not in image_embeddings:  # Skip if already processed\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                    batch_images.append((img_path, img_processed))\n",
    "                except Exception as e:\n",
    "                    # Skip corrupted/invalid images\n",
    "                    skipped_images.append(img_path)\n",
    "                    continue\n",
    "        \n",
    "        if batch_images:\n",
    "            # Stack batch\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            # Store\n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()  # Ensure float32\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Update grouped data to only include valid images\n",
    "    for idx, row in grouped.iterrows():\n",
    "        valid_paths = [p for p in row['image_path'] if p in image_embeddings]\n",
    "        grouped.at[idx, 'image_path'] = valid_paths\n",
    "    \n",
    "    # Filter out combinations with no valid images\n",
    "    grouped = grouped[grouped['image_path'].apply(len) > 0].reset_index(drop=True)\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per combination (ensure we get exactly num_trials)\n",
    "    combinations_list = grouped['class_color'].tolist()\n",
    "    trials_per_combo = num_trials // len(combinations_list)\n",
    "    remaining_trials = num_trials % len(combinations_list)\n",
    "    \n",
    "    # Create trial distribution\n",
    "    trial_distribution = []\n",
    "    for i, combo in enumerate(combinations_list):\n",
    "        n_trials = trials_per_combo + (1 if i < remaining_trials else 0)\n",
    "        trial_distribution.extend([combo] * n_trials)\n",
    "    \n",
    "    # Shuffle trials\n",
    "    random.shuffle(trial_distribution)\n",
    "    \n",
    "    print(f\"\\nRunning {len(trial_distribution)} trials...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for trial_idx in tqdm(range(len(trial_distribution)), desc=\"Trials\"):\n",
    "        # Get query class-color\n",
    "        query_combo = trial_distribution[trial_idx]\n",
    "        query_data = grouped[grouped['class_color'] == query_combo].iloc[0]\n",
    "        query_class = query_data['class']\n",
    "        query_color = query_data['color']\n",
    "        \n",
    "        # Select random query image from valid images\n",
    "        valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "        if not valid_query_paths:\n",
    "            continue\n",
    "        query_img_path = random.choice(valid_query_paths)\n",
    "        \n",
    "        # Select 3 distractors (different class AND different color)\n",
    "        valid_distractors = grouped[\n",
    "            (grouped['class'] != query_class) & \n",
    "            (grouped['color'] != query_color)\n",
    "        ]\n",
    "        \n",
    "        # Filter distractors to those with valid images\n",
    "        valid_distractors = valid_distractors[valid_distractors['image_path'].apply(len) > 0]\n",
    "        \n",
    "        if len(valid_distractors) < 3:\n",
    "            continue  # Skip if not enough valid distractors\n",
    "        \n",
    "        distractor_combos = valid_distractors.sample(n=3)['class_color'].tolist()\n",
    "        \n",
    "        # Create candidate list (query + distractors)\n",
    "        all_combos = [query_combo] + distractor_combos\n",
    "        random.shuffle(all_combos)\n",
    "        \n",
    "        # Get correct index\n",
    "        correct_idx = all_combos.index(query_combo)\n",
    "        \n",
    "        # Select random images for each candidate\n",
    "        candidate_imgs = []\n",
    "        candidate_texts = []\n",
    "        skip_trial = False\n",
    "        \n",
    "        for combo in all_combos:\n",
    "            combo_data = grouped[grouped['class_color'] == combo].iloc[0]\n",
    "            valid_paths = [p for p in combo_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_paths:\n",
    "                skip_trial = True\n",
    "                break\n",
    "            img_path = random.choice(valid_paths)\n",
    "            candidate_imgs.append(img_path)\n",
    "            \n",
    "            # Create text prompt with color + class\n",
    "            text_prompt = f\"{combo_data['color']} {combo_data['class'].lower()}\"\n",
    "            candidate_texts.append(text_prompt)\n",
    "        \n",
    "        if skip_trial:\n",
    "            continue\n",
    "        \n",
    "        # Encode text prompts (batch encoding)\n",
    "        with torch.no_grad():\n",
    "            if \"clip\" in model_name:\n",
    "                tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                txt_features = model.encode_text(tokens)\n",
    "                txt_features = extractor.norm_features(txt_features)\n",
    "            else:  # CVCL\n",
    "                tokens, token_len = model.tokenize(candidate_texts)\n",
    "                tokens = tokens.to(device)\n",
    "                if isinstance(token_len, torch.Tensor):\n",
    "                    token_len = token_len.to(device)\n",
    "                txt_features = model.encode_text(tokens, token_len)\n",
    "                txt_features = extractor.norm_features(txt_features)\n",
    "        \n",
    "        # Get query image embedding\n",
    "        query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Calculate similarity following original predict() method\n",
    "        # Ensure both are float32\n",
    "        query_embedding = query_embedding.float()\n",
    "        txt_features = txt_features.float()\n",
    "        \n",
    "        similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_idx = similarity.argmax(dim=1).item()\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (pred_idx == correct_idx)\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        # Store trial result\n",
    "        trial_results.append({\n",
    "            'trial': trial_idx + 1,\n",
    "            'query_class': query_class,\n",
    "            'query_color': query_color,\n",
    "            'query_img': os.path.basename(query_img_path),\n",
    "            'correct_idx': correct_idx,\n",
    "            'predicted_idx': pred_idx,\n",
    "            'correct': is_correct,\n",
    "            'candidate_texts': candidate_texts,\n",
    "            'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - DCDC Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'DCDC-TextVision-CVCLTraining',\n",
    "        'Dataset': 'SyntheticKonkle_224',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    # Append to results file\n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL DCDC Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDC Text-Vision Test with cvcl-resnext\n",
      "CVCL Training Classes Only\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "Loaded 2832 images with color annotations\n",
      "Unique classes: 24\n",
      "Unique colors: 12\n",
      "Unique class-color combinations: 243\n",
      "\n",
      "Using 243 unique class-color combinations\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 177/177 [00:08<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 2806 images\n",
      "Skipped 23 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trials: 100%|██████████| 4000/4000 [00:42<00:00, 93.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - DCDC Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1197\n",
      "Accuracy: 0.2993 (29.93%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test\n",
    "cvcl_trials, cvcl_accuracy = run_dcdc_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP DCDC Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDC Text-Vision Test with clip-resnext\n",
      "CVCL Training Classes Only\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "Loaded 2832 images with color annotations\n",
      "Unique classes: 24\n",
      "Unique colors: 12\n",
      "Unique class-color combinations: 243\n",
      "\n",
      "Using 243 unique class-color combinations\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/177 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 177/177 [00:06<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 2806 images\n",
      "Skipped 23 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trials: 100%|██████████| 4000/4000 [00:26<00:00, 149.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - DCDC Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3957\n",
      "Accuracy: 0.9892 (98.92%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test\n",
    "clip_trials, clip_accuracy = run_dcdc_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DCDC TEXT-VISION TEST COMPARISON - CVCL TRAINING CLASSES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}% on its training classes\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}% even on CVCL's training classes\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
