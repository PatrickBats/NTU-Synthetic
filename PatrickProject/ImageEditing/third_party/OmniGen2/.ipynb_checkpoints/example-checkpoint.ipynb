{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/miniconda3/envs/omnigen2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/attention_processor.py:33: UserWarning: Cannot import flash_attn, install flash_attn to use Flash2Varlen attention for better performance\n",
      "  warnings.warn(\"Cannot import flash_attn, install flash_attn to use Flash2Varlen attention for better performance\")\n",
      "/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/transformers/block_lumina2.py:37: UserWarning: Cannot import flash_attn, install flash_attn to use fused SwiGLU for better performance\n",
      "  warnings.warn(\"Cannot import flash_attn, install flash_attn to use fused SwiGLU for better performance\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import accelerate\n",
    "from pathlib import Path\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(root_dir)\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline\n",
    "from omnigen2.models.transformers.transformer_omnigen2 import OmniGen2Transformer2DModel\n",
    "from omnigen2.utils.img_util import create_collage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def preprocess(input_image_path: Union[str, List[str], None] = None) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Preprocess the input images by:\n",
    "    - Accepting a single path, list of paths, or a directory\n",
    "    - Loading only common image files\n",
    "    - Correcting orientation via EXIF\n",
    "    - Converting to 3‑channel RGB (drops alpha)\n",
    "    \"\"\"\n",
    "    if input_image_path is None:\n",
    "        return []\n",
    "\n",
    "    # Normalize to a list of paths\n",
    "    if isinstance(input_image_path, str):\n",
    "        paths = [input_image_path]\n",
    "    else:\n",
    "        paths = input_image_path\n",
    "\n",
    "    images: List[Image.Image] = []\n",
    "    for p in paths:\n",
    "        if os.path.isdir(p):\n",
    "            for fname in os.listdir(p):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                    img = Image.open(os.path.join(p, fname))\n",
    "                    images.append(img)\n",
    "        else:\n",
    "            img = Image.open(p)\n",
    "            images.append(img)\n",
    "\n",
    "    # EXIF transpose + strip alpha channel\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        img = ImageOps.exif_transpose(img).convert(\"RGB\")\n",
    "        processed.append(img)\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't connect to the Hub: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/OmniGen2/OmniGen2 (Request ID: Root=1-6884c345-2d3d465e729ccd133d176715;cf66c58b-1864-44e1-835d-160794a89d7d)\n",
      "\n",
      "Invalid credentials in Authorization header.\n",
      "Will try to load from local cache.\n",
      "Keyword arguments {'trust_remote_code': True} are not expected by OmniGen2Pipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|                                                                                                                              | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████                                                           | 1/2 [00:01<00:01,  1.06s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.32it/s]\u001b[A\n",
      "Loading pipeline components...:  40%|█████████████████████████████████████████████▏                                                                   | 2/5 [00:01<00:02,  1.11it/s]\n",
      "Loading checkpoint shards:   0%|                                                                                                                              | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  25%|█████████████████████████████▌                                                                                        | 1/4 [00:00<00:01,  2.56it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████                                                           | 2/4 [00:00<00:00,  2.60it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      "Loading pipeline components...:  60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:02<00:02,  1.02s/it]Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading pipeline components...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.40it/s]\n",
      "Expected types for transformer: (<class 'omnigen2.models.transformers.transformer_omnigen2.OmniGen2Transformer2DModel'>,), got <class 'diffusers_modules.local.transformer_omnigen2.OmniGen2Transformer2DModel'>.\n",
      "Fetching 2 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20460.02it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "model_path=\"OmniGen2/OmniGen2\"\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    token=\"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\",\n",
    ")\n",
    "pipeline.transformer = OmniGen2Transformer2DModel.from_pretrained(\n",
    "    model_path,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process objects: ['ball', 'muffin', 'pitcher', 'tennisracquet', 'phone', 'headband', 'bagel', 'grill', 'basket', 'bell', 'sodacan', 'microwave', 'trophy', 'fan', 'lei', 'stapler', 'dumbell', 'handgun', 'seashell', 'powerstrip', 'lipstick', 'lantern', 'doorknob', 'abacus', 'jack-o-lantern', 'camcorder', 'bird', 'saddle', 'handbag', 'stool', 'toyrabbit', 'candleholderwithcandle', 'cushion', 'lock', 'train', 'bonzai', 'ring', 'goggle', 'trumpet', 'vase', 'axe', 'tricycle', 'toothpaste', 'nailpolish', 'calculator', 'pen', 'tree', 'earrings', 'toyhorse', 'gamehandheld', 'apple', 'babushkadolls', 'saltpeppershake', 'dresser', 'rug', 'wineglass', 'breadloaf', 'keyboard', 'meat', 'sippycup', 'cookie', 'helmet', 'doll', 'pipe', 'christmastreeornamantball', 'telescope', 'suitcase']\n",
      "\n",
      "=== Processing 'ball' ===\n",
      "✅ Done 'ball' — outputs in /home/patrick/ssd/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/synthetic_dataset/ball_color\n",
      "\n",
      "=== Processing 'muffin' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████████████▊                                                                                                 | 16/50 [00:28<01:00,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     90\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChange the object to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolour\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and make the background white\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m gen = torch.Generator(device=accelerator.device).manual_seed(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_imgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_guidance_scale\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_guidance_scale\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEG_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpil\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m result.images[\u001b[32m0\u001b[39m].save(out_path)\n\u001b[32m    106\u001b[39m csv_writer.writerow([out_name, size, texture, variant, colour, OBJECT_NAME])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/omnigen2/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/pipelines/omnigen2/pipeline_omnigen2.py:584\u001b[39m, in \u001b[36mOmniGen2Pipeline.__call__\u001b[39m\u001b[34m(self, prompt, negative_prompt, prompt_embeds, negative_prompt_embeds, prompt_attention_mask, negative_prompt_attention_mask, max_sequence_length, callback_on_step_end_tensor_inputs, input_images, num_images_per_prompt, height, width, max_pixels, max_input_image_side_length, align_res, num_inference_steps, text_guidance_scale, image_guidance_scale, cfg_range, attention_kwargs, timesteps, generator, latents, output_type, return_dict, verbose, step_func)\u001b[39m\n\u001b[32m    567\u001b[39m latents = \u001b[38;5;28mself\u001b[39m.prepare_latents(\n\u001b[32m    568\u001b[39m     batch_size * num_images_per_prompt,\n\u001b[32m    569\u001b[39m     latent_channels,\n\u001b[32m   (...)\u001b[39m\u001b[32m    575\u001b[39m     latents,\n\u001b[32m    576\u001b[39m )\n\u001b[32m    578\u001b[39m freqs_cis = OmniGen2RotaryPosEmbed.get_freqs_cis(\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m.transformer.config.axes_dim_rope,\n\u001b[32m    580\u001b[39m     \u001b[38;5;28mself\u001b[39m.transformer.config.axes_lens,\n\u001b[32m    581\u001b[39m     theta=\u001b[32m10000\u001b[39m,\n\u001b[32m    582\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_latents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m image = F.interpolate(image, size=(ori_height, ori_width), mode=\u001b[33m'\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    602\u001b[39m image = \u001b[38;5;28mself\u001b[39m.image_processor.postprocess(image, output_type=output_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/pipelines/omnigen2/pipeline_omnigen2.py:661\u001b[39m, in \u001b[36mOmniGen2Pipeline.processing\u001b[39m\u001b[34m(self, latents, ref_latents, prompt_embeds, freqs_cis, negative_prompt_embeds, prompt_attention_mask, negative_prompt_attention_mask, num_inference_steps, timesteps, device, dtype, verbose, step_func)\u001b[39m\n\u001b[32m    658\u001b[39m     teacache_params.is_first_or_last_step = i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(timesteps) - \u001b[32m1\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28mself\u001b[39m.transformer.teacache_params = teacache_params\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m model_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_image_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m text_guidance_scale = \u001b[38;5;28mself\u001b[39m.text_guidance_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg_range[\u001b[32m0\u001b[39m] <= i / \u001b[38;5;28mlen\u001b[39m(timesteps) <= \u001b[38;5;28mself\u001b[39m.cfg_range[\u001b[32m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1.0\u001b[39m\n\u001b[32m    670\u001b[39m image_guidance_scale = \u001b[38;5;28mself\u001b[39m.image_guidance_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg_range[\u001b[32m0\u001b[39m] <= i / \u001b[38;5;28mlen\u001b[39m(timesteps) <= \u001b[38;5;28mself\u001b[39m.cfg_range[\u001b[32m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/pipelines/omnigen2/pipeline_omnigen2.py:766\u001b[39m, in \u001b[36mOmniGen2Pipeline.predict\u001b[39m\u001b[34m(self, t, latents, prompt_embeds, freqs_cis, prompt_attention_mask, ref_image_hidden_states)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mref_image_hidden_states\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(inspect.signature(\u001b[38;5;28mself\u001b[39m.transformer.forward).parameters.keys()):\n\u001b[32m    764\u001b[39m     optional_kwargs[\u001b[33m'\u001b[39m\u001b[33mref_image_hidden_states\u001b[39m\u001b[33m'\u001b[39m] = ref_image_hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m model_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptional_kwargs\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/omnigen2/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/omnigen2/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/transformers/transformer_omnigen2.py:608\u001b[39m, in \u001b[36mOmniGen2Transformer2DModel.forward\u001b[39m\u001b[34m(self, hidden_states, timestep, text_hidden_states, freqs_cis, text_attention_mask, ref_image_hidden_states, attention_kwargs, return_dict)\u001b[39m\n\u001b[32m    588\u001b[39m temb, text_hidden_states = \u001b[38;5;28mself\u001b[39m.time_caption_embed(timestep, text_hidden_states, hidden_states[\u001b[32m0\u001b[39m].dtype)\n\u001b[32m    590\u001b[39m (\n\u001b[32m    591\u001b[39m     hidden_states,\n\u001b[32m    592\u001b[39m     ref_image_hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m     img_sizes,\n\u001b[32m    599\u001b[39m ) = \u001b[38;5;28mself\u001b[39m.flat_and_pad_to_seq(hidden_states, ref_image_hidden_states)\n\u001b[32m    601\u001b[39m (\n\u001b[32m    602\u001b[39m     context_rotary_emb,\n\u001b[32m    603\u001b[39m     ref_img_rotary_emb,\n\u001b[32m    604\u001b[39m     noise_rotary_emb,\n\u001b[32m    605\u001b[39m     rotary_emb,\n\u001b[32m    606\u001b[39m     encoder_seq_lengths,\n\u001b[32m    607\u001b[39m     seq_lengths,\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrope_embedder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_effective_ref_img_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_effective_img_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_img_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# 2. Context refinement\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.context_refiner:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/omnigen2/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/omnigen2/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/transformers/repo.py:57\u001b[39m, in \u001b[36mOmniGen2RotaryPosEmbed.forward\u001b[39m\u001b[34m(self, freqs_cis, attention_mask, l_effective_ref_img_len, l_effective_img_len, ref_img_sizes, img_sizes, device)\u001b[39m\n\u001b[32m     54\u001b[39m p = \u001b[38;5;28mself\u001b[39m.patch_size\n\u001b[32m     56\u001b[39m encoder_seq_len = attention_mask.shape[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m l_effective_cap_len = \u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m seq_lengths = [cap_len + \u001b[38;5;28msum\u001b[39m(ref_img_len) + img_len \u001b[38;5;28;01mfor\u001b[39;00m cap_len, ref_img_len, img_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(l_effective_cap_len, l_effective_ref_img_len, l_effective_img_len)]\n\u001b[32m     61\u001b[39m max_seq_len = \u001b[38;5;28mmax\u001b[39m(seq_lengths)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import csv, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# --- User‑level config ----------------------------------------\n",
    "COLORS = [\"red\",\"green\",\"blue\",\"yellow\",\"orange\",\n",
    "          \"purple\",\"pink\",\"brown\",\"black\",\"grey\"]\n",
    "\n",
    "NEG_PROMPT = (\n",
    "    \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, \"\n",
    "    \"mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, \"\n",
    "    \"messy drawing, broken legs, censor, censored, censor_bar\"\n",
    ")\n",
    "\n",
    "# Root of your synthetic_dataset folder\n",
    "SYNTHETIC_DIR = Path(\n",
    "    \"/home/patrick/ssd/discover-hidden-visual-concepts/PatrickProject/\"\n",
    "    \"ImageEditing/third_party/OmniGen2/synthetic_dataset\"\n",
    ")\n",
    "\n",
    "# CSV listing which objects to process (header: \"class\")\n",
    "OBJECTS_CSV = SYNTHETIC_DIR / \"objectstorun.csv\"\n",
    "\n",
    "\n",
    "# --- Read list of object names from CSV ----------------------\n",
    "object_names = []\n",
    "with OBJECTS_CSV.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        name = row.get(\"class\", \"\").strip()\n",
    "        if not name or name.lower() == \"class\":\n",
    "            continue\n",
    "        # strip literal \"_bases\" if present\n",
    "        if name.endswith(\"_bases\"):\n",
    "            name = name[:-len(\"_bases\")]\n",
    "        object_names.append(name)\n",
    "\n",
    "print(\"Will process objects:\", object_names)\n",
    "\n",
    "\n",
    "# --- Main loop: one pass per object ---------------------------\n",
    "for OBJECT_NAME in object_names:\n",
    "    print(f\"\\n=== Processing '{OBJECT_NAME}' ===\")\n",
    "\n",
    "    # 1) Locate bases and make sure they exist\n",
    "    BASE_DIR = SYNTHETIC_DIR / f\"{OBJECT_NAME}_bases\"\n",
    "    pngs = sorted(BASE_DIR.glob(\"*.png\"))\n",
    "    if not BASE_DIR.exists() or not pngs:\n",
    "        print(f\"⚠️  No bases found at {BASE_DIR}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2) Prepare output folder <object>_color\n",
    "    COLOR_DIR = SYNTHETIC_DIR / f\"{OBJECT_NAME}_color\"\n",
    "    COLOR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # 3) CSV for this object\n",
    "    CSV_PATH = COLOR_DIR / \"labels.csv\"\n",
    "    write_header = not CSV_PATH.exists()\n",
    "    csv_file = CSV_PATH.open(\"a\", newline=\"\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    if write_header:\n",
    "        csv_writer.writerow([\"filename\", \"size\", \"texture\", \"variant\", \"colour\", \"class\"])\n",
    "\n",
    "    # 4) Process each base\n",
    "    for base_png in pngs:\n",
    "        stem = base_png.stem\n",
    "        if not stem.startswith(\"base_\"):\n",
    "            # safety check\n",
    "            print(\"⚠️  skipping unexpected file:\", base_png.name)\n",
    "            continue\n",
    "\n",
    "        # remove \"base_\" prefix\n",
    "        rest = stem[len(\"base_\"):]           # e.g. \"small_extra_bumpy_01\"\n",
    "        parts = rest.split(\"_\")\n",
    "        size = parts[0]                      # \"small\"\n",
    "        variant = parts[-1]                  # \"01\"\n",
    "        texture = \"_\".join(parts[1:-1])      # \"extra_bumpy\" (handles any number of underscores)\n",
    "\n",
    "        # your existing preprocess function\n",
    "        input_imgs = preprocess(str(base_png))\n",
    "\n",
    "        # one output per colour\n",
    "        for colour in COLORS:\n",
    "            out_name = f\"{OBJECT_NAME}_{size}_{texture}_{variant}_{colour}.png\"\n",
    "            out_path = COLOR_DIR / out_name\n",
    "\n",
    "            if out_path.exists():\n",
    "                continue  # skip existing\n",
    "\n",
    "            prompt = f\"Change the object to {colour} and make the background white\"\n",
    "            gen = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "            result = pipeline(\n",
    "                prompt                = prompt,\n",
    "                input_images          = input_imgs,\n",
    "                num_inference_steps   = 50,\n",
    "                max_sequence_length   = 1024,\n",
    "                text_guidance_scale   = 5.0,\n",
    "                image_guidance_scale  = 2.0,\n",
    "                negative_prompt       = NEG_PROMPT,\n",
    "                num_images_per_prompt = 1,\n",
    "                generator             = gen,\n",
    "                output_type           = \"pil\",\n",
    "            )\n",
    "\n",
    "            result.images[0].save(out_path)\n",
    "            csv_writer.writerow([out_name, size, texture, variant, colour, OBJECT_NAME])\n",
    "            print(\"✔\", out_name)\n",
    "\n",
    "    csv_file.close()\n",
    "    print(f\"✅ Done '{OBJECT_NAME}' — outputs in {COLOR_DIR}\")\n",
    "\n",
    "print(\"\\n🎉 All objects processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
