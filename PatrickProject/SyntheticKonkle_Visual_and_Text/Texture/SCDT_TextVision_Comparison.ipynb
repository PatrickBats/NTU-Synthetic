{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Class Different Texture (SCDT) Text-Vision Comparison - SyntheticKonkle\n",
    "\n",
    "This notebook compares CVCL and CLIP models on text-vision matching using the SyntheticKonkle dataset.\n",
    "The task is 4-way classification where distractors are from the SAME class but DIFFERENT textures.\n",
    "\n",
    "## Test Characteristics\n",
    "- **Visual**: All candidates from SAME class, with controlled color and size\n",
    "- **Variation**: Only texture differs between candidates (smooth vs bumpy)\n",
    "- **Control**: Color and size held constant visually but NOT mentioned in text\n",
    "- **Text format**: `\"{texture} {class}\"` (e.g., \"smooth apple\", \"bumpy apple\")\n",
    "- **Difficulty**: Very hard - only texture provides discrimination within same class\n",
    "\n",
    "## Special Note on 4-way Choice with 2 Textures\n",
    "Since there are only 2 texture values (smooth and bumpy), we create 4-way choice by:\n",
    "- Using multiple images of each texture\n",
    "- Duplicating texture values across the 4 choices (e.g., 2 smooth, 2 bumpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7865 images with texture annotations\n",
      "Unique classes: 67\n",
      "Unique textures: ['bumpy', 'smooth']\n",
      "Unique colors: 11\n",
      "Unique sizes: 3\n",
      "\n",
      "Class-Color-Size combinations with both textures: 1961\n",
      "Examples: ['abacus_black_large', 'abacus_black_medium', 'abacus_black_small']\n",
      "\n",
      "Sample data:\n",
      "    class texture   color   size\n",
      "0  abacus   bumpy     red  large\n",
      "1  abacus   bumpy   green  large\n",
      "2  abacus   bumpy    blue  large\n",
      "3  abacus   bumpy  yellow  large\n",
      "4  abacus   bumpy  orange  large\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_synthetic_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset with metadata for texture testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid texture values\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_size'] = df['class'] + '_' + df['color'] + '_' + df['size']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['size'] + '_' + df['texture']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    \n",
    "    # Find class-color-size combinations that have both textures\n",
    "    ccs_groups = df.groupby('class_color_size')['texture'].nunique()\n",
    "    valid_ccs = ccs_groups[ccs_groups == 2].index.tolist()  # Must have both smooth and bumpy\n",
    "    \n",
    "    print(f\"\\nClass-Color-Size combinations with both textures: {len(valid_ccs)}\")\n",
    "    if len(valid_ccs) > 0:\n",
    "        print(f\"Examples: {valid_ccs[:3]}\")\n",
    "    \n",
    "    return df, valid_ccs\n",
    "\n",
    "# Load data\n",
    "data_df, valid_combinations = load_synthetic_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'texture', 'color', 'size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scdt_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Same Class Different Texture text-vision test with controlled color and size\n",
    "    \n",
    "    Text format: \"{texture} {class}\"\n",
    "    Example: \"smooth apple\", \"bumpy apple\"\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDT Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Same Class Different Texture - Controlled Color & Size)\")\n",
    "    print(f\"Text format: {{texture}} {{class}}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid texture values\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_size'] = df['class'] + '_' + df['color'] + '_' + df['size']\n",
    "    \n",
    "    # Find class-color-size combinations with both textures\n",
    "    ccs_groups = df.groupby('class_color_size')\n",
    "    valid_ccs = []\n",
    "    for ccs, group in ccs_groups:\n",
    "        unique_textures = group['texture'].unique()\n",
    "        if len(unique_textures) == 2:  # Has both smooth and bumpy\n",
    "            valid_ccs.append(ccs)\n",
    "    \n",
    "    if len(valid_ccs) == 0:\n",
    "        print(\"ERROR: No class-color-size combinations have both textures.\")\n",
    "        print(\"Cannot run SCDT test.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_ccs)} class-color-size combinations with both textures\")\n",
    "    \n",
    "    # Pre-compute image embeddings\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['class_color_size'].isin(valid_ccs)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per combination\n",
    "    trials_per_ccs = num_trials // len(valid_ccs)\n",
    "    remaining_trials = num_trials % len(valid_ccs)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_ccs)} combinations...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for ccs_idx, ccs in enumerate(tqdm(valid_ccs, desc=\"Processing combinations\")):\n",
    "        # Get all images for this class-color-size combination\n",
    "        ccs_data = df_valid[df_valid['class_color_size'] == ccs]\n",
    "        \n",
    "        # Group by texture\n",
    "        texture_groups = ccs_data.groupby('texture')['image_path'].apply(list).to_dict()\n",
    "        \n",
    "        # Should have exactly 2 textures\n",
    "        if len(texture_groups) != 2:\n",
    "            continue\n",
    "        \n",
    "        # Parse class from combination string\n",
    "        class_name = ccs.split('_')[0]\n",
    "        \n",
    "        # Determine number of trials for this combination\n",
    "        n_trials = trials_per_ccs + (1 if ccs_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # For 4-way choice with only 2 textures, we duplicate each texture\n",
    "            # This creates: [smooth1, smooth2, bumpy1, bumpy2]\n",
    "            candidates = []\n",
    "            candidate_texts = []\n",
    "            \n",
    "            for texture in ['smooth', 'bumpy']:\n",
    "                if texture in texture_groups:\n",
    "                    # Get 2 different images for this texture if possible\n",
    "                    texture_images = [p for p in texture_groups[texture] if p in image_embeddings]\n",
    "                    if len(texture_images) >= 2:\n",
    "                        selected = random.sample(texture_images, 2)\n",
    "                    elif len(texture_images) == 1:\n",
    "                        selected = [texture_images[0], texture_images[0]]  # Duplicate if only 1\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    for img_path in selected:\n",
    "                        candidates.append((img_path, texture))\n",
    "                        candidate_texts.append(f\"{texture} {class_name.lower()}\")\n",
    "            \n",
    "            if len(candidates) != 4:\n",
    "                continue\n",
    "            \n",
    "            # Select query from candidates\n",
    "            query_idx = random.randint(0, 3)\n",
    "            query_img_path, query_texture = candidates[query_idx]\n",
    "            \n",
    "            # Shuffle candidates for random order\n",
    "            shuffled_order = list(range(4))\n",
    "            random.shuffle(shuffled_order)\n",
    "            shuffled_candidates = [candidates[i] for i in shuffled_order]\n",
    "            shuffled_texts = [candidate_texts[i] for i in shuffled_order]\n",
    "            correct_idx = shuffled_order.index(query_idx)\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(shuffled_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(shuffled_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': class_name,\n",
    "                'query_texture': query_texture,\n",
    "                'class_color_size': ccs,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': shuffled_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDT Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDT-TextVision',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDT Text-Vision Test with cvcl-resnext\n",
      "(Same Class Different Texture - Controlled Color & Size)\n",
      "Text format: {texture} {class}\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1961 class-color-size combinations with both textures\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 488/488 [00:21<00:00, 22.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7786 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 1961 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 1961/1961 [00:40<00:00, 48.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1167\n",
      "Accuracy: 0.2918 (29.18%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_scdt_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDT Text-Vision Test with clip-resnext\n",
      "(Same Class Different Texture - Controlled Color & Size)\n",
      "Text format: {texture} {class}\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 1961 class-color-size combinations with both textures\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/488 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 488/488 [00:17<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7786 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 1961 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 1961/1961 [00:21<00:00, 89.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1322\n",
      "Accuracy: 0.3305 (33.05%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_scdt_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDT TEXT-VISION TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Same Class Different Texture (4-way forced choice)\")\n",
    "print(f\"Control: Color and size held constant (not mentioned in text)\")\n",
    "print(f\"Text format: '{{texture}} {{class}}'\")\n",
    "print(f\"Example: 'smooth apple' vs 'bumpy apple' (2 of each texture)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Hardest single-attribute test: only texture differs\")\n",
    "print(\"- Texture is a subtle tactile property\")\n",
    "print(\"- 4-way choice with 2 textures (each appears twice)\")\n",
    "print(\"- Tests whether models encode tactile information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDT Text-Vision Test Characteristics:\n",
    "- **Visual Control**: All 4 candidates have same class, color, and size\n",
    "- **Variation**: Only texture differs (smooth vs bumpy)\n",
    "- **Text Prompts**: \"{texture} {class}\" (e.g., \"smooth apple\")\n",
    "- **4-way Implementation**: Each texture appears twice (2 smooth, 2 bumpy)\n",
    "\n",
    "### What This Tests:\n",
    "- Pure texture discrimination within same class\n",
    "- Model's ability to encode tactile/surface properties\n",
    "- Whether texture information transfers from vision to language\n",
    "\n",
    "### Expected Performance:\n",
    "- Likely the hardest test - texture is subtle\n",
    "- Performance may be near chance (25%) if models don't encode texture well\n",
    "- CVCL might have advantage if child-directed speech emphasizes texture\n",
    "\n",
    "### Why This Is Challenging:\n",
    "- Texture is primarily a tactile property\n",
    "- Visual texture cues can be subtle\n",
    "- Only 2 texture values limit discrimination\n",
    "- Same class constraint removes object-level cues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
