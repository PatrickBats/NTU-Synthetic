{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Prototype Per-Class Analysis (CVCL Training Classes Only)\n",
    "\n",
    "This notebook runs class discrimination tests using visual prototypes with controlled visual properties.\n",
    "**Only classes that appear in CVCL training data are tested** (based on CVCLKonkMatches.csv).\n",
    "Distractors are matched for size, color, and texture to ensure the model must rely on class identity.\n",
    "Multiple seeds are run to obtain confidence intervals for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# SyntheticKonkle paths - Using 224x224 resized images for faster processing\n",
    "DATA_DIR = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224')\n",
    "RESULTS_DIR = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CVCL training classes\n",
    "def load_cvcl_training_classes():\n",
    "    \"\"\"Load the list of classes that appear in CVCL training data.\"\"\"\n",
    "    cvcl_csv_path = os.path.join(REPO_ROOT, 'data', 'CVCL_Konkle_Overlap', 'CVCLKonkMatches.csv')\n",
    "    \n",
    "    print(f\"Loading CVCL training classes from: {cvcl_csv_path}\")\n",
    "    cvcl_df = pd.read_csv(cvcl_csv_path)\n",
    "    \n",
    "    # Get unique classes from the CSV\n",
    "    cvcl_classes = cvcl_df['Class'].unique().tolist()\n",
    "    \n",
    "    print(f\"Found {len(cvcl_classes)} CVCL training classes\")\n",
    "    print(f\"Classes: {', '.join(sorted(cvcl_classes))}\")\n",
    "    \n",
    "    return cvcl_classes\n",
    "\n",
    "# Load CVCL classes\n",
    "CVCL_TRAINING_CLASSES = load_cvcl_training_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup with CVCL class filtering\n",
    "def build_synthetic_dataset(filter_classes=None):\n",
    "    \"\"\"Load the master labels CSV with all visual properties.\n",
    "    \n",
    "    Args:\n",
    "        filter_classes: List of class names to include. If None, include all.\n",
    "    \"\"\"\n",
    "    # Use the master_labels.csv which has all the attribute information\n",
    "    master_csv = os.path.join(DATA_DIR, 'master_labels.csv')\n",
    "    \n",
    "    if not os.path.exists(master_csv):\n",
    "        print(f\"Warning: {master_csv} not found, trying alternative path...\")\n",
    "        # Try the original SyntheticKonkle folder\n",
    "        master_csv = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "    \n",
    "    print(f\"Loading master labels from: {master_csv}\")\n",
    "    df = pd.read_csv(master_csv)\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    required_cols = ['folder', 'filename', 'class', 'color', 'size', 'texture']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Clean the data\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    \n",
    "    # Filter for CVCL training classes if specified\n",
    "    if filter_classes is not None:\n",
    "        print(f\"\\nFiltering for {len(filter_classes)} CVCL training classes...\")\n",
    "        df = df[df['class'].isin(filter_classes)]\n",
    "        print(f\"After filtering: {len(df)} images from {df['class'].nunique()} classes\")\n",
    "    \n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"Total images: {len(df)}\")\n",
    "    print(f\"Classes: {df['class'].nunique()} unique\")\n",
    "    print(f\"Colors: {df['color'].nunique()} unique\")\n",
    "    print(f\"Sizes: {df['size'].nunique()} unique\")\n",
    "    print(f\"Textures: {df['texture'].nunique()} unique\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "class SyntheticImageDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform):\n",
    "        self.df = df\n",
    "        # For SyntheticKonkle_224, images are in nested structure\n",
    "        self.data_dir = os.path.join(data_dir, 'SyntheticKonkle')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.data_dir, row['folder'], row['filename'])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "        except:\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    classes = [b[1] for b in batch]\n",
    "    colors = [b[2] for b in batch]\n",
    "    sizes = [b[3] for b in batch]\n",
    "    textures = [b[4] for b in batch]\n",
    "    idxs = [b[5] for b in batch]\n",
    "    return imgs, classes, colors, sizes, textures, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_class_prototype_test_per_class(model_name, seed=0, device='cuda' if torch.cuda.is_available() else 'cpu', \n",
    "                                       batch_size=32, trials_per_class=250, filter_classes=None):\n",
    "    \"\"\"Run class test using visual prototypes and return per-class results.\n",
    "    \n",
    "    Args:\n",
    "        filter_classes: List of class names to test. If None, test all classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Load model & transform\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    \n",
    "    # Build dataset and extract embeddings\n",
    "    df = build_synthetic_dataset(filter_classes=filter_classes)\n",
    "    ds = SyntheticImageDataset(df, DATA_DIR, transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(\"Extracting image embeddings...\")\n",
    "    all_embs, all_classes, all_colors, all_sizes, all_textures, all_idxs = [], [], [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, classes, colors, sizes, textures, idxs in tqdm(loader, desc=\"Processing images\"):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = extractor.get_img_feature(imgs)\n",
    "            feats = extractor.norm_features(feats).cpu().float()\n",
    "            all_embs.append(feats)\n",
    "            all_classes.extend(classes)\n",
    "            all_colors.extend(colors)\n",
    "            all_sizes.extend(sizes)\n",
    "            all_textures.extend(textures)\n",
    "            all_idxs.extend(idxs)\n",
    "    \n",
    "    all_embs = torch.cat(all_embs, dim=0)\n",
    "    print(f\"Extracted {len(all_embs)} image embeddings\")\n",
    "\n",
    "    # Group by class and attributes\n",
    "    class_attr_idxs = defaultdict(lambda: defaultdict(list))\n",
    "    idx_to_row = {idx: i for i, idx in enumerate(all_idxs)}\n",
    "    \n",
    "    for i, (idx, cls, col, size, texture) in enumerate(zip(all_idxs, all_classes, all_colors, all_sizes, all_textures)):\n",
    "        class_attr_idxs[cls][(col, size, texture)].append(idx)\n",
    "\n",
    "    # Get unique classes\n",
    "    unique_classes = list(class_attr_idxs.keys())\n",
    "    print(f\"Testing {len(unique_classes)} classes\")\n",
    "    \n",
    "    # Create visual prototypes for each class\n",
    "    print(\"Creating visual prototypes...\")\n",
    "    class_prototypes = {}\n",
    "    for cls in unique_classes:\n",
    "        # Get all images for this class\n",
    "        class_idxs = []\n",
    "        for attr_idxs in class_attr_idxs[cls].values():\n",
    "            class_idxs.extend(attr_idxs)\n",
    "        \n",
    "        # Average features to create prototype\n",
    "        class_features = all_embs[[idx_to_row[idx] for idx in class_idxs]]\n",
    "        prototype = class_features.mean(0)\n",
    "        prototype = prototype / prototype.norm()  # Normalize\n",
    "        class_prototypes[cls] = prototype\n",
    "    \n",
    "    # Track per-class performance\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    \n",
    "    print(f\"Running {trials_per_class} trials per class...\")\n",
    "    \n",
    "    # Run trials for each class\n",
    "    for target_class in tqdm(unique_classes, desc=f\"Testing {model_name}\"):\n",
    "        trials_done = 0\n",
    "        \n",
    "        # Get all attribute combinations for this class\n",
    "        for (color, size, texture), idx_list in class_attr_idxs[target_class].items():\n",
    "            if trials_done >= trials_per_class:\n",
    "                break\n",
    "                \n",
    "            # Find distractors from other classes with SAME attributes\n",
    "            other_classes_with_attr = []\n",
    "            for other_cls in unique_classes:\n",
    "                if other_cls != target_class:\n",
    "                    if (color, size, texture) in class_attr_idxs[other_cls]:\n",
    "                        other_classes_with_attr.append(other_cls)\n",
    "            \n",
    "            # Need at least 3 distractor classes with matching attributes\n",
    "            if len(idx_list) >= 1 and len(other_classes_with_attr) >= 3:\n",
    "                # Run multiple trials for this combination\n",
    "                n_trials = min(10, trials_per_class - trials_done)\n",
    "                \n",
    "                for _ in range(n_trials):\n",
    "                    # Pick query image from target class\n",
    "                    query_idx = random.choice(idx_list)\n",
    "                    query_features = all_embs[idx_to_row[query_idx]]\n",
    "                    \n",
    "                    # Pick 3 distractor classes with matching attributes\n",
    "                    distractor_classes = random.sample(other_classes_with_attr, min(3, len(other_classes_with_attr)))\n",
    "                    \n",
    "                    # Create list of candidate prototypes (target + 3 distractors)\n",
    "                    candidate_classes = [target_class] + distractor_classes\n",
    "                    candidate_prototypes = torch.stack([class_prototypes[cls] for cls in candidate_classes])\n",
    "                    \n",
    "                    # Compute similarities between query and all prototypes\n",
    "                    similarities = query_features @ candidate_prototypes.T\n",
    "                    \n",
    "                    # Check if model correctly identifies target class (index 0)\n",
    "                    prediction = similarities.argmax().item()\n",
    "                    \n",
    "                    # Update counts\n",
    "                    class_correct[target_class] += int(prediction == 0)\n",
    "                    class_total[target_class] += 1\n",
    "                    trials_done += 1\n",
    "                    \n",
    "                    if trials_done >= trials_per_class:\n",
    "                        break\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for cls in unique_classes:\n",
    "        if class_total[cls] > 0:\n",
    "            class_accuracies[cls] = class_correct[cls] / class_total[cls]\n",
    "        else:\n",
    "            class_accuracies[cls] = 0.0\n",
    "    \n",
    "    # Print summary\n",
    "    overall_correct = sum(class_correct.values())\n",
    "    overall_total = sum(class_total.values())\n",
    "    overall_acc = overall_correct / overall_total if overall_total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nOverall: {overall_correct}/{overall_total} = {overall_acc:.3f}\")\n",
    "    print(f\"Classes tested: {len([c for c in class_accuracies if class_total[c] > 0])}\")\n",
    "    \n",
    "    return class_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple seeds for statistical analysis - CVCL training classes only\n",
    "n_seeds = 3  # Limited seeds due to potential rate limiting\n",
    "trials_per_class = 500  # More trials per seed for robust statistics\n",
    "models_to_test = ['cvcl-resnext', 'clip-res']\n",
    "\n",
    "# Check dataset first\n",
    "test_df = build_synthetic_dataset(filter_classes=CVCL_TRAINING_CLASSES)\n",
    "n_classes = len(test_df['class'].unique())\n",
    "print(f\"\\nFound {n_classes} CVCL training classes in the dataset\")\n",
    "\n",
    "print(f\"\\nStarting Visual Prototype evaluation (CVCL Training Classes Only):\")\n",
    "print(f\"Configuration: {n_seeds} seeds × {trials_per_class} trials/class × {n_classes} classes\")\n",
    "print(f\"Key features:\")\n",
    "print(f\"  - Only testing classes that appear in CVCL training data\")\n",
    "print(f\"  - Using visual prototypes (averaged class features)\")\n",
    "print(f\"  - Distractors have MATCHING size, color, and texture\\n\")\n",
    "\n",
    "all_results = {model: defaultdict(list) for model in models_to_test}\n",
    "\n",
    "# Run evaluation\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {model_name} with visual prototype approach\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        print(f\"\\nSeed {seed+1}/{n_seeds} for {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            class_acc = run_class_prototype_test_per_class(\n",
    "                model_name, \n",
    "                seed=seed, \n",
    "                trials_per_class=trials_per_class,\n",
    "                filter_classes=CVCL_TRAINING_CLASSES  # Filter for CVCL classes\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            for cls, acc in class_acc.items():\n",
    "                all_results[model_name][cls].append(acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if len(class_acc) > 0:\n",
    "                mean_acc = np.mean(list(class_acc.values()))\n",
    "                print(f\"  Mean accuracy across classes: {mean_acc:.3f}\")\n",
    "                print(f\"  Classes successfully tested: {len(class_acc)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            if \"404\" in str(e) or \"rate\" in str(e).lower():\n",
    "                print(f\"  Rate limit hit - waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                # Retry once\n",
    "                try:\n",
    "                    class_acc = run_class_prototype_test_per_class(\n",
    "                        model_name, seed=seed, trials_per_class=trials_per_class,\n",
    "                        filter_classes=CVCL_TRAINING_CLASSES\n",
    "                    )\n",
    "                    for cls, acc in class_acc.items():\n",
    "                        all_results[model_name][cls].append(acc)\n",
    "                    print(f\"  Retry successful!\")\n",
    "                except:\n",
    "                    print(f\"  Retry failed - skipping seed {seed}\")\n",
    "                    continue\n",
    "        \n",
    "        # Add delay between seeds for CVCL\n",
    "        if 'cvcl' in model_name and seed < n_seeds - 1:\n",
    "            print(\"  Waiting 30 seconds before next seed...\")\n",
    "            time.sleep(30)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_results = {}\n",
    "for model_name in models_to_test:\n",
    "    stats_results[model_name] = {}\n",
    "    for cls, accs in all_results[model_name].items():\n",
    "        if len(accs) > 0:\n",
    "            n_samples = len(accs)\n",
    "            stats_results[model_name][cls] = {\n",
    "                'mean': np.mean(accs),\n",
    "                'std': np.std(accs, ddof=1) if n_samples > 1 else 0,\n",
    "                'se': np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'ci95': 1.96 * np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'n_samples': n_samples,\n",
    "                'total_trials': n_samples * trials_per_class,\n",
    "                'raw': accs\n",
    "            }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUAL PROTOTYPE EVALUATION COMPLETE (CVCL TRAINING CLASSES)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "detailed_df = []\n",
    "for model_name in models_to_test:\n",
    "    for cls, stats in stats_results[model_name].items():\n",
    "        for seed_idx, acc in enumerate(stats['raw']):\n",
    "            detailed_df.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'seed': seed_idx,\n",
    "                'accuracy': acc,\n",
    "                'n_trials': trials_per_class,\n",
    "                'test_type': 'visual_prototype_cvcl_training'\n",
    "            })\n",
    "\n",
    "if len(detailed_df) > 0:\n",
    "    detailed_df = pd.DataFrame(detailed_df)\n",
    "    output_path = os.path.join(RESULTS_DIR, 'class_prototype_cvcl_training_perclass_results.csv')\n",
    "    detailed_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved detailed results to {output_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary_stats = []\n",
    "    for model_name in models_to_test:\n",
    "        for cls, stats in stats_results[model_name].items():\n",
    "            summary_stats.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'mean_accuracy': stats['mean'],\n",
    "                'std': stats['std'],\n",
    "                'se': stats['se'],\n",
    "                'ci95': stats['ci95'],\n",
    "                'n_seeds': stats['n_samples'],\n",
    "                'total_trials': stats['total_trials']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_path = os.path.join(RESULTS_DIR, 'class_prototype_cvcl_training_perclass_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Saved summary statistics to {summary_path}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for CVCL training classes\n",
    "if len(stats_results[models_to_test[0]]) > 0:\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Single plot for CVCL training classes (should be ~24 classes)\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    # Prepare data\n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    \n",
    "    # Define colors and markers\n",
    "    colors = {\n",
    "        'cvcl-resnext': '#2a9d8f',  # Teal for CVCL\n",
    "        'clip-res': '#e63946'  # Red for CLIP\n",
    "    }\n",
    "    markers = {\n",
    "        'cvcl-resnext': 'o',\n",
    "        'clip-res': 's'\n",
    "    }\n",
    "    avg_line_styles = {\n",
    "        'cvcl-resnext': '--',\n",
    "        'clip-res': '-.'\n",
    "    }\n",
    "    \n",
    "    x_pos = np.arange(len(classes))\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        means = [stats_results[model_name][cls]['mean'] * 100 for cls in classes]\n",
    "        errors = [stats_results[model_name][cls]['ci95'] * 100 for cls in classes]\n",
    "        \n",
    "        ax.errorbar(x_pos, means, yerr=errors,\n",
    "                   label=model_name.upper().replace('-', ' '),\n",
    "                   color=colors[model_name],\n",
    "                   marker=markers[model_name],\n",
    "                   markersize=8,\n",
    "                   linewidth=0,\n",
    "                   capsize=5,\n",
    "                   capthick=2,\n",
    "                   alpha=0.9,\n",
    "                   markeredgecolor='black',\n",
    "                   markeredgewidth=0.5)\n",
    "    \n",
    "    # Add chance level\n",
    "    ax.axhline(y=25, color='#ffa500', linestyle=':', alpha=0.8, linewidth=1.5,\n",
    "              label='Chance Level (25%)')\n",
    "    \n",
    "    # Calculate and add average lines\n",
    "    for model_name in models_to_test:\n",
    "        all_means = [stats_results[model_name][cls]['mean'] * 100 for cls in classes]\n",
    "        avg_performance = np.mean(all_means)\n",
    "        ax.axhline(y=avg_performance,\n",
    "                  color=colors[model_name],\n",
    "                  linestyle=avg_line_styles[model_name],\n",
    "                  alpha=0.7,\n",
    "                  linewidth=2,\n",
    "                  label=f'{model_name.upper().split(\"-\")[0]} Average ({avg_performance:.1f}%)')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_ylabel('Visual Prototype Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('CVCL Training Classes', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=10)\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.set_facecolor('#fafafa')\n",
    "    \n",
    "    # Title\n",
    "    ax.set_title('Visual Prototype Per-Class Performance (CVCL Training Classes Only)\\nControlled Visual Properties (Size, Color, Texture)',\n",
    "                fontsize=14, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(loc='upper left', fontsize=10, frameon=True, fancybox=True, shadow=True,\n",
    "             framealpha=0.95, ncol=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    png_path = os.path.join(RESULTS_DIR, 'class_prototype_cvcl_training_perclass.png')\n",
    "    pdf_path = os.path.join(RESULTS_DIR, 'class_prototype_cvcl_training_perclass.pdf')\n",
    "    \n",
    "    plt.savefig(png_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(pdf_path, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved plots to:\")\n",
    "    print(f\"  - {png_path}\")\n",
    "    print(f\"  - {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for CVCL training classes\n",
    "if len(stats_results) > 0 and len(stats_results[models_to_test[0]]) > 0:\n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    \n",
    "    summary_data = []\n",
    "    for cls in classes:\n",
    "        row = {'Class': cls}\n",
    "        for model in models_to_test:\n",
    "            if cls in stats_results[model]:\n",
    "                stats = stats_results[model][cls]\n",
    "                row[f\"{model}_mean\"] = f\"{stats['mean']:.3f}\"\n",
    "                row[f\"{model}_ci95\"] = f\"±{stats['ci95']:.3f}\"\n",
    "                row[f\"{model}_trials\"] = stats['total_trials']\n",
    "        \n",
    "        # Add difference if both models have results\n",
    "        if cls in stats_results['clip-res'] and cls in stats_results['cvcl-resnext']:\n",
    "            diff = stats_results['clip-res'][cls]['mean'] - stats_results['cvcl-resnext'][cls]['mean']\n",
    "            row['difference'] = f\"{diff:+.3f}\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUAL PROTOTYPE PER-CLASS PERFORMANCE (CVCL TRAINING CLASSES ONLY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERALL VISUAL PROTOTYPE PERFORMANCE (CVCL TRAINING CLASSES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        all_accs = []\n",
    "        for cls in classes:\n",
    "            if cls in stats_results[model]:\n",
    "                all_accs.extend(stats_results[model][cls]['raw'])\n",
    "        \n",
    "        if len(all_accs) > 0:\n",
    "            mean = np.mean(all_accs)\n",
    "            std = np.std(all_accs)\n",
    "            se = std / np.sqrt(len(all_accs))\n",
    "            ci95 = 1.96 * se\n",
    "            print(f\"{model}: {mean:.3f} ± {ci95:.3f} (SE: {se:.3f}, n={len(all_accs)} samples)\")\n",
    "    \n",
    "    # Statistical test\n",
    "    from scipy import stats as scipy_stats\n",
    "    \n",
    "    cvcl_all = []\n",
    "    clip_all = []\n",
    "    for cls in classes:\n",
    "        if cls in stats_results['cvcl-resnext']:\n",
    "            cvcl_all.extend(stats_results['cvcl-resnext'][cls]['raw'])\n",
    "        if cls in stats_results['clip-res']:\n",
    "            clip_all.extend(stats_results['clip-res'][cls]['raw'])\n",
    "    \n",
    "    if len(cvcl_all) > 0 and len(clip_all) > 0:\n",
    "        t_stat, p_value = scipy_stats.ttest_ind(cvcl_all, clip_all)\n",
    "        print(f\"\\nt-test: t={t_stat:.3f}, p={p_value:.6f}\")\n",
    "        if p_value < 0.001:\n",
    "            print(\"Result: Highly significant difference (p < 0.001)\")\n",
    "        elif p_value < 0.01:\n",
    "            print(\"Result: Significant difference (p < 0.01)\")\n",
    "        elif p_value < 0.05:\n",
    "            print(\"Result: Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"Result: No significant difference\")\n",
    "    \n",
    "    print(f\"\\nNote: These results are specifically for the {len(classes)} classes that appear in CVCL training data.\")\n",
    "    print(\"Visual prototypes are created by averaging features from all images within each class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create difference plot for CVCL training classes\n",
    "if len(stats_results) > 0 and len(stats_results[models_to_test[0]]) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    classes = sorted(list(stats_results[models_to_test[0]].keys()))\n",
    "    differences = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        if cls in stats_results['clip-res'] and cls in stats_results['cvcl-resnext']:\n",
    "            diff = stats_results['clip-res'][cls]['mean'] - stats_results['cvcl-resnext'][cls]['mean']\n",
    "            differences.append(diff)\n",
    "        else:\n",
    "            differences.append(0)\n",
    "    \n",
    "    colors_diff = ['#2ecc71' if d > 0 else '#e74c3c' for d in differences]\n",
    "    bars = plt.bar(range(len(classes)), differences, color=colors_diff, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on significant differences\n",
    "    for i, (cls, diff) in enumerate(zip(classes, differences)):\n",
    "        if abs(diff) > 0.1:  # Only label large differences\n",
    "            plt.text(i, diff + (0.01 if diff > 0 else -0.02), f'{diff:.2f}',\n",
    "                    ha='center', va='bottom' if diff > 0 else 'top', fontsize=8)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel('CVCL Training Classes', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Performance Difference\\n(CLIP - CVCL)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Visual Prototype Performance Differences (CVCL Training Classes Only)\\nWith Controlled Visual Properties',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#2ecc71', alpha=0.7, label='CLIP Better'),\n",
    "        Patch(facecolor='#e74c3c', alpha=0.7, label='CVCL Better')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Add annotation\n",
    "    plt.text(0.02, 0.98, f'Visual Prototype Method\\n{len(classes)} CVCL training classes',\n",
    "            transform=plt.gca().transAxes, fontsize=10, fontweight='bold',\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    diff_plot_path = os.path.join(RESULTS_DIR, 'class_prototype_cvcl_training_difference.png')\n",
    "    plt.savefig(diff_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved difference plot to {diff_plot_path}\")\n",
    "    \n",
    "    # Print summary of differences\n",
    "    clip_better = sum(1 for d in differences if d > 0)\n",
    "    cvcl_better = sum(1 for d in differences if d < 0)\n",
    "    tied = sum(1 for d in differences if d == 0)\n",
    "    \n",
    "    print(f\"\\nSummary (CVCL Training Classes - Visual Prototype):\")\n",
    "    print(f\"  CLIP performs better: {clip_better}/{len(classes)} classes\")\n",
    "    print(f\"  CVCL performs better: {cvcl_better}/{len(classes)} classes\")\n",
    "    if tied > 0:\n",
    "        print(f\"  No difference: {tied}/{len(classes)} classes\")\n",
    "    \n",
    "    avg_diff = np.mean([d for d in differences if d != 0])\n",
    "    print(f\"  Average difference: {avg_diff:.3f}\")\n",
    "    print(f\"\\nThese classes were seen during CVCL training, making this a direct comparison.\")\n",
    "    print(f\"Visual prototypes test pure visual recognition without language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare visual prototype vs text-vision results if both exist\n",
    "import os\n",
    "\n",
    "# Try to load text-vision results for comparison\n",
    "textvision_path = os.path.join(RESULTS_DIR, 'Textvision', 'class_textvision_cvcl_training_perclass_summary.csv')\n",
    "\n",
    "if os.path.exists(textvision_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARING VISUAL PROTOTYPE VS TEXT-VISION METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load text-vision results\n",
    "    textvision_df = pd.read_csv(textvision_path)\n",
    "    \n",
    "    # Create comparison for each model\n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        \n",
    "        # Get visual prototype results\n",
    "        vp_accs = []\n",
    "        for cls in classes:\n",
    "            if cls in stats_results[model_name]:\n",
    "                vp_accs.extend(stats_results[model_name][cls]['raw'])\n",
    "        \n",
    "        if len(vp_accs) > 0:\n",
    "            vp_mean = np.mean(vp_accs)\n",
    "            print(f\"  Visual Prototype: {vp_mean:.3f}\")\n",
    "        \n",
    "        # Get text-vision results\n",
    "        tv_data = textvision_df[textvision_df['model'] == model_name]['mean_accuracy']\n",
    "        if len(tv_data) > 0:\n",
    "            tv_mean = tv_data.mean()\n",
    "            print(f\"  Text-Vision:      {tv_mean:.3f}\")\n",
    "            \n",
    "            # Compare\n",
    "            if len(vp_accs) > 0:\n",
    "                diff = vp_mean - tv_mean\n",
    "                print(f\"  Difference:       {diff:+.3f} (Visual Prototype {'better' if diff > 0 else 'worse'})\")\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Visual Prototype: Tests pure visual recognition using averaged class features\")\n",
    "    print(\"- Text-Vision: Tests multimodal understanding using text encodings of class names\")\n",
    "    print(\"- Both use controlled visual properties (matching size, color, texture for distractors)\")\n",
    "else:\n",
    "    print(\"\\nText-vision results not found for comparison.\")\n",
    "    print(f\"Run the Class_TextVision_CVCLTraining_PerClass_Analysis.ipynb notebook first to enable comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}