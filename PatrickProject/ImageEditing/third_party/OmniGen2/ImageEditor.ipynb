{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/miniconda3/envs/omnigen2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/attention_processor.py:33: UserWarning: Cannot import flash_attn, install flash_attn to use Flash2Varlen attention for better performance\n",
      "  warnings.warn(\"Cannot import flash_attn, install flash_attn to use Flash2Varlen attention for better performance\")\n",
      "/home/localstorage/ssd/patrick/discover-hidden-visual-concepts/PatrickProject/ImageEditing/third_party/OmniGen2/omnigen2/models/transformers/block_lumina2.py:37: UserWarning: Cannot import flash_attn, install flash_attn to use fused SwiGLU for better performance\n",
      "  warnings.warn(\"Cannot import flash_attn, install flash_attn to use fused SwiGLU for better performance\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import accelerate\n",
    "from pathlib import Path\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(root_dir)\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline\n",
    "from omnigen2.models.transformers.transformer_omnigen2 import OmniGen2Transformer2DModel\n",
    "from omnigen2.utils.img_util import create_collage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def preprocess(input_image_path: Union[str, List[str], None] = None) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Preprocess the input images by:\n",
    "    - Accepting a single path, list of paths, or a directory\n",
    "    - Loading only common image files\n",
    "    - Correcting orientation via EXIF\n",
    "    - Converting to 3‑channel RGB (drops alpha)\n",
    "    \"\"\"\n",
    "    if input_image_path is None:\n",
    "        return []\n",
    "\n",
    "    # Normalize to a list of paths\n",
    "    if isinstance(input_image_path, str):\n",
    "        paths = [input_image_path]\n",
    "    else:\n",
    "        paths = input_image_path\n",
    "\n",
    "    images: List[Image.Image] = []\n",
    "    for p in paths:\n",
    "        if os.path.isdir(p):\n",
    "            for fname in os.listdir(p):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                    img = Image.open(os.path.join(p, fname))\n",
    "                    images.append(img)\n",
    "        else:\n",
    "            img = Image.open(p)\n",
    "            images.append(img)\n",
    "\n",
    "    # EXIF transpose + strip alpha channel\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        img = ImageOps.exif_transpose(img).convert(\"RGB\")\n",
    "        processed.append(img)\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't connect to the Hub: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/OmniGen2/OmniGen2 (Request ID: Root=1-6884c7d2-4da896402310b7302e2b0e2c;b5d1a93e-1672-4854-ae4a-c0b7c02ad7aa)\n",
      "\n",
      "Invalid credentials in Authorization header.\n",
      "Will try to load from local cache.\n",
      "Keyword arguments {'trust_remote_code': True} are not expected by OmniGen2Pipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|                                                                                                                                          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|                                                                                                                                               | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████████████▌                                                                   | 1/2 [00:00<00:00,  2.86it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.25it/s]\u001b[A\n",
      "Loading pipeline components...:  20%|██████████████████████████                                                                                                        | 1/5 [00:01<00:04,  1.17s/it]Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading pipeline components...:  40%|████████████████████████████████████████████████████                                                                              | 2/5 [00:01<00:02,  1.20it/s]\n",
      "Loading checkpoint shards:   0%|                                                                                                                                               | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  25%|█████████████████████████████████▊                                                                                                     | 1/4 [00:00<00:01,  2.55it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████████████▌                                                                   | 2/4 [00:00<00:00,  2.95it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.92it/s]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.73it/s]\n",
      "Expected types for transformer: (<class 'omnigen2.models.transformers.transformer_omnigen2.OmniGen2Transformer2DModel'>,), got <class 'diffusers_modules.local.transformer_omnigen2.OmniGen2Transformer2DModel'>.\n",
      "Fetching 2 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20360.70it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "model_path=\"OmniGen2/OmniGen2\"\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    token=\"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\",\n",
    ")\n",
    "pipeline.transformer = OmniGen2Transformer2DModel.from_pretrained(\n",
    "    model_path,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process objects: ['doll', 'pipe', 'telescope', 'suitcase', 'christmastreeornamentball']\n",
      "\n",
      "=== Processing 'doll' ===\n",
      "✅ All 120 combos present for 'doll'.\n",
      "\n",
      "=== Processing 'pipe' ===\n",
      "✅ All 120 combos present for 'pipe'.\n",
      "\n",
      "=== Processing 'telescope' ===\n",
      "✅ All 120 combos present for 'telescope'.\n",
      "\n",
      "=== Processing 'suitcase' ===\n",
      "✅ All 120 combos present for 'suitcase'.\n",
      "\n",
      "=== Processing 'christmastreeornamentball' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_red.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_green.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:37<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_blue.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_yellow.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_orange.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:37<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_purple.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:37<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_pink.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_brown.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:35<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_black.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:36<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_01_gray.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:36<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ christmastreeornamentball_large_bumpy_02_red.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 36/50 [01:09<00:28,  2.03s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "import csv, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# --- User‑level config ----------------------------------------\n",
    "COLORS = [\"red\",\"green\",\"blue\",\"yellow\",\"orange\",\n",
    "          \"purple\",\"pink\",\"brown\",\"black\",\"gray\"]\n",
    "\n",
    "NEG_PROMPT = (\n",
    "    \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, \"\n",
    "    \"mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, \"\n",
    "    \"messy drawing, broken legs, censor, censored, censor_bar\"\n",
    ")\n",
    "\n",
    "# Root of your synthetic_dataset folder\n",
    "SYNTHETIC_DIR = Path(\n",
    "    \"/home/patrick/ssd/discover-hidden-visual-concepts/PatrickProject/\"\n",
    "    \"ImageEditing/third_party/OmniGen2/synthetic_dataset\"\n",
    ")\n",
    "\n",
    "# CSV listing which objects to process (header: \"class\")\n",
    "OBJECTS_CSV = SYNTHETIC_DIR / \"objectstorun.csv\"\n",
    "\n",
    "# --- Read list of object names from CSV ----------------------\n",
    "object_names = []\n",
    "with OBJECTS_CSV.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        name = row.get(\"class\", \"\").strip()\n",
    "        if not name or name.lower() == \"class\":\n",
    "            continue\n",
    "        # strip literal \"_bases\" if present\n",
    "        if name.endswith(\"_bases\"):\n",
    "            name = name[:-len(\"_bases\")]\n",
    "        object_names.append(name)\n",
    "print(\"Will process objects:\", object_names)\n",
    "\n",
    "# --- Main loop: one pass per object ---------------------------\n",
    "for OBJECT_NAME in object_names:\n",
    "    print(f\"\\n=== Processing '{OBJECT_NAME}' ===\")\n",
    "\n",
    "    # 1) Locate bases\n",
    "    BASE_DIR = SYNTHETIC_DIR / f\"{OBJECT_NAME}_bases\"\n",
    "    pngs = sorted(BASE_DIR.glob(\"*.png\"))\n",
    "    if not pngs:\n",
    "        print(f\"⚠️ No bases in {BASE_DIR}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2) Prepare color output folder\n",
    "    COLOR_DIR = SYNTHETIC_DIR / f\"{OBJECT_NAME}_color\"\n",
    "    COLOR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # 3) Prepare CSV\n",
    "    CSV_PATH = COLOR_DIR / \"labels.csv\"\n",
    "    write_header = not CSV_PATH.exists()\n",
    "    with CSV_PATH.open(\"a\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        if write_header:\n",
    "            writer.writerow([\"filename\",\"size\",\"texture\",\"variant\",\"colour\",\"class\"])\n",
    "\n",
    "        # 4) Process each base\n",
    "        for base_png in pngs:\n",
    "            stem = base_png.stem\n",
    "            if not stem.startswith(\"base_\"):\n",
    "                print(\"⚠️ skipping unexpected file:\", stem)\n",
    "                continue\n",
    "\n",
    "            # remove prefix and split\n",
    "            parts = stem[len(\"base_\"):].split(\"_\")\n",
    "            # drop last segment (class tag)\n",
    "            parts = parts[:-1]\n",
    "            # now parts = [size, *(texture parts), variant]\n",
    "            size = parts[0]\n",
    "            variant = parts[-1]\n",
    "            texture = \"_\".join(parts[1:-1]) if len(parts) > 2 else \"\"\n",
    "\n",
    "            # preprocess and generate\n",
    "            input_imgs = preprocess(str(base_png))\n",
    "            for colour in COLORS:\n",
    "                out_name = f\"{OBJECT_NAME}_{size}_{texture}_{variant}_{colour}.png\"\n",
    "                out_path = COLOR_DIR / out_name\n",
    "                if out_path.exists():\n",
    "                    continue\n",
    "                # generate\n",
    "                prompt = f\"Change the object to {colour} and make the background white\"\n",
    "                gen = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "                result = pipeline(\n",
    "                    prompt              = prompt,\n",
    "                    input_images        = input_imgs,\n",
    "                    num_inference_steps = 50,\n",
    "                    max_sequence_length = 1024,\n",
    "                    text_guidance_scale = 5.0,\n",
    "                    image_guidance_scale= 2.0,\n",
    "                    negative_prompt     = NEG_PROMPT,\n",
    "                    num_images_per_prompt=1,\n",
    "                    generator           = gen,\n",
    "                    output_type         = \"pil\",\n",
    "                )\n",
    "                result.images[0].save(out_path)\n",
    "                writer.writerow([out_name, size, texture, variant, colour, OBJECT_NAME])\n",
    "                print(\"✔\", out_name)\n",
    "\n",
    "    # 5) Check completeness\n",
    "    expected = set()\n",
    "    for base_png in pngs:\n",
    "        parts = base_png.stem[len(\"base_\"):].split(\"_\")[:-1]\n",
    "        size, variant = parts[0], parts[-1]\n",
    "        texture = \"_\".join(parts[1:-1]) if len(parts) > 2 else \"\"\n",
    "        for colour in COLORS:\n",
    "            expected.add(f\"{OBJECT_NAME}_{size}_{texture}_{variant}_{colour}.png\")\n",
    "    existing = {p.name for p in COLOR_DIR.glob(\"*.png\")}\n",
    "    missing = sorted(expected - existing)\n",
    "    if missing:\n",
    "        print(\"🚨 Missing combos:\", missing)\n",
    "    else:\n",
    "        print(f\"✅ All {len(expected)} combos present for '{OBJECT_NAME}'.\")\n",
    "\n",
    "print(\"\\n🎉 All objects processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
