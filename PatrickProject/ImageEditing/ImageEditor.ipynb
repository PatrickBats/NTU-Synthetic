{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import accelerate\n",
    "from pathlib import Path\n",
    "\n",
    "# Add OmniGen2 to path\n",
    "root_dir = Path().resolve()\n",
    "omnigen_path = root_dir / \"OmniGen2-main\"\n",
    "sys.path.append(str(omnigen_path))\n",
    "\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline\n",
    "from omnigen2.models.transformers.transformer_omnigen2 import OmniGen2Transformer2DModel\n",
    "from omnigen2.utils.img_util import create_collage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def preprocess(input_image_path: Union[str, List[str], None] = None) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Preprocess the input images by:\n",
    "    - Accepting a single path, list of paths, or a directory\n",
    "    - Loading only common image files\n",
    "    - Correcting orientation via EXIF\n",
    "    - Converting to 3‑channel RGB (drops alpha)\n",
    "    \"\"\"\n",
    "    if input_image_path is None:\n",
    "        return []\n",
    "\n",
    "    # Normalize to a list of paths\n",
    "    if isinstance(input_image_path, str):\n",
    "        paths = [input_image_path]\n",
    "    else:\n",
    "        paths = input_image_path\n",
    "\n",
    "    images: List[Image.Image] = []\n",
    "    for p in paths:\n",
    "        if os.path.isdir(p):\n",
    "            for fname in os.listdir(p):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                    img = Image.open(os.path.join(p, fname))\n",
    "                    images.append(img)\n",
    "        else:\n",
    "            img = Image.open(p)\n",
    "            images.append(img)\n",
    "\n",
    "    # EXIF transpose + strip alpha channel\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        img = ImageOps.exif_transpose(img).convert(\"RGB\")\n",
    "        processed.append(img)\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'trust_remote_code': True, 'enable_model_cpu_offload': True} are not expected by OmniGen2Pipeline and will be ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n",
      "Loading pipeline components...:  40%|████      | 2/5 [00:11<00:14,  4.92s/it]Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading pipeline components...:  60%|██████    | 3/5 [00:12<00:05,  2.96s/it]C:\\Users\\jbats\\.cache\\huggingface\\modules\\diffusers_modules\\local\\transformer_omnigen2.py:1004: UserWarning: Cannot import triton, install triton to use fused RMSNorm for better performance\n",
      "  warnings.warn(\"Cannot import triton, install triton to use fused RMSNorm for better performance\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.10s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:27<00:00,  5.47s/it]\n",
      "Expected types for transformer: (<class 'omnigen2.models.transformers.transformer_omnigen2.OmniGen2Transformer2DModel'>,), got <class 'diffusers_modules.local.transformer_omnigen2.OmniGen2Transformer2DModel'>.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 4000.29it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "# Initialize the pipeline from Hugging Face with CPU offloading\n",
    "model_path = \"OmniGen2/OmniGen2\"\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    enable_model_cpu_offload=True  # Reduces VRAM usage by ~50% with minimal speed impact\n",
    ")\n",
    "pipeline.transformer = OmniGen2Transformer2DModel.from_pretrained(\n",
    "    model_path,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of editing an image\n",
    "def edit_image(image_path: str, prompt: str, negative_prompt: str = None):\n",
    "    \"\"\"\n",
    "    Edit an image using OmniGen2\n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        prompt: Instruction for editing\n",
    "        negative_prompt: What to avoid in generation\n",
    "    \"\"\"\n",
    "    if negative_prompt is None:\n",
    "        negative_prompt = \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    input_imgs = preprocess(image_path)\n",
    "    \n",
    "    # Generate\n",
    "    gen = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        input_images=input_imgs,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=2.0,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=gen,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax1.imshow(input_imgs[0])\n",
    "    ax1.set_title(\"Input Image\")\n",
    "    ax1.axis(\"off\")\n",
    "    \n",
    "    ax2.imshow(result.images[0])\n",
    "    ax2.set_title(\"Edited Image\")\n",
    "    ax2.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    return result.images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnigen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
