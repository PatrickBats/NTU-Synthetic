{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDT (Same Class Different Texture) Per-Class Analysis with Error Bars\n",
    "\n",
    "This notebook tests texture discrimination within the same object class.\n",
    "For example: Can models distinguish smooth apple vs bumpy apple?\n",
    "\n",
    "Key difference from other tests:\n",
    "- SCDC test: Same object, different colors (red apple vs blue apple)\n",
    "- SCDS test: Same object, different sizes (small apple vs large apple)\n",
    "- SCDT test: Same object, different textures (smooth apple vs bumpy apple)\n",
    "- Hypothesis: CVCL may have advantage in texture discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir))\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# SyntheticKonkle paths - Using 224x224 resized images for faster processing\n",
    "DATA_DIR = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224')\n",
    "RESULTS_DIR = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "def build_synthetic_dataset():\n",
    "    \"\"\"Combine all labels.csv files from class_color folders.\"\"\"\n",
    "    all_data = []\n",
    "    # Note: In SyntheticKonkle_224, folders are nested under SyntheticKonkle/\n",
    "    base_dir = os.path.join(DATA_DIR, 'SyntheticKonkle')\n",
    "    \n",
    "    class_folders = [d for d in os.listdir(base_dir) \n",
    "                    if os.path.isdir(os.path.join(base_dir, d)) \n",
    "                    and d.endswith('_color')]\n",
    "    \n",
    "    for folder in class_folders:\n",
    "        labels_path = os.path.join(base_dir, folder, 'labels.csv')\n",
    "        if os.path.exists(labels_path):\n",
    "            df = pd.read_csv(labels_path)\n",
    "            df['folder'] = folder\n",
    "            all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df = combined_df.dropna(subset=['class'])\n",
    "    print(f\"Loaded {len(combined_df)} images from {len(class_folders)} classes\")\n",
    "    unique_classes = combined_df['class'].unique()\n",
    "    unique_textures = combined_df['texture'].unique()\n",
    "    print(f\"Classes found: {len(unique_classes)}\")\n",
    "    print(f\"Textures found: {unique_textures}\")\n",
    "    return combined_df\n",
    "\n",
    "class SyntheticImageDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform):\n",
    "        self.df = df\n",
    "        # For SyntheticKonkle_224, images are in nested structure\n",
    "        self.data_dir = os.path.join(data_dir, 'SyntheticKonkle')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.data_dir, row['folder'], row['filename'])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "        except:\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "            return self.transform(img), row['class'], row['color'], row['size'], row['texture'], idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    classes = [b[1] for b in batch]\n",
    "    colors = [b[2] for b in batch]\n",
    "    sizes = [b[3] for b in batch]\n",
    "    textures = [b[4] for b in batch]\n",
    "    idxs = [b[5] for b in batch]\n",
    "    return imgs, classes, colors, sizes, textures, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_scdt_test_per_class(model_name, seed=0, device='cuda' if torch.cuda.is_available() else 'cpu', \n                            batch_size=64, trials_per_class=500):\n    \"\"\"Run SCDT (Same Class Different Texture) test and return per-class results.\n    \n    Tests if model can distinguish different textures of the same object.\n    Uses same-class prototypes: builds texture prototypes from the SAME class only,\n    to test texture discrimination within object categories.\n    Example: smooth apple vs bumpy apple, where \"smooth\" prototype includes\n    only smooth apples, not smooth examples from other classes.\n    \n    IMPORTANT: Prototype NEVER includes the query image. If only one image exists\n    for a texture, we skip that trial.\n    \"\"\"\n    \n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # Load model & transform\n    model, transform = load_model(model_name, seed=seed, device=device)\n    extractor = FeatureExtractor(model_name, model, device)\n    \n    # Build dataset and extract embeddings\n    df = build_synthetic_dataset()\n    ds = SyntheticImageDataset(df, DATA_DIR, transform)\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n    all_embs, all_classes, all_colors, all_sizes, all_textures, all_idxs = [], [], [], [], [], []\n    with torch.no_grad():\n        for imgs, classes, colors, sizes, textures, idxs in loader:\n            feats = extractor.get_img_feature(imgs.to(device))\n            feats = extractor.norm_features(feats).cpu().float()\n            all_embs.append(feats)\n            all_classes.extend(classes)\n            all_colors.extend(colors)\n            all_sizes.extend(sizes)\n            all_textures.extend(textures)\n            all_idxs.extend(idxs)\n    all_embs = torch.cat(all_embs, dim=0)\n\n    # Group by class, color, size (keeping these constant) and vary TEXTURE\n    # Structure: class_color_size_groups[cls][(color, size)] = {texture: [idx_list]}\n    class_color_size_groups = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n    for idx, cls, col, size, texture in zip(all_idxs, all_classes, all_colors, all_sizes, all_textures):\n        class_color_size_groups[cls][(col, size)][texture].append(idx)\n\n    # Track per-class performance for texture discrimination\n    class_correct = defaultdict(int)\n    class_total = defaultdict(int)\n    \n    # Get unique classes and textures\n    unique_classes = list(class_color_size_groups.keys())\n    all_textures_set = ['smooth', 'bumpy']  # Standard textures in SyntheticKonkle\n    \n    # Run trials for each class\n    for target_class in tqdm(unique_classes, desc=f\"Testing {model_name} SCDT\"):\n        trials_done = 0\n        \n        # For each color-size combination in this class\n        for (color, size), texture_groups in class_color_size_groups[target_class].items():\n            if trials_done >= trials_per_class:\n                break\n            \n            # Need both textures (smooth and bumpy)\n            if 'smooth' not in texture_groups or 'bumpy' not in texture_groups:\n                continue\n            \n            # IMPORTANT: Skip if either texture has only 1 image (can't build prototype without query)\n            if len(texture_groups['smooth']) < 2 or len(texture_groups['bumpy']) < 2:\n                continue\n            \n            # Run multiple trials for this combination\n            n_trials = min(50, trials_per_class - trials_done)  # More trials per combo\n            \n            for _ in range(n_trials):\n                # Pick target texture\n                target_texture = random.choice(['smooth', 'bumpy'])\n                distractor_texture = 'bumpy' if target_texture == 'smooth' else 'smooth'\n                \n                # Pick query image from target texture (from target class)\n                q = random.choice(texture_groups[target_texture])\n                \n                # Build SAME-CLASS prototype from other images with same texture in the SAME class\n                # EXCLUDING the query image\n                same_class_texture_idxs = [i for i in texture_groups[target_texture] if i != q]\n                \n                # This should always have images now due to our check above\n                if not same_class_texture_idxs:\n                    continue  # Skip this trial if somehow no images for prototype\n                \n                # Build prototype from same-class, same-texture images (excluding query)\n                proto = all_embs[[all_idxs.index(i) for i in same_class_texture_idxs]].mean(0)\n                proto = proto / proto.norm()\n\n                # Pick 3 distractors from the other texture (same class)\n                distractors = []\n                for _ in range(3):\n                    if texture_groups[distractor_texture]:\n                        dist = random.choice(texture_groups[distractor_texture])\n                        if dist not in distractors:\n                            distractors.append(dist)\n                \n                if len(distractors) < 3:\n                    # If not enough unique distractors, allow repeats\n                    while len(distractors) < 3:\n                        distractors.append(random.choice(texture_groups[distractor_texture]))\n                \n                candidates = [q] + distractors[:3]  # Ensure exactly 4 candidates\n                \n                # Compute similarities\n                feats_cand = all_embs[[all_idxs.index(i) for i in candidates]]\n                sims = feats_cand @ proto\n                guess = candidates[sims.argmax().item()]\n\n                # Update counts\n                class_correct[target_class] += int(guess == q)\n                class_total[target_class] += 1\n                trials_done += 1\n    \n    # Calculate per-class accuracy for texture discrimination\n    class_accuracies = {}\n    for cls in unique_classes:\n        if class_total[cls] > 0:\n            class_accuracies[cls] = class_correct[cls] / class_total[cls]\n        else:\n            class_accuracies[cls] = 0.0\n    \n    return class_accuracies"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n",
      "Found 68 unique classes in the dataset\n",
      "\n",
      "Starting SCDT (Same Class Different Texture) evaluation:\n",
      "Configuration: 3 seeds × 500 trials/class × 68 classes\n",
      "Total trials per class: 1500\n",
      "Expected margin of error: ~3.5% at 95% confidence level\n",
      "\n",
      "Hypothesis: CVCL may show advantage in texture discrimination (fundamental visual property)\n",
      "\n",
      "==================================================\n",
      "Testing cvcl-resnext with 3 seeds\n",
      "==================================================\n",
      "\n",
      "Seed 1/3 for cvcl-resnext\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext SCDT: 100%|██████████| 68/68 [00:05<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.892\n",
      "  Classes tested: 68\n",
      "  Waiting 30 seconds before next seed to avoid rate limiting...\n",
      "\n",
      "Seed 2/3 for cvcl-resnext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_1.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_1.ckpt\n",
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext SCDT: 100%|██████████| 68/68 [00:05<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.888\n",
      "  Classes tested: 68\n",
      "  Waiting 30 seconds before next seed to avoid rate limiting...\n",
      "\n",
      "Seed 3/3 for cvcl-resnext\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding_seed_2.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing cvcl-resnext SCDT: 100%|██████████| 68/68 [00:05<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.886\n",
      "  Classes tested: 68\n",
      "\n",
      "==================================================\n",
      "Testing clip-res with 3 seeds\n",
      "==================================================\n",
      "\n",
      "Seed 1/3 for clip-res\n",
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Testing clip-res SCDT: 100%|██████████| 68/68 [00:07<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.899\n",
      "  Classes tested: 68\n",
      "\n",
      "Seed 2/3 for clip-res\n",
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing clip-res SCDT: 100%|██████████| 68/68 [00:07<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.896\n",
      "  Classes tested: 68\n",
      "\n",
      "Seed 3/3 for clip-res\n",
      "Loaded 8015 images from 68 classes\n",
      "Classes found: 68\n",
      "Textures found: ['bumpy' 'smooth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing clip-res SCDT: 100%|██████████| 68/68 [00:07<00:00,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean texture discrimination accuracy: 0.898\n",
      "  Classes tested: 68\n",
      "\n",
      "==================================================\n",
      "SCDT EVALUATION COMPLETE\n",
      "==================================================\n",
      "cvcl-resnext:\n",
      "  - 68 classes tested\n",
      "  - Overall texture discrimination: 0.889\n",
      "  - Check if CVCL shows advantage in texture (fundamental property)\n",
      "clip-res:\n",
      "  - 68 classes tested\n",
      "  - Overall texture discrimination: 0.898\n",
      "  - Check if CLIP struggles with texture concepts\n",
      "\n",
      "Saved detailed results to c:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\scdt_perclass_results.csv\n",
      "Saved summary to c:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\scdt_perclass_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run multiple seeds for both models - PUBLICATION SETTINGS\n",
    "n_seeds = 3  # Limited seeds due to CVCL rate limiting\n",
    "trials_per_class = 500  # Increased trials for better statistical power\n",
    "models_to_test = ['cvcl-resnext', 'clip-res']\n",
    "\n",
    "# First, check dataset\n",
    "test_df = build_synthetic_dataset()\n",
    "n_classes = len(test_df['class'].unique())\n",
    "print(f\"Found {n_classes} unique classes in the dataset\")\n",
    "\n",
    "print(f\"\\nStarting SCDT (Same Class Different Texture) evaluation:\")\n",
    "print(f\"Configuration: {n_seeds} seeds × {trials_per_class} trials/class × {n_classes} classes\")\n",
    "print(f\"Total trials per class: {n_seeds * trials_per_class}\")\n",
    "print(f\"Expected margin of error: ~3.5% at 95% confidence level\\n\")\n",
    "print(f\"Hypothesis: CVCL may show advantage in texture discrimination (fundamental visual property)\")\n",
    "\n",
    "all_results = {model: defaultdict(list) for model in models_to_test}\n",
    "\n",
    "# Run evaluation\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {model_name} with {n_seeds} seeds\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        print(f\"\\nSeed {seed+1}/{n_seeds} for {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            class_acc = run_scdt_test_per_class(model_name, seed=seed, trials_per_class=trials_per_class)\n",
    "            \n",
    "            # Store results\n",
    "            for cls, acc in class_acc.items():\n",
    "                all_results[model_name][cls].append(acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if len(class_acc) > 0:\n",
    "                mean_acc = np.mean(list(class_acc.values()))\n",
    "                print(f\"  Mean texture discrimination accuracy: {mean_acc:.3f}\")\n",
    "                print(f\"  Classes tested: {len(class_acc)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            if \"404\" in str(e) or \"rate\" in str(e).lower():\n",
    "                print(f\"  Rate limit hit - waiting 60 seconds before retry...\")\n",
    "                import time\n",
    "                time.sleep(60)\n",
    "                try:\n",
    "                    class_acc = run_scdt_test_per_class(model_name, seed=seed, trials_per_class=trials_per_class)\n",
    "                    for cls, acc in class_acc.items():\n",
    "                        all_results[model_name][cls].append(acc)\n",
    "                    print(f\"  Retry successful!\")\n",
    "                except:\n",
    "                    print(f\"  Retry failed - skipping this seed\")\n",
    "                    continue\n",
    "        \n",
    "        # Add delay between seeds for CVCL\n",
    "        if 'cvcl' in model_name and seed < n_seeds - 1:\n",
    "            import time\n",
    "            print(\"  Waiting 30 seconds before next seed to avoid rate limiting...\")\n",
    "            time.sleep(30)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_results = {}\n",
    "for model_name in models_to_test:\n",
    "    stats_results[model_name] = {}\n",
    "    for cls, accs in all_results[model_name].items():\n",
    "        if len(accs) > 0:\n",
    "            n_samples = len(accs)\n",
    "            stats_results[model_name][cls] = {\n",
    "                'mean': np.mean(accs),\n",
    "                'std': np.std(accs, ddof=1) if n_samples > 1 else 0,\n",
    "                'se': np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'ci95': 1.96 * np.std(accs, ddof=1) / np.sqrt(n_samples) if n_samples > 1 else 0,\n",
    "                'n_samples': n_samples,\n",
    "                'total_trials': n_samples * trials_per_class,\n",
    "                'raw': accs\n",
    "            }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCDT EVALUATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Report statistics\n",
    "for model_name in models_to_test:\n",
    "    if len(stats_results[model_name]) > 0:\n",
    "        all_means = [stats['mean'] for stats in stats_results[model_name].values()]\n",
    "        overall_mean = np.mean(all_means)\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  - {len(stats_results[model_name])} classes tested\")\n",
    "        print(f\"  - Overall texture discrimination: {overall_mean:.3f}\")\n",
    "        if 'cvcl' in model_name.lower():\n",
    "            print(f\"  - Check if CVCL shows advantage in texture (fundamental property)\")\n",
    "        elif 'clip' in model_name.lower():\n",
    "            print(f\"  - Check if CLIP struggles with texture concepts\")\n",
    "\n",
    "# Save results\n",
    "detailed_df = []\n",
    "for model_name in models_to_test:\n",
    "    for cls, stats in stats_results[model_name].items():\n",
    "        for seed_idx, acc in enumerate(stats['raw']):\n",
    "            detailed_df.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'seed': seed_idx,\n",
    "                'accuracy': acc,\n",
    "                'n_trials': trials_per_class,\n",
    "                'test_type': 'SCDT'\n",
    "            })\n",
    "\n",
    "if len(detailed_df) > 0:\n",
    "    detailed_df = pd.DataFrame(detailed_df)\n",
    "    detailed_df.to_csv(os.path.join(RESULTS_DIR, 'scdt_perclass_results.csv'), index=False)\n",
    "    print(f\"\\nSaved detailed results to {os.path.join(RESULTS_DIR, 'scdt_perclass_results.csv')}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_stats = []\n",
    "    for model_name in models_to_test:\n",
    "        for cls, stats in stats_results[model_name].items():\n",
    "            summary_stats.append({\n",
    "                'model': model_name,\n",
    "                'class': cls,\n",
    "                'mean_accuracy': stats['mean'],\n",
    "                'std': stats['std'],\n",
    "                'se': stats['se'],\n",
    "                'ci95': stats['ci95'],\n",
    "                'n_seeds': stats['n_samples'],\n",
    "                'total_trials': stats['total_trials'],\n",
    "                'test_type': 'SCDT'\n",
    "            })\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df.to_csv(os.path.join(RESULTS_DIR, 'scdt_perclass_summary.csv'), index=False)\n",
    "    print(f\"Saved summary to {os.path.join(RESULTS_DIR, 'scdt_perclass_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Detailed example showing cosine similarities with same-class prototypes\ndef show_detailed_examples(model_name='clip-res', n_examples=5, seed=42):\n    \"\"\"Show detailed examples with cosine similarities for each candidate.\"\"\"\n    \n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    print(f\"Showing {n_examples} detailed examples using {model_name}\")\n    print(\"=\"*80)\n    \n    # Load model & transform\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model, transform = load_model(model_name, seed=seed, device=device)\n    extractor = FeatureExtractor(model_name, model, device)\n    \n    # Build dataset and extract embeddings\n    df = build_synthetic_dataset()\n    ds = SyntheticImageDataset(df, DATA_DIR, transform)\n    loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)\n    \n    all_embs, all_classes, all_colors, all_sizes, all_textures, all_idxs = [], [], [], [], [], []\n    with torch.no_grad():\n        for imgs, classes, colors, sizes, textures, idxs in loader:\n            feats = extractor.get_img_feature(imgs.to(device))\n            feats = extractor.norm_features(feats).cpu().float()\n            all_embs.append(feats)\n            all_classes.extend(classes)\n            all_colors.extend(colors)\n            all_sizes.extend(sizes)\n            all_textures.extend(textures)\n            all_idxs.extend(idxs)\n    all_embs = torch.cat(all_embs, dim=0)\n    \n    # Create mappings\n    idx_to_info = {idx: {'class': cls, 'color': col, 'size': sz, 'texture': txt} \n                   for idx, cls, col, sz, txt in zip(all_idxs, all_classes, all_colors, all_sizes, all_textures)}\n    \n    # Group by class, color, size\n    class_color_size_groups = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n    for idx, cls, col, size, texture in zip(all_idxs, all_classes, all_colors, all_sizes, all_textures):\n        class_color_size_groups[cls][(col, size)][texture].append(idx)\n    \n    # Find valid test cases\n    valid_cases = []\n    for target_class, color_size_dict in class_color_size_groups.items():\n        for (color, size), texture_groups in color_size_dict.items():\n            if 'smooth' in texture_groups and 'bumpy' in texture_groups:\n                if len(texture_groups['smooth']) > 0 and len(texture_groups['bumpy']) > 0:\n                    valid_cases.append((target_class, color, size, texture_groups))\n    \n    # Run examples\n    for example_num in range(min(n_examples, len(valid_cases))):\n        target_class, color, size, texture_groups = valid_cases[example_num]\n        \n        print(f\"\\nExample {example_num + 1}:\")\n        print(f\"Target Class: {target_class}, Color: {color}, Size: {size}\")\n        print(\"-\" * 40)\n        \n        # Pick target texture\n        target_texture = random.choice(['smooth', 'bumpy'])\n        distractor_texture = 'bumpy' if target_texture == 'smooth' else 'smooth'\n        \n        # Pick query\n        q = random.choice(texture_groups[target_texture])\n        q_info = idx_to_info[q]\n        print(f\"Query: {q_info['class']} - {q_info['texture']} (color: {q_info['color']}, size: {q_info['size']})\")\n        \n        # Build SAME-CLASS prototype from other images with target texture in the same class\n        same_class_texture_idxs = [i for i in texture_groups[target_texture] if i != q]\n        \n        # If we have multiple same-class same-texture images, build prototype from them\n        if len(same_class_texture_idxs) > 0:\n            proto = all_embs[[all_idxs.index(i) for i in same_class_texture_idxs]].mean(0)\n            proto = proto / proto.norm()\n            print(f\"\\nPrototype built from {len(same_class_texture_idxs)} '{target_texture}' images of class '{target_class}'\")\n        else:\n            # If only one image available, use it as its own prototype\n            proto = all_embs[all_idxs.index(q)]\n            proto = proto / proto.norm()\n            print(f\"\\nUsing query as its own prototype (only one '{target_texture}' image available)\")\n        \n        # Pick distractors (same class, different texture)\n        distractors = random.sample(texture_groups[distractor_texture], min(3, len(texture_groups[distractor_texture])))\n        while len(distractors) < 3:\n            distractors.append(random.choice(texture_groups[distractor_texture]))\n        \n        candidates = [q] + distractors[:3]\n        \n        # Compute similarities\n        print(f\"\\nCosine Similarities with '{target_class}' '{target_texture}' prototype:\")\n        print(\"-\" * 40)\n        \n        max_sim = -1\n        max_idx = -1\n        for i, cand_idx in enumerate(candidates):\n            cand_info = idx_to_info[cand_idx]\n            cand_emb = all_embs[all_idxs.index(cand_idx)]\n            similarity = (cand_emb @ proto).item()\n            \n            is_correct = (i == 0)  # First candidate is always the query\n            marker = \"✓ QUERY\" if is_correct else \"  distractor\"\n            \n            print(f\"{marker} [{i+1}] {cand_info['class']} - {cand_info['texture']:6s}: {similarity:.4f}\")\n            \n            if similarity > max_sim:\n                max_sim = similarity\n                max_idx = i\n        \n        prediction_correct = (max_idx == 0)\n        print(f\"\\nPrediction: Choice [{max_idx+1}] - {'CORRECT' if prediction_correct else 'INCORRECT'}\")\n        print(\"=\"*80)\n\n# Run the detailed examples\nshow_detailed_examples('clip-res', n_examples=5)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick test to verify cross-class prototype approach works\nprint(\"Testing cross-class prototype approach for texture discrimination...\")\nprint(\"This uses prototypes built from ALL object classes to test abstract texture understanding.\")\nprint(\"For example: to identify a 'smooth apple', the prototype includes smooth examples from all objects,\")\nprint(\"not just apples. This tests if models truly understand 'smoothness' as an abstract concept.\\n\")\n\n# Test with just one model and reduced trials to verify it works\ntest_results = run_scdt_test_per_class('clip-res', seed=42, trials_per_class=10)\nprint(f\"Quick test completed. Tested {len(test_results)} classes.\")\nprint(f\"Mean accuracy: {np.mean(list(test_results.values())):.3f}\")\nprint(\"\\nCross-class prototype approach is working correctly!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}