{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Class Different Texture (SCDT) Text-Vision Comparison - SyntheticKonkle\n",
    "\n",
    "This notebook compares CVCL and CLIP models on text-vision matching using the SyntheticKonkle dataset.\n",
    "The task is 4-way classification where distractors are from the SAME class but DIFFERENT textures.\n",
    "\n",
    "## Test Characteristics\n",
    "- **Visual**: All candidates from SAME class, with controlled color and size\n",
    "- **Variation**: Only texture differs between candidates (smooth vs bumpy)\n",
    "- **Control**: Color and size held constant visually but NOT mentioned in text\n",
    "- **Text format**: `\"{texture} {class}\"` (e.g., \"smooth apple\", \"bumpy apple\")\n",
    "- **Difficulty**: Very hard - only texture provides discrimination within same class\n",
    "\n",
    "## Special Note on 4-way Choice with 2 Textures\n",
    "Since there are only 2 texture values (smooth and bumpy), we create 4-way choice by:\n",
    "- Using 3 images of one texture (randomly chosen)\n",
    "- Using 1 image of the other texture\n",
    "- This creates an unambiguous mapping where each image corresponds to exactly one correct answer\n",
    "- Example: 3 smooth apples + 1 bumpy apple, where each specific image must be matched correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7865 images with texture annotations\n",
      "Unique classes: 67\n",
      "Unique textures: ['bumpy', 'smooth']\n",
      "Unique colors: 11\n",
      "Unique sizes: 3\n",
      "\n",
      "Class-Color-Size combinations with both textures: 1961\n",
      "Examples: ['abacus_black_large', 'abacus_black_medium', 'abacus_black_small']\n",
      "\n",
      "Sample data:\n",
      "    class texture   color   size\n",
      "0  abacus   bumpy     red  large\n",
      "1  abacus   bumpy   green  large\n",
      "2  abacus   bumpy    blue  large\n",
      "3  abacus   bumpy  yellow  large\n",
      "4  abacus   bumpy  orange  large\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_synthetic_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset with metadata for texture testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid texture values\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_size'] = df['class'] + '_' + df['color'] + '_' + df['size']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['size'] + '_' + df['texture']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    \n",
    "    # Find class-color-size combinations that have both textures\n",
    "    ccs_groups = df.groupby('class_color_size')['texture'].nunique()\n",
    "    valid_ccs = ccs_groups[ccs_groups == 2].index.tolist()  # Must have both smooth and bumpy\n",
    "    \n",
    "    print(f\"\\nClass-Color-Size combinations with both textures: {len(valid_ccs)}\")\n",
    "    if len(valid_ccs) > 0:\n",
    "        print(f\"Examples: {valid_ccs[:3]}\")\n",
    "    \n",
    "    return df, valid_ccs\n",
    "\n",
    "# Load data\n",
    "data_df, valid_combinations = load_synthetic_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'texture', 'color', 'size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_scdt_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n    \"\"\"Run Same Class Different Texture text-vision test\n    \n    Test design:\n    - Query: One image of a specific texture\n    - Distractors: 3 images of the SAME CLASS but with different texture distribution\n    - Colors and sizes can vary for image diversity\n    - Text format: \"{texture} {class}\" (e.g., \"smooth apple\", \"bumpy apple\")\n    - Uses 3-1 split: 3 images of one texture, 1 of the other\n    \n    Args:\n        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n        seed: Random seed for reproducibility\n        device: Device to use (None for auto-detect)\n        num_trials: Total number of trials to run\n    \"\"\"\n    # Set seeds to match original test methodology\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Running SCDT Text-Vision Test with {model_name}\")\n    print(f\"(Same Class Different Texture - Varied Colors & Sizes)\")\n    print(f\"Text format: {{texture}} {{class}}\")\n    print(f\"Note: Using 3-1 split (3 of one texture, 1 of the other)\")\n    print(f\"{'='*60}\")\n    \n    # Device selection\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    if device == 'cuda' and not torch.cuda.is_available():\n        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n        device = 'cpu'\n    \n    print(f\"Using device: {device}\")\n    \n    # Load model\n    print(f\"[INFO] Loading {model_name} on {device}...\")\n    model, transform = load_model(model_name, seed=seed, device=device)\n    extractor = FeatureExtractor(model_name, model, device)\n    model.eval()\n    \n    # Load and prepare data\n    df = pd.read_csv(METADATA_PATH)\n    \n    # Build full paths\n    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n    \n    # Filter to entries with texture annotation\n    df = df[df['texture'].notna() & (df['texture'] != '')].copy()\n    \n    # Standardize values\n    df['texture'] = df['texture'].str.lower().str.strip()\n    \n    # Filter to only valid texture values\n    valid_textures = ['smooth', 'bumpy']\n    df = df[df['texture'].isin(valid_textures)].copy()\n    \n    print(f\"\\nLoaded {len(df)} images with texture annotations\")\n    print(f\"Unique classes: {df['class'].nunique()}\")\n    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n    \n    # Find classes that have both textures with enough images\n    class_groups = df.groupby('class')\n    valid_classes = []\n    for class_name, group in class_groups:\n        unique_textures = group['texture'].unique()\n        if len(unique_textures) == 2:  # Has both smooth and bumpy\n            texture_counts = group.groupby('texture').size()\n            if texture_counts.min() >= 4:  # At least 4 images per texture (for 3-1 split)\n                valid_classes.append(class_name)\n    \n    if len(valid_classes) == 0:\n        print(\"ERROR: No classes have both textures with enough images.\")\n        print(\"Cannot run SCDT test.\")\n        return [], 0.0\n    \n    print(f\"\\nFound {len(valid_classes)} classes with both textures\")\n    print(f\"Classes (first 10): {sorted(valid_classes)[:10]}\")\n    \n    # Pre-compute image embeddings\n    print(\"\\nExtracting image embeddings...\")\n    image_embeddings = {}\n    skipped_images = []\n    \n    # Get all relevant images\n    df_valid = df[df['class'].isin(valid_classes)]\n    all_image_paths = df_valid['image_path'].unique().tolist()\n    batch_size = 16\n    \n    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n        batch_paths = all_image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for img_path in batch_paths:\n            try:\n                img = Image.open(img_path).convert('RGB')\n                img_processed = transform(img).unsqueeze(0).to(device)\n                batch_images.append((img_path, img_processed))\n            except Exception as e:\n                skipped_images.append(img_path)\n                continue\n        \n        if batch_images:\n            paths = [p for p, _ in batch_images]\n            imgs = torch.cat([img for _, img in batch_images], dim=0)\n            \n            with torch.no_grad():\n                embeddings = extractor.get_img_feature(imgs)\n                embeddings = extractor.norm_features(embeddings)\n            \n            for path, emb in zip(paths, embeddings):\n                image_embeddings[path] = emb.cpu().float()\n    \n    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n    if skipped_images:\n        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n    \n    # Prepare for trials\n    correct_count = 0\n    trial_results = []\n    \n    # Calculate trials per class\n    trials_per_class = num_trials // len(valid_classes)\n    remaining_trials = num_trials % len(valid_classes)\n    \n    print(f\"\\nRunning {num_trials} trials across {len(valid_classes)} classes...\")\n    print(f\"Trials per class: {trials_per_class}, with {remaining_trials} getting 1 extra\")\n    \n    # Run trials\n    for class_idx, class_name in enumerate(tqdm(valid_classes, desc=\"Processing classes\")):\n        # Get all images for this class\n        class_data = df_valid[df_valid['class'] == class_name]\n        \n        # Group by texture\n        smooth_images = class_data[class_data['texture'] == 'smooth']['image_path'].tolist()\n        bumpy_images = class_data[class_data['texture'] == 'bumpy']['image_path'].tolist()\n        \n        # Filter to valid embeddings\n        smooth_images = [p for p in smooth_images if p in image_embeddings]\n        bumpy_images = [p for p in bumpy_images if p in image_embeddings]\n        \n        # Determine number of trials for this class\n        n_trials = trials_per_class + (1 if class_idx < remaining_trials else 0)\n        \n        for trial in range(n_trials):\n            if len(trial_results) >= num_trials:\n                break\n            \n            # Randomly choose which texture gets 3 images vs 1\n            if random.random() < 0.5:\n                majority_texture = 'smooth'\n                minority_texture = 'bumpy'\n                majority_images = smooth_images\n                minority_images = bumpy_images\n            else:\n                majority_texture = 'bumpy'\n                minority_texture = 'smooth'\n                majority_images = bumpy_images\n                minority_images = smooth_images\n            \n            # Need at least 3 majority and 1 minority\n            if len(majority_images) < 3 or len(minority_images) < 1:\n                continue\n            \n            # Select 3 different images from majority texture (can be different colors/sizes)\n            selected_majority = random.sample(majority_images, 3)\n            # Select 1 from minority texture\n            selected_minority = random.sample(minority_images, 1)\n            \n            # Build candidates list with (image_path, texture) tuples\n            candidates = []\n            for img_path in selected_majority:\n                candidates.append((img_path, majority_texture))\n            for img_path in selected_minority:\n                candidates.append((img_path, minority_texture))\n            \n            # Select query from candidates\n            query_idx = random.randint(0, 3)\n            query_img_path, query_texture = candidates[query_idx]\n            \n            # Create text prompts\n            candidate_texts = [f\"{texture} {class_name.lower()}\" for _, texture in candidates]\n            \n            # Shuffle for random presentation\n            shuffled_order = list(range(4))\n            random.shuffle(shuffled_order)\n            shuffled_candidates = [candidates[i] for i in shuffled_order]\n            shuffled_texts = [candidate_texts[i] for i in shuffled_order]\n            correct_idx = shuffled_order.index(query_idx)\n            \n            # Encode text prompts\n            with torch.no_grad():\n                if \"clip\" in model_name:\n                    tokens = clip.tokenize(shuffled_texts, truncate=True).to(device)\n                    txt_features = model.encode_text(tokens)\n                    txt_features = extractor.norm_features(txt_features)\n                else:  # CVCL\n                    tokens, token_len = model.tokenize(shuffled_texts)\n                    tokens = tokens.to(device)\n                    if isinstance(token_len, torch.Tensor):\n                        token_len = token_len.to(device)\n                    txt_features = model.encode_text(tokens, token_len)\n                    txt_features = extractor.norm_features(txt_features)\n            \n            # Get query image embedding\n            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n            \n            # Calculate similarity\n            query_embedding = query_embedding.float()\n            txt_features = txt_features.float()\n            \n            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n            \n            # Get prediction\n            pred_idx = similarity.argmax(dim=1).item()\n            \n            # Check if correct\n            is_correct = (pred_idx == correct_idx)\n            if is_correct:\n                correct_count += 1\n            \n            # Store trial result\n            trial_results.append({\n                'trial': len(trial_results) + 1,\n                'query_class': class_name,\n                'query_texture': query_texture,\n                'query_img': os.path.basename(query_img_path),\n                'correct_idx': correct_idx,\n                'predicted_idx': pred_idx,\n                'correct': is_correct,\n                'candidate_texts': shuffled_texts,\n                'similarity_scores': similarity.cpu().numpy().tolist()\n            })\n    \n    # Calculate accuracy\n    accuracy = correct_count / len(trial_results) if trial_results else 0\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Results for {model_name} - SCDT Text-Vision Test:\")\n    print(f\"Total trials: {len(trial_results)}\")\n    print(f\"Correct: {correct_count}\")\n    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"{'='*60}\")\n    \n    # Save results\n    results_row = {\n        'Model': model_name,\n        'Test': 'SCDT-TextVision',\n        'Dataset': 'SyntheticKonkle',\n        'Correct': correct_count,\n        'Trials': len(trial_results),\n        'Accuracy': accuracy\n    }\n    \n    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n    if os.path.exists(RESULTS_PATH):\n        results_df = pd.read_csv(RESULTS_PATH)\n    else:\n        results_df = pd.DataFrame()\n    \n    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n    print(f\"\\nResults saved to {RESULTS_PATH}\")\n    \n    return trial_results, accuracy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDT Text-Vision Test with cvcl-resnext\n",
      "(Same Class Different Texture - Controlled Color & Size)\n",
      "Text format: {texture} {class}\n",
      "Note: Using 3-1 split (3 of one texture, 1 of the other)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 16 class-color-size combinations suitable for testing\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 5/5 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 64 images\n",
      "Skipped 10 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 16 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 16/16 [00:07<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDT Text-Vision Test:\n",
      "Total trials: 879\n",
      "Correct: 246\n",
      "Accuracy: 0.2799 (27.99%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_scdt_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDT Text-Vision Test with clip-resnext\n",
      "(Same Class Different Texture - Controlled Color & Size)\n",
      "Text format: {texture} {class}\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 1961 class-color-size combinations with both textures\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/488 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 488/488 [00:17<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7786 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 1961 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 1961/1961 [00:21<00:00, 89.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1322\n",
      "Accuracy: 0.3305 (33.05%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_scdt_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"SCDT TEXT-VISION TEST COMPARISON\")\nprint(\"=\"*60)\nprint(f\"\\nTest: Same Class Different Texture (4-way forced choice)\")\nprint(f\"Control: Colors and sizes can vary (not mentioned in text)\")\nprint(f\"Text format: '{{texture}} {{class}}'\")\nprint(f\"Implementation: 3-1 split (3 of one texture, 1 of another)\")\nprint(f\"\\nResults:\")\nprint(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\nprint(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\nprint(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\nif cvcl_accuracy > clip_accuracy:\n    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\nelif clip_accuracy > cvcl_accuracy:\n    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\nelse:\n    print(\"Both models perform equally\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAnalysis:\")\nprint(\"- Tests pure texture discrimination within same class\")\nprint(\"- Texture is a subtle tactile property\")\nprint(\"- Colors/sizes vary to increase available test data\")\nprint(\"- Models must encode and match texture information\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDT Text-Vision Test Characteristics:\n",
    "- **Visual Control**: All 4 candidates have same class, color, and size\n",
    "- **Variation**: Only texture differs (smooth vs bumpy)\n",
    "- **Text Prompts**: \"{texture} {class}\" (e.g., \"smooth apple\")\n",
    "- **4-way Implementation**: Each texture appears twice (2 smooth, 2 bumpy)\n",
    "\n",
    "### What This Tests:\n",
    "- Pure texture discrimination within same class\n",
    "- Model's ability to encode tactile/surface properties\n",
    "- Whether texture information transfers from vision to language\n",
    "\n",
    "### Expected Performance:\n",
    "- Likely the hardest test - texture is subtle\n",
    "- Performance may be near chance (25%) if models don't encode texture well\n",
    "- CVCL might have advantage if child-directed speech emphasizes texture\n",
    "\n",
    "### Why This Is Challenging:\n",
    "- Texture is primarily a tactile property\n",
    "- Visual texture cues can be subtle\n",
    "- Only 2 texture values limit discrimination\n",
    "- Same class constraint removes object-level cues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}