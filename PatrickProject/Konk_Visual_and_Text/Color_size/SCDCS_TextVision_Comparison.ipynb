{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Class Different Color and Size (SCDCS) Text-Vision Comparison - SyntheticKonkle\n",
    "\n",
    "This notebook compares CVCL and CLIP models on text-vision matching using the SyntheticKonkle dataset.\n",
    "The task is 4-way classification where distractors are from the SAME class but DIFFERENT colors AND sizes.\n",
    "\n",
    "## Text Ordering Considerations\n",
    "\n",
    "**We use natural English adjective ordering**: `\"{size} {color} {class}\"`\n",
    "- Example: \"large red apple\", \"small green apple\", \"medium blue apple\"\n",
    "- This follows standard English grammar rules (opinion → size → age → shape → color → origin → material → purpose → noun)\n",
    "- Both CVCL (trained on child-directed speech) and CLIP (trained on internet text) expect this natural ordering\n",
    "- Parents typically say \"big red ball\" not \"red big ball\" or \"ball big red\"\n",
    "\n",
    "## Test Characteristics\n",
    "- **Visual**: All candidates from SAME class, with controlled texture\n",
    "- **Variation**: Both color AND size differ between candidates\n",
    "- **Control**: Texture held constant visually but NOT mentioned in text\n",
    "- **Difficulty**: Medium - harder than DCDC/DCDS (where class also varies) but easier than pure color or size tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7882 images with size, color, and texture annotations\n",
      "Unique classes: 67\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Size values: ['bumpy', 'large', 'medium', 'small']\n",
      "Color values: ['ball', 'black', 'blue', 'brown', 'gray', 'green', 'orange', 'pink', 'purple', 'red']...\n",
      "\n",
      "Class-Texture combinations with 3+ color-size pairs: 136\n",
      "Examples: ['abacus_bumpy', 'abacus_smooth', 'apple_bumpy']\n",
      "  abacus_bumpy has color-size pairs like: ['red_large' 'green_large' 'blue_large' 'yellow_large']\n",
      "\n",
      "Sample data:\n",
      "    class   color   size texture    color_size\n",
      "0  abacus     red  large   bumpy     red_large\n",
      "1  abacus   green  large   bumpy   green_large\n",
      "2  abacus    blue  large   bumpy    blue_large\n",
      "3  abacus  yellow  large   bumpy  yellow_large\n",
      "4  abacus  orange  large   bumpy  orange_large\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_synthetic_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset with metadata for color-size testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_texture'] = df['class'] + '_' + df['texture']\n",
    "    df['color_size'] = df['color'] + '_' + df['size']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with size, color, and texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    print(f\"Size values: {sorted(df['size'].unique())}\")\n",
    "    print(f\"Color values: {sorted(df['color'].unique())[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Find class-texture combinations that have multiple color-size pairs\n",
    "    ct_groups = df.groupby('class_texture')['color_size'].nunique()\n",
    "    valid_ct = ct_groups[ct_groups >= 3].index.tolist()\n",
    "    \n",
    "    print(f\"\\nClass-Texture combinations with 3+ color-size pairs: {len(valid_ct)}\")\n",
    "    if len(valid_ct) > 0:\n",
    "        print(f\"Examples: {valid_ct[:3]}\")\n",
    "        # Show color-size distribution for first example\n",
    "        if len(valid_ct) > 0:\n",
    "            example = valid_ct[0]\n",
    "            color_sizes = df[df['class_texture'] == example]['color_size'].unique()[:4]\n",
    "            print(f\"  {example} has color-size pairs like: {color_sizes}\")\n",
    "    \n",
    "    return df, valid_ct\n",
    "\n",
    "# Load data\n",
    "data_df, valid_combinations = load_synthetic_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'size', 'texture', 'color_size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scdcs_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Same Class Different Color and Size text-vision test with controlled texture\n",
    "    \n",
    "    Text format uses natural English ordering: \"{size} {color} {class}\"\n",
    "    Example: \"large red apple\", \"small green apple\"\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDCS Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Same Class Different Color & Size - Controlled Texture)\")\n",
    "    print(f\"Text format: {{size}} {{color}} {{class}} (natural English order)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_texture'] = df['class'] + '_' + df['texture']\n",
    "    df['color_size'] = df['color'] + '_' + df['size']\n",
    "    \n",
    "    # Find class-texture combinations with at least 3 different color-size pairs\n",
    "    ct_groups = df.groupby('class_texture')\n",
    "    valid_ct = []\n",
    "    for ct, group in ct_groups:\n",
    "        unique_color_sizes = group['color_size'].unique()\n",
    "        if len(unique_color_sizes) >= 3:\n",
    "            valid_ct.append(ct)\n",
    "    \n",
    "    if len(valid_ct) == 0:\n",
    "        print(\"ERROR: No class-texture combinations have 3+ different color-size pairs.\")\n",
    "        print(\"Cannot run SCDCS test with strict controls.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_ct)} class-texture combinations with 3+ color-size pairs\")\n",
    "    \n",
    "    # Pre-compute image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['class_texture'].isin(valid_ct)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per combination\n",
    "    trials_per_ct = num_trials // len(valid_ct)\n",
    "    remaining_trials = num_trials % len(valid_ct)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_ct)} combinations...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for ct_idx, ct in enumerate(tqdm(valid_ct, desc=\"Processing combinations\")):\n",
    "        # Get all images for this class-texture combination\n",
    "        ct_data = df_valid[df_valid['class_texture'] == ct]\n",
    "        \n",
    "        # Group by color-size\n",
    "        cs_groups = ct_data.groupby('color_size').agg({\n",
    "            'image_path': list,\n",
    "            'color': 'first',\n",
    "            'size': 'first'\n",
    "        }).to_dict('index')\n",
    "        \n",
    "        available_color_sizes = list(cs_groups.keys())\n",
    "        \n",
    "        if len(available_color_sizes) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Parse class from combination string\n",
    "        class_name = ct.split('_')[0]\n",
    "        \n",
    "        # Determine number of trials for this combination\n",
    "        n_trials = trials_per_ct + (1 if ct_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Select color-size pairs for 4-way choice\n",
    "            if len(available_color_sizes) == 3:\n",
    "                # Use all 3 pairs plus duplicate one for 4-way choice\n",
    "                selected_pairs = available_color_sizes.copy()\n",
    "                selected_pairs.append(random.choice(available_color_sizes))\n",
    "            else:\n",
    "                # Select 4 different pairs if possible\n",
    "                selected_pairs = random.sample(available_color_sizes, min(4, len(available_color_sizes)))\n",
    "            \n",
    "            # First pair is the query\n",
    "            query_pair = selected_pairs[0]\n",
    "            query_data = cs_groups[query_pair]\n",
    "            \n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            query_color = query_data['color']\n",
    "            query_size = query_data['size']\n",
    "            \n",
    "            # Shuffle for candidate order\n",
    "            random.shuffle(selected_pairs)\n",
    "            correct_idx = selected_pairs.index(query_pair)\n",
    "            \n",
    "            # Create text prompts - NATURAL ENGLISH ORDER: {size} {color} {class}\n",
    "            candidate_texts = []\n",
    "            for pair in selected_pairs:\n",
    "                pair_data = cs_groups[pair]\n",
    "                # Natural English order: size before color before noun\n",
    "                text_prompt = f\"{pair_data['size']} {pair_data['color']} {class_name.lower()}\"\n",
    "                candidate_texts.append(text_prompt)\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': class_name,\n",
    "                'query_color': query_color,\n",
    "                'query_size': query_size,\n",
    "                'class_texture': ct,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': candidate_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDCS Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDCS-TextVision',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDCS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCS Text-Vision Test with cvcl-resnext\n",
      "(Same Class Different Color & Size - Controlled Texture)\n",
      "Text format: {size} {color} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 136 class-texture combinations with 3+ color-size pairs\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 492/492 [00:21<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7841 images\n",
      "Skipped 31 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 136 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 136/136 [00:38<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDCS Text-Vision Test:\n",
      "Total trials: 3971\n",
      "Correct: 1078\n",
      "Accuracy: 0.2715 (27.15%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_scdcs_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDCS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCS Text-Vision Test with clip-resnext\n",
      "(Same Class Different Color & Size - Controlled Texture)\n",
      "Text format: {size} {color} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 136 class-texture combinations with 3+ color-size pairs\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/492 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 492/492 [00:17<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7841 images\n",
      "Skipped 31 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 136 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 136/136 [00:20<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDCS Text-Vision Test:\n",
      "Total trials: 3971\n",
      "Correct: 3599\n",
      "Accuracy: 0.9063 (90.63%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_scdcs_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCDCS TEXT-VISION TEST COMPARISON\n",
      "============================================================\n",
      "\n",
      "Test: Same Class Different Color & Size (4-way forced choice)\n",
      "Control: Texture held constant (not mentioned in text)\n",
      "Text format: '{size} {color} {class}' (natural English order)\n",
      "Example: 'large red apple' vs 'small green apple'\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2715 (27.15%)\n",
      "  CLIP Accuracy: 0.9063 (90.63%)\n",
      "\n",
      "Difference: 0.6349 (63.49%)\n",
      "CLIP performs better by 63.49%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- Tests multi-attribute discrimination within same class\n",
      "- Both color AND size provide discriminative signals\n",
      "- Natural English ordering helps both models\n",
      "- Should perform better than single-attribute tests (SCDC or SCDS)\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDCS TEXT-VISION TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Same Class Different Color & Size (4-way forced choice)\")\n",
    "print(f\"Control: Texture held constant (not mentioned in text)\")\n",
    "print(f\"Text format: '{{size}} {{color}} {{class}}' (natural English order)\")\n",
    "print(f\"Example: 'large red apple' vs 'small green apple'\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Tests multi-attribute discrimination within same class\")\n",
    "print(\"- Both color AND size provide discriminative signals\")\n",
    "print(\"- Natural English ordering helps both models\")\n",
    "print(\"- Should perform better than single-attribute tests (SCDC or SCDS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### SCDCS Text-Vision Test Characteristics:\n",
    "- **Visual Control**: All 4 candidates have same class and texture\n",
    "- **Variation**: Both color AND size differ between candidates\n",
    "- **Text Prompts**: Natural order \"{size} {color} {class}\" (e.g., \"large red apple\")\n",
    "- **NOT mentioned**: Texture is controlled but excluded from text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
