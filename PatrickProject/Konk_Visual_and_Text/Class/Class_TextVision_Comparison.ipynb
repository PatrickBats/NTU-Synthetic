{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------  12.6/12.8 MB 87.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 72.9 MB/s  0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Text-Vision Comparison - KonkLab\n",
    "\n",
    "This notebook compares CVCL and CLIP models using text-based prototypes for class discrimination.\n",
    "Instead of averaging image features to create prototypes, we use text descriptions.\n",
    "The task remains 4-way forced choice classification with 4000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import clip\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# SyntheticKonkle paths - Updated from KonkLab\n",
    "CSV_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "# Fixed path - the resized images have an extra nested folder\n",
    "IMG_DIR = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "MASTER_CSV = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "CUDA version: 11.8\n",
      "GPU test time: 0.017s\n"
     ]
    }
   ],
   "source": [
    "# Quick test to check if GPU is available and model loading speed\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Test GPU speed\n",
    "    x = torch.randn(32, 3, 224, 224).cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = x * 2\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU test time: {time.time() - start:.3f}s\")\n",
    "else:\n",
    "    print(\"WARNING: Running on CPU will be VERY slow!\")\n",
    "    print(\"If you have a GPU, make sure CUDA is properly installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class with optimized loading for SyntheticKonkle\n",
    "class ClassImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform):\n",
    "        print(f\"[DEBUG] Loading CSV from: {csv_path}\")\n",
    "        print(f\"[DEBUG] Image directory: {img_dir}\")\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        print(f\"[DEBUG] CSV loaded with {len(self.df)} rows\")\n",
    "        \n",
    "        # SyntheticKonkle has lowercase 'class' instead of 'Class'\n",
    "        assert 'filename' in self.df and 'class' in self.df and 'folder' in self.df, \\\n",
    "            \"CSV needs filename, class, and folder columns\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Pre-compute paths to avoid repeated joins\n",
    "        # SyntheticKonkle structure: {img_dir}/{folder}/{filename}\n",
    "        self.paths = [os.path.join(img_dir, row['folder'], row['filename']) \n",
    "                      for _, row in self.df.iterrows()]\n",
    "        \n",
    "        print(f\"[DEBUG] Generated {len(self.paths)} image paths\")\n",
    "        print(f\"[DEBUG] First path: {self.paths[0] if self.paths else 'None'}\")\n",
    "        print(f\"[DEBUG] First path exists: {os.path.exists(self.paths[0]) if self.paths else 'N/A'}\")\n",
    "        \n",
    "        # Pre-filter to only valid images\n",
    "        valid_indices = []\n",
    "        for idx, path in enumerate(self.paths):\n",
    "            try:\n",
    "                # Quick check if image can be opened\n",
    "                img = Image.open(path)\n",
    "                img.close()\n",
    "                valid_indices.append(idx)\n",
    "            except:\n",
    "                if idx < 5:  # Print first few failures\n",
    "                    print(f\"[DEBUG] Failed to open image {idx}: {path}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"[DEBUG] Found {len(valid_indices)} valid images out of {len(self.paths)}\")\n",
    "        \n",
    "        # Filter dataframe and paths to only valid images\n",
    "        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.paths = [self.paths[i] for i in valid_indices]\n",
    "        print(f\"Dataset initialized with {len(self.paths)} valid images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        cls = row['class']  # lowercase in SyntheticKonkle\n",
    "        # Use pre-computed path\n",
    "        path = self.paths[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load image at index {idx}: {path}\")\n",
    "            # Return a blank image if corrupted\n",
    "            img = Image.new('RGB', (224, 224), color='white')\n",
    "        return self.transform(img), cls, idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch])\n",
    "    classes = [b[1] for b in batch]\n",
    "    idxs = [b[2] for b in batch]\n",
    "    return imgs, classes, idxs\n",
    "\n",
    "def run_class_text_vision_test(model_name, seed=0, device=None, batch_size=16, \n",
    "                                trials_per_class=None, max_trials=4000):\n",
    "    \"\"\"\n",
    "    Run 4-way classification test using the EXACT methodology from discover-hidden-visual-concepts predict() method.\n",
    "    Now using SyntheticKonkle dataset.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    if device == 'cpu':\n",
    "        print(\"[WARNING] Running on CPU - this will be SLOW!\")\n",
    "        print(\"Reducing batch size to 4 for CPU\")\n",
    "        batch_size = 4\n",
    "    else:\n",
    "        print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Check if model supports text encoding\n",
    "    if model_name in ['resnext', 'dino_s_resnext50']:\n",
    "        print(f\"[WARNING] {model_name} has no text encoder, skipping\")\n",
    "        return {}, 0.0\n",
    "\n",
    "    # 1) Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    print(f\"[INFO] Model loaded in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    # 2) Load dataset\n",
    "    print(f\"[DEBUG] Creating dataset with CSV_PATH={CSV_PATH}\")\n",
    "    print(f\"[DEBUG] Creating dataset with IMG_DIR={IMG_DIR}\")\n",
    "    ds = ClassImageDataset(CSV_PATH, IMG_DIR, transform)\n",
    "    print(f\"[INFO] Dataset: {len(ds)} images\")\n",
    "    \n",
    "    if len(ds) == 0:\n",
    "        print(\"[ERROR] Dataset is empty! No valid images found.\")\n",
    "        return {}, 0.0\n",
    "    \n",
    "    # Use multiple workers only if not on Windows or if on Linux/Mac\n",
    "    num_workers = 0 if os.name == 'nt' else 2\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                   num_workers=num_workers, collate_fn=collate_fn, \n",
    "                   pin_memory=(device=='cuda'))\n",
    "    \n",
    "    # 3) Extract image embeddings\n",
    "    print(f\"[INFO] Extracting embeddings (batch_size={batch_size})...\")\n",
    "    all_img_embs, all_classes, all_idxs = [], [], []\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, classes, idxs in tqdm(dl, desc=\"Extracting embeddings\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            feats = extractor.get_img_feature(imgs)\n",
    "            feats = extractor.norm_features(feats).cpu()\n",
    "            \n",
    "            all_img_embs.append(feats)\n",
    "            all_classes.extend(classes)\n",
    "            all_idxs.extend(idxs)\n",
    "    \n",
    "    if not all_img_embs:\n",
    "        print(\"[ERROR] No embeddings extracted!\")\n",
    "        return {}, 0.0\n",
    "                \n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    print(f\"[INFO] Extracted {len(all_idxs)} embeddings in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    # 4) Encode text labels\n",
    "    unique_classes = list(set(all_classes))\n",
    "    print(f\"[INFO] Encoding {len(unique_classes)} class labels...\")\n",
    "    \n",
    "    class_text_features = {}\n",
    "    with torch.no_grad():\n",
    "        if \"clip\" in model_name:\n",
    "            tokens = clip.tokenize(unique_classes, truncate=True).to(device)\n",
    "            txt_features = model.encode_text(tokens)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu()\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "        else:  # CVCL\n",
    "            tokens, token_len = model.tokenize(unique_classes)\n",
    "            tokens = tokens.to(device)\n",
    "            if isinstance(token_len, torch.Tensor):\n",
    "                token_len = token_len.to(device)\n",
    "            txt_features = model.encode_text(tokens, token_len)\n",
    "            txt_features = extractor.norm_features(txt_features).cpu()\n",
    "            for i, cls in enumerate(unique_classes):\n",
    "                class_text_features[cls] = txt_features[i]\n",
    "    \n",
    "    print(f\"[INFO] Text encoding complete\")\n",
    "    \n",
    "    # 5) Build mappings\n",
    "    idx2class = {i:c for i,c in zip(all_idxs, all_classes)}\n",
    "    idx2row = {i:r for r,i in enumerate(all_idxs)}\n",
    "    class2idxs = defaultdict(list)\n",
    "    for i,c in idx2class.items():\n",
    "        class2idxs[c].append(i)\n",
    "\n",
    "    # 6) Run trials - FIXED to run exactly max_trials\n",
    "    class_results = {}\n",
    "    total_correct = 0\n",
    "    total_trials = 0\n",
    "    \n",
    "    # Get valid classes (those with at least 1 image)\n",
    "    valid_classes = [c for c in class2idxs if len(class2idxs[c]) >= 1]\n",
    "    n_classes = len(valid_classes)\n",
    "    \n",
    "    # Calculate trials per class if not specified\n",
    "    if trials_per_class is None:\n",
    "        # Distribute trials evenly across classes, with some getting extra\n",
    "        trials_per_class = max_trials // n_classes\n",
    "        extra_trials = max_trials % n_classes\n",
    "    else:\n",
    "        extra_trials = 0\n",
    "    \n",
    "    print(f\"[INFO] Running {max_trials} total trials across {n_classes} classes\")\n",
    "    print(f\"[INFO] Base trials per class: {trials_per_class}, extra trials for first {extra_trials} classes\")\n",
    "    \n",
    "    # Run trials for each class\n",
    "    for class_idx, cls in enumerate(valid_classes):\n",
    "        if total_trials >= max_trials:\n",
    "            break\n",
    "            \n",
    "        idxs = class2idxs[cls]\n",
    "        \n",
    "        # Add extra trial for first few classes to reach exactly max_trials\n",
    "        current_trials = trials_per_class + (1 if class_idx < extra_trials else 0)\n",
    "        # Don't exceed max_trials\n",
    "        current_trials = min(current_trials, max_trials - total_trials)\n",
    "        \n",
    "        correct = 0\n",
    "        txt_feature = class_text_features[cls].unsqueeze(0)\n",
    "        \n",
    "        for trial in range(current_trials):\n",
    "            # Pick query and distractors\n",
    "            q = random.choice(idxs)\n",
    "            others = [i for i in all_idxs if idx2class[i] != cls]\n",
    "            if len(others) < 3:\n",
    "                continue\n",
    "            distractors = random.sample(others, 3)\n",
    "            \n",
    "            # 4-way classification\n",
    "            candidates = [q] + distractors\n",
    "            cand_features = torch.stack([all_img_embs[idx2row[i]] for i in candidates])\n",
    "            cand_features = cand_features.unsqueeze(0)\n",
    "            \n",
    "            # Compute similarity\n",
    "            txt_feature_expanded = txt_feature.unsqueeze(1)\n",
    "            similarity = (100.0 * cand_features @ txt_feature_expanded.transpose(-2, -1)).softmax(dim=1)\n",
    "            similarity = similarity.squeeze()\n",
    "            \n",
    "            # Predict (query is at index 0)\n",
    "            if similarity.argmax().item() == 0:\n",
    "                correct += 1\n",
    "                total_correct += 1\n",
    "            total_trials += 1\n",
    "\n",
    "        acc = correct / current_trials if current_trials > 0 else 0\n",
    "        class_results[cls] = {'correct': correct, 'trials': current_trials, 'accuracy': acc}\n",
    "        \n",
    "        if total_trials % 500 == 0:\n",
    "            print(f\"[PROGRESS] {total_trials}/{max_trials} trials completed\")\n",
    "\n",
    "    # Final progress update\n",
    "    print(f\"[FINAL] Completed {total_trials} trials\")\n",
    "    \n",
    "    overall_acc = total_correct / total_trials if total_trials else 0.0\n",
    "    \n",
    "    print(f\"\\n[RESULTS] Accuracy: {total_correct}/{total_trials} ({overall_acc:.1%})\")\n",
    "    \n",
    "    # Save results\n",
    "    summary_df = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Test': 'Class-TextVision-Original',\n",
    "        'Dataset': 'SyntheticKonkle',  # Changed from KonkLab\n",
    "        'Correct': total_correct,\n",
    "        'Trials': total_trials,\n",
    "        'Accuracy': overall_acc\n",
    "    }])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(MASTER_CSV), exist_ok=True)\n",
    "    if os.path.exists(MASTER_CSV):\n",
    "        summary_df.to_csv(MASTER_CSV, mode='a', header=False, index=False, float_format='%.4f')\n",
    "    else:\n",
    "        summary_df.to_csv(MASTER_CSV, index=False, float_format='%.4f')\n",
    "\n",
    "    return class_results, overall_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVCL Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded in 0.6s\n",
      "[DEBUG] Creating dataset with CSV_PATH=C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Creating dataset with IMG_DIR=C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] Loading CSV from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Image directory: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] CSV loaded with 7882 rows\n",
      "[DEBUG] Generated 7882 image paths\n",
      "[DEBUG] First path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\\abacus_color\\abacus_large_bumpy_01_red.png\n",
      "[DEBUG] First path exists: True\n",
      "[DEBUG] Found 7849 valid images out of 7882\n",
      "Dataset initialized with 7849 valid images\n",
      "[INFO] Dataset: 7849 images\n",
      "[INFO] Extracting embeddings (batch_size=16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 491/491 [00:22<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracted 7849 embeddings in 22.0s\n",
      "[INFO] Encoding 67 class labels...\n",
      "[INFO] Text encoding complete\n",
      "[INFO] Running 4000 total trials across 67 classes\n",
      "[INFO] Base trials per class: 59, extra trials for first 47 classes\n",
      "[PROGRESS] 1500/4000 trials completed\n",
      "[PROGRESS] 4000/4000 trials completed\n",
      "[FINAL] Completed 4000 trials\n",
      "\n",
      "[RESULTS] Accuracy: 1086/4000 (27.2%)\n",
      "\n",
      "CVCL Results per Class:\n",
      "ring                : 44/60 (73.3%)\n",
      "grill               : 38/60 (63.3%)\n",
      "butterfly           : 31/60 (51.7%)\n",
      "microwave           : 31/60 (51.7%)\n",
      "apple               : 30/60 (50.0%)\n",
      "phone               : 30/60 (50.0%)\n",
      "stapler             : 29/59 (49.2%)\n",
      "basket              : 29/60 (48.3%)\n",
      "wineglass           : 23/59 (39.0%)\n",
      "bonzai              : 23/60 (38.3%)\n",
      "dresser             : 23/60 (38.3%)\n",
      "telescope           : 21/59 (35.6%)\n",
      "goggle              : 21/60 (35.0%)\n",
      "doll                : 20/60 (33.3%)\n",
      "pumpkin             : 20/60 (33.3%)\n",
      "seashell            : 19/59 (32.2%)\n",
      "horse               : 19/59 (32.2%)\n",
      "trophy              : 19/59 (32.2%)\n",
      "lei                 : 19/60 (31.7%)\n",
      "abacus              : 18/60 (30.0%)\n",
      "\n",
      "CVCL Overall Accuracy: 27.2%\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL text-vision classification\n",
    "cvcl_results, cvcl_overall = run_class_text_vision_test('cvcl-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCVCL Results per Class:\")\n",
    "for cls, res in sorted(cvcl_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCVCL Overall Accuracy: {cvcl_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "[INFO] Model loaded in 1.7s\n",
      "[DEBUG] Creating dataset with CSV_PATH=C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Creating dataset with IMG_DIR=C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] Loading CSV from: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "[DEBUG] Image directory: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "[DEBUG] CSV loaded with 7882 rows\n",
      "[DEBUG] Generated 7882 image paths\n",
      "[DEBUG] First path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\\abacus_color\\abacus_large_bumpy_01_red.png\n",
      "[DEBUG] First path exists: True\n",
      "[DEBUG] Found 7849 valid images out of 7882\n",
      "Dataset initialized with 7849 valid images\n",
      "[INFO] Dataset: 7849 images\n",
      "[INFO] Extracting embeddings (batch_size=16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/491 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 491/491 [00:18<00:00, 26.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracted 7849 embeddings in 18.8s\n",
      "[INFO] Encoding 67 class labels...\n",
      "[INFO] Text encoding complete\n",
      "[INFO] Running 4000 total trials across 67 classes\n",
      "[INFO] Base trials per class: 59, extra trials for first 47 classes\n",
      "[PROGRESS] 1500/4000 trials completed\n",
      "[PROGRESS] 4000/4000 trials completed\n",
      "[FINAL] Completed 4000 trials\n",
      "\n",
      "[RESULTS] Accuracy: 3367/4000 (84.2%)\n",
      "\n",
      "CLIP Results per Class:\n",
      "bird                : 60/60 (100.0%)\n",
      "calculator          : 60/60 (100.0%)\n",
      "doll                : 60/60 (100.0%)\n",
      "earrings            : 60/60 (100.0%)\n",
      "muffin              : 60/60 (100.0%)\n",
      "tennisracquet       : 59/59 (100.0%)\n",
      "rabbit              : 59/59 (100.0%)\n",
      "wineglass           : 59/59 (100.0%)\n",
      "apple               : 59/60 (98.3%)\n",
      "axe                 : 59/60 (98.3%)\n",
      "christmastreeornamentball: 59/60 (98.3%)\n",
      "handbag             : 59/60 (98.3%)\n",
      "handgun             : 59/60 (98.3%)\n",
      "horse               : 58/59 (98.3%)\n",
      "vase                : 58/59 (98.3%)\n",
      "nailpolish          : 58/60 (96.7%)\n",
      "pen                 : 58/60 (96.7%)\n",
      "pitcher             : 58/60 (96.7%)\n",
      "babushkadolls       : 57/60 (95.0%)\n",
      "butterfly           : 57/60 (95.0%)\n",
      "\n",
      "CLIP Overall Accuracy: 84.2%\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP text-vision classification\n",
    "clip_results, clip_overall = run_class_text_vision_test('clip-resnext', max_trials=4000)\n",
    "\n",
    "print(\"\\nCLIP Results per Class:\")\n",
    "for cls, res in sorted(clip_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)[:20]:\n",
    "    print(f\"{cls:20s}: {res['correct']}/{res['trials']} ({res['accuracy']:.1%})\")\n",
    "print(f\"\\nCLIP Overall Accuracy: {clip_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEXT-VISION vs VISUAL PROTOTYPE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Text-Vision Classification (using text descriptions as prototypes):\n",
      "  CVCL: 27.2%\n",
      "  CLIP: 84.2%\n",
      "  CLIP advantage: 57.0 percentage points\n",
      "\n",
      "Note: Compare these results with the visual prototype version\n",
      "to see how text-based prototypes perform vs image-based prototypes.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEXT-VISION vs VISUAL PROTOTYPE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nText-Vision Classification (using text descriptions as prototypes):\")\n",
    "print(f\"  CVCL: {cvcl_overall:.1%}\")\n",
    "print(f\"  CLIP: {clip_overall:.1%}\")\n",
    "print(f\"  CLIP advantage: {(clip_overall - cvcl_overall)*100:.1f} percentage points\")\n",
    "print(f\"\\nNote: Compare these results with the visual prototype version\")\n",
    "print(f\"to see how text-based prototypes perform vs image-based prototypes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
