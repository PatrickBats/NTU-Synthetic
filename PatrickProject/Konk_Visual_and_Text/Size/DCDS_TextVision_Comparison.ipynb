{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Image path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "IMG_PATH = os.path.join(DATA_PATH, )\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Image path: {IMG_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7882 images with size, color, and texture annotations\n",
      "Unique classes: 67\n",
      "Unique sizes: 4\n",
      "Size values: ['bumpy', 'large', 'medium', 'small']\n",
      "Unique color-texture combinations: 38\n",
      "\n",
      "Color-Texture combinations with 3+ class-size pairs: 22\n",
      "Examples: ['black_bumpy', 'black_smooth', 'blue_bumpy']\n",
      "  black_bumpy has class-size pairs like: ['abacus_large' 'abacus_medium' 'abacus_small' 'apple_large']\n",
      "\n",
      "Sample data:\n",
      "    class   size   color texture    class_size\n",
      "0  abacus  large     red   bumpy  abacus_large\n",
      "1  abacus  large   green   bumpy  abacus_large\n",
      "2  abacus  large    blue   bumpy  abacus_large\n",
      "3  abacus  large  yellow   bumpy  abacus_large\n",
      "4  abacus  large  orange   bumpy  abacus_large\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_konklab_data():\n",
    "    \"\"\"Load KonkLab dataset with metadata for size testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['color_texture'] = df['color'] + '_' + df['texture']\n",
    "    df['class_size'] = df['class'] + '_' + df['size']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with size, color, and texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    print(f\"Size values: {sorted(df['size'].unique())}\")\n",
    "    print(f\"Unique color-texture combinations: {df['color_texture'].nunique()}\")\n",
    "    \n",
    "    # Find color-texture combinations that have multiple class-size pairs\n",
    "    # For 4-way choice with 3 sizes, we need at least 3-4 class-size combinations\n",
    "    ct_groups = df.groupby('color_texture')['class_size'].nunique()\n",
    "    valid_ct = ct_groups[ct_groups >= 3].index.tolist()  # Changed from 4 to 3\n",
    "    \n",
    "    print(f\"\\nColor-Texture combinations with 3+ class-size pairs: {len(valid_ct)}\")\n",
    "    if len(valid_ct) > 0:\n",
    "        print(f\"Examples: {valid_ct[:3]}\")\n",
    "        # Show class-size distribution for first example\n",
    "        if len(valid_ct) > 0:\n",
    "            example = valid_ct[0]\n",
    "            class_sizes = df[df['color_texture'] == example]['class_size'].unique()[:4]\n",
    "            print(f\"  {example} has class-size pairs like: {class_sizes}\")\n",
    "    \n",
    "    return df, valid_ct\n",
    "\n",
    "# Load data\n",
    "data_df, valid_color_textures = load_konklab_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'size', 'color', 'texture', 'class_size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dcds_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Different Class Different Size text-vision test with controlled color/texture\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-res')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running DCDS Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Different Class Different Size - Controlled Color/Texture)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    # SyntheticKonkle already has lowercase 'color' column\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['color_texture'] = df['color'] + '_' + df['texture']\n",
    "    df['class_size'] = df['class'] + '_' + df['size']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    # Find color-texture combinations with at least 3 different class-size pairs\n",
    "    ct_groups = df.groupby('color_texture')\n",
    "    valid_ct = []\n",
    "    for ct, group in ct_groups:\n",
    "        unique_class_sizes = group['class_size'].unique()\n",
    "        unique_classes = group['class'].unique()\n",
    "        unique_sizes = group['size'].unique()\n",
    "        # Need at least 3 different class-size combinations for 4-way choice\n",
    "        # We'll duplicate one if needed\n",
    "        if len(unique_class_sizes) >= 3 and len(unique_classes) >= 2 and len(unique_sizes) >= 2:\n",
    "            valid_ct.append(ct)\n",
    "    \n",
    "    if len(valid_ct) == 0:\n",
    "        print(\"ERROR: No color-texture combinations have enough class-size diversity.\")\n",
    "        print(\"Cannot run DCDS test with strict controls.\")\n",
    "        return [], 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_ct)} color-texture combinations with sufficient diversity\")\n",
    "    \n",
    "    # Pre-compute image embeddings\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['color_texture'].isin(valid_ct)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                # Skip corrupted/invalid images\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per color-texture combination\n",
    "    trials_per_ct = num_trials // len(valid_ct)\n",
    "    remaining_trials = num_trials % len(valid_ct)\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_ct)} color-texture combinations...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for ct_idx, ct in enumerate(tqdm(valid_ct, desc=\"Processing combinations\")):\n",
    "        # Get all images for this color-texture combination\n",
    "        ct_data = df_valid[df_valid['color_texture'] == ct]\n",
    "        \n",
    "        # Group by class-size\n",
    "        class_size_groups = ct_data.groupby('class_size').agg({\n",
    "            'image_path': list,\n",
    "            'class': 'first',\n",
    "            'size': 'first'\n",
    "        }).to_dict('index')\n",
    "        \n",
    "        available_class_sizes = list(class_size_groups.keys())\n",
    "        \n",
    "        if len(available_class_sizes) < 3:  # Changed from 4 to 3\n",
    "            continue\n",
    "        \n",
    "        # Determine number of trials for this combination\n",
    "        n_trials = trials_per_ct + (1 if ct_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Select class-size pairs for 4-way choice\n",
    "            if len(available_class_sizes) == 3:\n",
    "                # Use all 3 pairs plus duplicate one for 4-way choice\n",
    "                selected_pairs = available_class_sizes.copy()\n",
    "                # Add a duplicate to make 4 options\n",
    "                selected_pairs.append(random.choice(available_class_sizes))\n",
    "            elif len(available_class_sizes) >= 4:\n",
    "                # Try to select 4 diverse pairs\n",
    "                selected_pairs = []\n",
    "                used_classes = set()\n",
    "                used_sizes = set()\n",
    "                \n",
    "                # Shuffle to get random selection\n",
    "                shuffled_pairs = available_class_sizes.copy()\n",
    "                random.shuffle(shuffled_pairs)\n",
    "                \n",
    "                for pair in shuffled_pairs:\n",
    "                    pair_class = class_size_groups[pair]['class']\n",
    "                    pair_size = class_size_groups[pair]['size']\n",
    "                    \n",
    "                    # Try to get diverse classes and sizes\n",
    "                    if len(selected_pairs) < 4:\n",
    "                        if pair_class not in used_classes or pair_size not in used_sizes or len(selected_pairs) < 4:\n",
    "                            selected_pairs.append(pair)\n",
    "                            used_classes.add(pair_class)\n",
    "                            used_sizes.add(pair_size)\n",
    "                \n",
    "                if len(selected_pairs) < 4:\n",
    "                    # If we can't get 4 diverse pairs, just take any 4\n",
    "                    selected_pairs = random.sample(available_class_sizes, 4)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # First pair is the query\n",
    "            query_pair = selected_pairs[0]\n",
    "            query_data = class_size_groups[query_pair]\n",
    "            \n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            query_class = query_data['class']\n",
    "            query_size = query_data['size']\n",
    "            \n",
    "            # Shuffle for candidate order\n",
    "            random.shuffle(selected_pairs)\n",
    "            correct_idx = selected_pairs.index(query_pair)\n",
    "            \n",
    "            # Create text prompts - ONLY size + class, no color/texture\n",
    "            candidate_texts = []\n",
    "            for pair in selected_pairs:\n",
    "                pair_data = class_size_groups[pair]\n",
    "                text_prompt = f\"{pair_data['size']} {pair_data['class'].lower()}\"\n",
    "                candidate_texts.append(text_prompt)\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': query_class,\n",
    "                'query_size': query_size,\n",
    "                'color_texture': ct,\n",
    "                'query_img': os.path.basename(query_img_path),\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct,\n",
    "                'candidate_texts': candidate_texts,\n",
    "                'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - DCDS Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'DCDS-TextVision-Controlled',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL DCDS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDS Text-Vision Test with cvcl-resnext\n",
      "(Different Class Different Size - Controlled Color/Texture)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 20 color-texture combinations with sufficient diversity\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 491/491 [00:21<00:00, 23.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7823 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 20 color-texture combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 20/20 [00:36<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - DCDS Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1161\n",
      "Accuracy: 0.2903 (29.03%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_dcds_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP DCDS Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDS Text-Vision Test with clip-resnext\n",
      "(Different Class Different Size - Controlled Color/Texture)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Found 20 color-texture combinations with sufficient diversity\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/491 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 491/491 [00:17<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7823 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 20 color-texture combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 20/20 [00:18<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - DCDS Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3210\n",
      "Accuracy: 0.8025 (80.25%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_dcds_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DCDS TEXT-VISION TEST COMPARISON (CONTROLLED)\n",
      "============================================================\n",
      "\n",
      "Test: Different Class Different Size (4-way forced choice)\n",
      "Control: Color and Texture held constant (not mentioned in text)\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2903 (29.03%)\n",
      "  CLIP Accuracy: 0.8025 (80.25%)\n",
      "\n",
      "Difference: 0.5122 (51.23%)\n",
      "CLIP performs better by 51.23%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- Tests size discrimination when class also varies\n",
      "- Color and texture are controlled to isolate size+class discrimination\n",
      "- Should be easier than SCDS since both size AND class provide cues\n",
      "- Performance shows how well models combine size and class understanding\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DCDS TEXT-VISION TEST COMPARISON (CONTROLLED)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Different Class Different Size (4-way forced choice)\")\n",
    "print(f\"Control: Color and Texture held constant (not mentioned in text)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Tests size discrimination when class also varies\")\n",
    "print(\"- Color and texture are controlled to isolate size+class discrimination\")\n",
    "print(\"- Should be easier than SCDS since both size AND class provide cues\")\n",
    "print(\"- Performance shows how well models combine size and class understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### DCDS Text-Vision Test Characteristics (Controlled Version):\n",
    "- **Visual Control**: All 4 candidates have same color and texture (when possible)\n",
    "- **Variation**: Both class AND size differ between candidates\n",
    "- **Text Prompts**: Only mention size + class (e.g., \"large apple\", \"small car\")\n",
    "- **NOT mentioned**: Color and texture are controlled but excluded from text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
