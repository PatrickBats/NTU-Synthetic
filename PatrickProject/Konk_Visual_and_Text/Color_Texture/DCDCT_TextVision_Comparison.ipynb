{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Class Different Color and Texture (DCDCT) Text-Vision Comparison - SyntheticKonkle\n",
    "\n",
    "This notebook compares CVCL and CLIP models on text-vision matching using the SyntheticKonkle dataset.\n",
    "The task is 4-way classification where distractors are from DIFFERENT classes with DIFFERENT colors AND textures.\n",
    "\n",
    "## Text Ordering Considerations\n",
    "\n",
    "**We use natural English adjective ordering**: `\"{color} {texture} {class}\"`\n",
    "- Example: \"red smooth apple\", \"blue bumpy car\", \"green smooth ball\"\n",
    "- This follows standard English grammar rules where color comes before texture\n",
    "- Both CVCL (trained on child-directed speech) and CLIP (trained on internet text) expect this natural ordering\n",
    "- Parents typically say \"red bumpy ball\" not \"bumpy red ball\"\n",
    "\n",
    "## Test Characteristics\n",
    "- **Visual**: All candidates from DIFFERENT classes, with controlled size\n",
    "- **Variation**: Class, color, AND texture all differ between candidates\n",
    "- **Control**: Size held constant visually but NOT mentioned in text\n",
    "- **Difficulty**: Easier - multiple discriminative cues from class, color, and texture differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "Metadata path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle\\master_labels.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup - Use absolute paths to avoid any confusion\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7865 images with size, color, and texture annotations\n",
      "Unique classes: 67\n",
      "Unique colors: 11\n",
      "Unique textures: ['bumpy', 'smooth']\n",
      "Unique sizes: ['large', 'medium', 'small']\n",
      "\n",
      "Total unique class-color-texture combinations: 1342\n",
      "Sample combinations: ['abacus_red_bumpy' 'abacus_green_bumpy' 'abacus_blue_bumpy'\n",
      " 'abacus_yellow_bumpy' 'abacus_orange_bumpy']\n",
      "\n",
      "Sample data:\n",
      "    class   color texture   size  class_color_texture\n",
      "0  abacus     red   bumpy  large     abacus_red_bumpy\n",
      "1  abacus   green   bumpy  large   abacus_green_bumpy\n",
      "2  abacus    blue   bumpy  large    abacus_blue_bumpy\n",
      "3  abacus  yellow   bumpy  large  abacus_yellow_bumpy\n",
      "4  abacus  orange   bumpy  large  abacus_orange_bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "def load_synthetic_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset with metadata for DCDCT testing\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid size, color, and texture information\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values (lowercase)\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid size and texture values\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['size'].isin(valid_sizes) & df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination columns\n",
    "    df['class_color_texture'] = df['class'] + '_' + df['color'] + '_' + df['texture']\n",
    "    df['full_combo'] = df['class'] + '_' + df['color'] + '_' + df['size'] + '_' + df['texture']\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images with size, color, and texture annotations\")\n",
    "    print(f\"Unique classes: {df['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique textures: {sorted(df['texture'].unique())}\")\n",
    "    print(f\"Unique sizes: {sorted(df['size'].unique())}\")\n",
    "    \n",
    "    # Show unique class-color-texture combinations\n",
    "    unique_cct = df['class_color_texture'].nunique()\n",
    "    print(f\"\\nTotal unique class-color-texture combinations: {unique_cct}\")\n",
    "    print(f\"Sample combinations: {df['class_color_texture'].unique()[:5]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data_df = load_synthetic_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'texture', 'size', 'class_color_texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dcdct_text_vision_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run Different Class Different Color and Texture text-vision test with controlled size\n",
    "    \n",
    "    Text format uses natural English ordering: \"{color} {texture} {class}\"\n",
    "    Example: \"red smooth apple\", \"blue bumpy car\", \"green smooth ball\"\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test ('cvcl-resnext' or 'clip-resnext')\n",
    "        seed: Random seed for reproducibility\n",
    "        device: Device to use (None for auto-detect)\n",
    "        num_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    # Set seeds to match original test methodology\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running DCDCT Text-Vision Test with {model_name}\")\n",
    "    print(f\"(Different Class Different Color & Texture - Controlled Size)\")\n",
    "    print(f\"Text format: {{color}} {{texture}} {{class}} (natural English order)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"[ERROR] CUDA requested but not available! Falling back to CPU.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to entries with all annotations and valid values\n",
    "    df = df[\n",
    "        df['size'].notna() & (df['size'] != '') &\n",
    "        df['color'].notna() & (df['color'] != '') &\n",
    "        df['texture'].notna() & (df['texture'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Standardize values\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    # Filter to only valid size and texture values\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['size'].isin(valid_sizes) & df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create unique identifier for each combination\n",
    "    df['class_color_texture_size'] = df['class'] + '_' + df['color'] + '_' + df['texture'] + '_' + df['size']\n",
    "    \n",
    "    # Group all data by unique combinations\n",
    "    combo_groups = df.groupby('class_color_texture_size').agg({\n",
    "        'image_path': list,\n",
    "        'class': 'first',\n",
    "        'color': 'first',\n",
    "        'texture': 'first',\n",
    "        'size': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"\\nTotal unique combinations: {len(combo_groups)}\")\n",
    "    print(f\"Unique classes: {combo_groups['class'].nunique()}\")\n",
    "    print(f\"Unique colors: {combo_groups['color'].nunique()}\")\n",
    "    print(f\"Unique textures: {combo_groups['texture'].nunique()} - {sorted(combo_groups['texture'].unique())}\")\n",
    "    print(f\"Unique sizes: {combo_groups['size'].nunique()} - {sorted(combo_groups['size'].unique())}\")\n",
    "    \n",
    "    # Pre-compute image embeddings for efficiency\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    all_image_paths = df['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for trial_num in tqdm(range(num_trials), desc=\"Running trials\"):\n",
    "        # For DCDCT: Select 4 combinations with DIFFERENT classes\n",
    "        # Try to control size when possible\n",
    "        \n",
    "        # First, randomly select a query combination\n",
    "        query_idx = random.randint(0, len(combo_groups) - 1)\n",
    "        query_combo = combo_groups.iloc[query_idx]\n",
    "        query_class = query_combo['class']\n",
    "        query_size = query_combo['size']\n",
    "        \n",
    "        # Find combinations with same size but different classes\n",
    "        same_size_diff_class = combo_groups[\n",
    "            (combo_groups['size'] == query_size) & \n",
    "            (combo_groups['class'] != query_class)\n",
    "        ]\n",
    "        \n",
    "        # Also get combinations with different classes (fallback if not enough same size)\n",
    "        diff_class_combos = combo_groups[combo_groups['class'] != query_class]\n",
    "        \n",
    "        # Build candidate list\n",
    "        candidate_combos = [query_combo]\n",
    "        \n",
    "        # Try to get 3 distractors with same size but different classes\n",
    "        if len(same_size_diff_class) >= 3:\n",
    "            # Prefer diverse classes, colors, and textures\n",
    "            distractors = same_size_diff_class.sample(min(3, len(same_size_diff_class)))\n",
    "        else:\n",
    "            # If not enough same-size options, use any different class\n",
    "            distractors = diff_class_combos.sample(3)\n",
    "        \n",
    "        for _, distractor in distractors.iterrows():\n",
    "            candidate_combos.append(distractor)\n",
    "        \n",
    "        # Ensure we have exactly 4 candidates\n",
    "        if len(candidate_combos) != 4:\n",
    "            continue\n",
    "            \n",
    "        # Select random query image from valid images\n",
    "        valid_query_paths = [p for p in query_combo['image_path'] if p in image_embeddings]\n",
    "        if not valid_query_paths:\n",
    "            continue\n",
    "        query_img_path = random.choice(valid_query_paths)\n",
    "        \n",
    "        # Shuffle candidates for random order (keeping track of correct index)\n",
    "        shuffled_order = list(range(4))\n",
    "        random.shuffle(shuffled_order)\n",
    "        shuffled_candidates = [candidate_combos[i] for i in shuffled_order]\n",
    "        correct_idx = shuffled_order.index(0)  # Find where the query (index 0) ended up\n",
    "        \n",
    "        # Create text prompts - NATURAL ENGLISH ORDER: {color} {texture} {class}\n",
    "        candidate_texts = []\n",
    "        for candidate in shuffled_candidates:\n",
    "            # Natural English order: color before texture before noun\n",
    "            text_prompt = f\"{candidate['color']} {candidate['texture']} {candidate['class'].lower()}\"\n",
    "            candidate_texts.append(text_prompt)\n",
    "        \n",
    "        # Encode text prompts\n",
    "        with torch.no_grad():\n",
    "            if \"clip\" in model_name:\n",
    "                tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                txt_features = model.encode_text(tokens)\n",
    "                txt_features = extractor.norm_features(txt_features)\n",
    "            else:  # CVCL\n",
    "                tokens, token_len = model.tokenize(candidate_texts)\n",
    "                tokens = tokens.to(device)\n",
    "                if isinstance(token_len, torch.Tensor):\n",
    "                    token_len = token_len.to(device)\n",
    "                txt_features = model.encode_text(tokens, token_len)\n",
    "                txt_features = extractor.norm_features(txt_features)\n",
    "        \n",
    "        # Get query image embedding\n",
    "        query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        query_embedding = query_embedding.float()\n",
    "        txt_features = txt_features.float()\n",
    "        \n",
    "        similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_idx = similarity.argmax(dim=1).item()\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (pred_idx == correct_idx)\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        # Store trial result\n",
    "        trial_results.append({\n",
    "            'trial': trial_num + 1,\n",
    "            'query_class': query_combo['class'],\n",
    "            'query_color': query_combo['color'],\n",
    "            'query_texture': query_combo['texture'],\n",
    "            'query_size': query_combo['size'],\n",
    "            'query_img': os.path.basename(query_img_path),\n",
    "            'correct_idx': correct_idx,\n",
    "            'predicted_idx': pred_idx,\n",
    "            'correct': is_correct,\n",
    "            'candidate_texts': candidate_texts,\n",
    "            'similarity_scores': similarity.cpu().numpy().tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - DCDCT Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'DCDCT-TextVision',\n",
    "        'Dataset': 'SyntheticKonkle',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return trial_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL DCDCT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDCT Text-Vision Test with cvcl-resnext\n",
      "(Different Class Different Color & Texture - Controlled Size)\n",
      "Text format: {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique combinations: 3952\n",
      "Unique classes: 67\n",
      "Unique colors: 11\n",
      "Unique textures: 2 - ['bumpy', 'smooth']\n",
      "Unique sizes: 3 - ['large', 'medium', 'small']\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 492/492 [00:21<00:00, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7835 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running trials: 100%|██████████| 4000/4000 [00:49<00:00, 80.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - DCDCT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1053\n",
      "Accuracy: 0.2632 (26.32%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test with seed=0 (matching original tests)\n",
    "cvcl_trials, cvcl_accuracy = run_dcdct_text_vision_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP DCDCT Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running DCDCT Text-Vision Test with clip-resnext\n",
      "(Different Class Different Color & Texture - Controlled Size)\n",
      "Text format: {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "\n",
      "Total unique combinations: 3952\n",
      "Unique classes: 67\n",
      "Unique colors: 11\n",
      "Unique textures: 2 - ['bumpy', 'smooth']\n",
      "Unique sizes: 3 - ['large', 'medium', 'small']\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/492 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 492/492 [00:18<00:00, 27.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 7835 images\n",
      "Skipped 22 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running trials: 100%|██████████| 4000/4000 [00:26<00:00, 151.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - DCDCT Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3848\n",
      "Accuracy: 0.9620 (96.20%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test with seed=0 (matching original tests)\n",
    "clip_trials, clip_accuracy = run_dcdct_text_vision_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DCDCT TEXT-VISION TEST COMPARISON\n",
      "============================================================\n",
      "\n",
      "Test: Different Class Different Color & Texture (4-way forced choice)\n",
      "Control: Size held constant (not mentioned in text)\n",
      "Text format: '{color} {texture} {class}' (natural English order)\n",
      "Example: 'red smooth apple' vs 'blue bumpy car' vs 'green smooth ball'\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2632 (26.32%)\n",
      "  CLIP Accuracy: 0.9620 (96.20%)\n",
      "\n",
      "Difference: 0.6987 (69.88%)\n",
      "CLIP performs better by 69.88%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analysis:\n",
      "- Multiple cues: class, color, AND texture all differ\n",
      "- Maximum discriminative power from three varying attributes\n",
      "- Natural English ordering (color before texture) helps both models\n",
      "- Should perform well due to multiple discriminative cues\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DCDCT TEXT-VISION TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest: Different Class Different Color & Texture (4-way forced choice)\")\n",
    "print(f\"Control: Size held constant (not mentioned in text)\")\n",
    "print(f\"Text format: '{{color}} {{texture}} {{class}}' (natural English order)\")\n",
    "print(f\"Example: 'red smooth apple' vs 'blue bumpy car' vs 'green smooth ball'\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}%\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Multiple cues: class, color, AND texture all differ\")\n",
    "print(\"- Maximum discriminative power from three varying attributes\")\n",
    "print(\"- Natural English ordering (color before texture) helps both models\")\n",
    "print(\"- Should perform well due to multiple discriminative cues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "### DCDCT Text-Vision Test Characteristics:\n",
    "- **Visual Control**: All 4 candidates have same size only\n",
    "- **Variation**: Class, color, AND texture all differ between candidates\n",
    "- **Text Prompts**: Natural order \"{color} {texture} {class}\" (e.g., \"red smooth apple\")\n",
    "- **NOT mentioned**: Size is controlled but excluded from text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
