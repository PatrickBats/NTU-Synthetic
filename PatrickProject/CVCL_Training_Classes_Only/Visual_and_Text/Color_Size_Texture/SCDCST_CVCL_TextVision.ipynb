{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDCST Text-Vision Test - CVCL Training Classes Only\\n\\nSame Class Different Color, Size and Texture\\n\\n**This version only tests on the 25 classes that appear in CVCL's training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\SyntheticKonkle_224\\SyntheticKonkle\n",
      "CVCL classes file: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\data\\CVCL_Konkle_Overlap\\CVCLKonkMatches.csv\n",
      "Results will be saved to: C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import clip\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path setup\n",
    "REPO_ROOT = r'C:\\Users\\jbats\\Projects\\NTU-Synthetic'\n",
    "\n",
    "# Add discover-hidden-visual-concepts to path\n",
    "DISCOVER_ROOT = os.path.join(REPO_ROOT, 'discover-hidden-visual-concepts')\n",
    "sys.path.insert(0, DISCOVER_ROOT)\n",
    "sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import from discover-hidden-visual-concepts repo\n",
    "sys.path.append(os.path.join(DISCOVER_ROOT, 'src'))\n",
    "from utils.model_loader import load_model\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Paths\n",
    "CVCL_CLASSES_PATH = os.path.join(REPO_ROOT, 'data', 'CVCL_Konkle_Overlap', 'CVCLKonkMatches.csv')\n",
    "DATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle_224', 'SyntheticKonkle')\n",
    "METADATA_PATH = os.path.join(REPO_ROOT, 'data', 'SyntheticKonkle', 'master_labels.csv')\n",
    "RESULTS_PATH = os.path.join(REPO_ROOT, 'PatrickProject', 'Chart_Generation', 'cvcl_training_text_vision_results.csv')\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"CVCL classes file: {CVCL_CLASSES_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVCL Training Classes (24):\n",
      "  ball\n",
      "  butterfly\n",
      "  phone\n",
      "  bagel\n",
      "  basket\n",
      "  bell\n",
      "  fan\n",
      "  seashell\n",
      "  bird\n",
      "  stool\n",
      "  train\n",
      "  ring\n",
      "  tricycle\n",
      "  toothpaste\n",
      "  pen\n",
      "  tree\n",
      "  apple\n",
      "  cookie\n",
      "  bread\n",
      "  pumpkin\n",
      "  camera\n",
      "  rabbit\n",
      "  pillow\n",
      "  horse\n"
     ]
    }
   ],
   "source": [
    "# Load CVCL training classes\n",
    "cvcl_df = pd.read_csv(CVCL_CLASSES_PATH)\n",
    "CVCL_TRAINING_CLASSES = cvcl_df['Class'].str.strip().tolist()\n",
    "\n",
    "print(f\"CVCL Training Classes ({len(CVCL_TRAINING_CLASSES)}):\")\n",
    "for cls in CVCL_TRAINING_CLASSES:\n",
    "    print(f\"  {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "\n",
      "Sample data:\n",
      "   class   color   size texture\n",
      "0  apple     red  large   bumpy\n",
      "1  apple   green  large   bumpy\n",
      "2  apple    blue  large   bumpy\n",
      "3  apple  yellow  large   bumpy\n",
      "4  apple  orange  large   bumpy\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data - FILTERED TO CVCL TRAINING CLASSES\n",
    "def load_cvcl_synthetickonkle_data():\n",
    "    \"\"\"Load SyntheticKonkle dataset filtered to CVCL training classes\"\"\"\n",
    "    # Read metadata\n",
    "    df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Filter to only CVCL training classes\n",
    "    df = df[df['class'].isin(CVCL_TRAINING_CLASSES)].copy()\n",
    "    \n",
    "    # Handle missing ball and bread\n",
    "    missing_classes = set(CVCL_TRAINING_CLASSES) - set(df['class'].unique())\n",
    "    if missing_classes:\n",
    "        print(f\"Adding missing classes from folders: {missing_classes}\")\n",
    "        for cls in missing_classes:\n",
    "            folder = f\"{cls}_color\"\n",
    "            folder_path = os.path.join(DATA_PATH, folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "                for img_file in image_files:\n",
    "                    # Parse filename to extract metadata\n",
    "                    parts = img_file.replace('.png', '').split('_')\n",
    "                    if len(parts) >= 5:\n",
    "                        new_row = {\n",
    "                            'folder': folder,\n",
    "                            'filename': img_file,\n",
    "                            'class': cls,\n",
    "                            'color': '_'.join(parts[4:]),\n",
    "                            'size': parts[1],\n",
    "                            'texture': parts[2],\n",
    "                            'variant': parts[3]\n",
    "                        }\n",
    "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Build full paths\n",
    "    df['image_path'] = df.apply(lambda row: os.path.join(DATA_PATH, row['folder'], row['filename']), axis=1)\n",
    "    \n",
    "    # Filter to only entries with valid metadata\n",
    "    df = df[df['color'].notna() & (df['color'] != '')].copy()\n",
    "    df = df[df['size'].notna() & (df['size'] != '')].copy()\n",
    "    df = df[df['texture'].notna() & (df['texture'] != '')].copy()\n",
    "    \n",
    "    # Standardize names (lowercase)\n",
    "    df['color'] = df['color'].str.lower().str.strip()\n",
    "    df['size'] = df['size'].str.lower().str.strip()\n",
    "    df['texture'] = df['texture'].str.lower().str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} images from {df['class'].nunique()} CVCL training classes\")\n",
    "    print(f\"Classes: {sorted(df['class'].unique())}\")\n",
    "    print(f\"Unique colors: {df['color'].nunique()}\")\n",
    "    print(f\"Unique sizes: {df['size'].nunique()}\")\n",
    "    print(f\"Unique textures: {df['texture'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data_df = load_cvcl_synthetickonkle_data()\n",
    "print(\"\\nSample data:\")\n",
    "print(data_df[['class', 'color', 'size', 'texture']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scdcst_test(model_name='cvcl-resnext', seed=0, device=None, num_trials=4000):\n",
    "    \"\"\"Run SCDCST text-vision test on CVCL training classes only\n",
    "    Same Class Different Color, Size and Texture test.\n",
    "    Text format uses natural English ordering: \"{size} {color} {texture} {class}\"\n",
    "    Example: \"large red smooth apple\", \"small blue bumpy apple\", \"medium green smooth apple\"\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SCDCST Text-Vision Test with {model_name}\")\n",
    "    print(f\"CVCL Training Classes Only\")\n",
    "    print(f\"(Same Class Different Color, Size & Texture)\")\n",
    "    print(f\"Text format: {{size}} {{color}} {{texture}} {{class}} (natural English order)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[INFO] Loading {model_name} on {device}...\")\n",
    "    model, transform = load_model(model_name, seed=seed, device=device)\n",
    "    extractor = FeatureExtractor(model_name, model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load filtered data - CVCL training classes only\n",
    "    df = load_cvcl_synthetickonkle_data()\n",
    "    \n",
    "    # Filter to only valid size and texture values\n",
    "    valid_sizes = ['small', 'medium', 'large']\n",
    "    valid_textures = ['smooth', 'bumpy']\n",
    "    df = df[df['size'].isin(valid_sizes) & df['texture'].isin(valid_textures)].copy()\n",
    "    \n",
    "    # Create combination column\n",
    "    df['color_size_texture'] = df['color'] + '_' + df['size'] + '_' + df['texture']\n",
    "    \n",
    "    # Find classes with at least 3 different color-size-texture combinations\n",
    "    class_groups = df.groupby('class')\n",
    "    valid_classes = []\n",
    "    for class_name, group in class_groups:\n",
    "        unique_cst = group['color_size_texture'].unique()\n",
    "        if len(unique_cst) >= 3:\n",
    "            valid_classes.append(class_name)\n",
    "    \n",
    "    if len(valid_classes) == 0:\n",
    "        print(\"ERROR: No CVCL training classes have 3+ different color-size-texture combinations.\")\n",
    "        return 0.0\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_classes)} CVCL training classes with 3+ color-size-texture combinations\")\n",
    "    \n",
    "    # Pre-compute image embeddings\n",
    "    print(\"\\nExtracting image embeddings...\")\n",
    "    image_embeddings = {}\n",
    "    skipped_images = []\n",
    "    \n",
    "    # Get all relevant images\n",
    "    df_valid = df[df['class'].isin(valid_classes)]\n",
    "    all_image_paths = df_valid['image_path'].unique().tolist()\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_image_paths), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_paths = all_image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_processed = transform(img).unsqueeze(0).to(device)\n",
    "                batch_images.append((img_path, img_processed))\n",
    "            except Exception as e:\n",
    "                skipped_images.append(img_path)\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            paths = [p for p, _ in batch_images]\n",
    "            imgs = torch.cat([img for _, img in batch_images], dim=0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = extractor.get_img_feature(imgs)\n",
    "                embeddings = extractor.norm_features(embeddings)\n",
    "            \n",
    "            for path, emb in zip(paths, embeddings):\n",
    "                image_embeddings[path] = emb.cpu().float()\n",
    "    \n",
    "    print(f\"Extracted embeddings for {len(image_embeddings)} images\")\n",
    "    if skipped_images:\n",
    "        print(f\"Skipped {len(skipped_images)} corrupted/invalid images\")\n",
    "    \n",
    "    # Prepare for trials\n",
    "    correct_count = 0\n",
    "    trial_results = []\n",
    "    \n",
    "    # Calculate trials per class\n",
    "    trials_per_class = num_trials // len(valid_classes) if valid_classes else 0\n",
    "    remaining_trials = num_trials % len(valid_classes) if valid_classes else 0\n",
    "    \n",
    "    print(f\"\\nRunning {num_trials} trials across {len(valid_classes)} classes...\")\n",
    "    \n",
    "    # Run trials\n",
    "    for class_idx, class_name in enumerate(tqdm(valid_classes, desc=\"Processing classes\")):\n",
    "        # Get all images for this class\n",
    "        class_data = df_valid[df_valid['class'] == class_name]\n",
    "        \n",
    "        # Group by color-size-texture\n",
    "        cst_groups = class_data.groupby('color_size_texture').agg({\n",
    "            'image_path': list,\n",
    "            'color': 'first',\n",
    "            'size': 'first',\n",
    "            'texture': 'first'\n",
    "        }).to_dict('index')\n",
    "        \n",
    "        available_cst = list(cst_groups.keys())\n",
    "        \n",
    "        if len(available_cst) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Determine number of trials for this class\n",
    "        n_trials = trials_per_class + (1 if class_idx < remaining_trials else 0)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            if len(trial_results) >= num_trials:\n",
    "                break\n",
    "                \n",
    "            # Select color-size-texture combinations for 4-way choice\n",
    "            if len(available_cst) == 3:\n",
    "                # Use all 3 plus duplicate one\n",
    "                selected_cst = available_cst.copy()\n",
    "                selected_cst.append(random.choice(available_cst))\n",
    "            else:\n",
    "                # Select 4 different combinations if possible\n",
    "                selected_cst = random.sample(available_cst, min(4, len(available_cst)))\n",
    "            \n",
    "            # First combination is the query\n",
    "            query_cst = selected_cst[0]\n",
    "            query_data = cst_groups[query_cst]\n",
    "            \n",
    "            # Select random query image from valid images\n",
    "            valid_query_paths = [p for p in query_data['image_path'] if p in image_embeddings]\n",
    "            if not valid_query_paths:\n",
    "                continue\n",
    "            query_img_path = random.choice(valid_query_paths)\n",
    "            query_color = query_data['color']\n",
    "            query_size = query_data['size']\n",
    "            query_texture = query_data['texture']\n",
    "            \n",
    "            # Shuffle for candidate order\n",
    "            random.shuffle(selected_cst)\n",
    "            correct_idx = selected_cst.index(query_cst)\n",
    "            \n",
    "            # Create text prompts - NATURAL ENGLISH ORDER: {size} {color} {texture} {class}\n",
    "            candidate_texts = []\n",
    "            for cst in selected_cst:\n",
    "                cst_data = cst_groups[cst]\n",
    "                # Natural English order: size → color → texture → noun\n",
    "                text_prompt = f\"{cst_data['size']} {cst_data['color']} {cst_data['texture']} {class_name.lower()}\"\n",
    "                candidate_texts.append(text_prompt)\n",
    "            \n",
    "            # Encode text prompts\n",
    "            with torch.no_grad():\n",
    "                if \"clip\" in model_name:\n",
    "                    tokens = clip.tokenize(candidate_texts, truncate=True).to(device)\n",
    "                    txt_features = model.encode_text(tokens)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "                else:  # CVCL\n",
    "                    tokens, token_len = model.tokenize(candidate_texts)\n",
    "                    tokens = tokens.to(device)\n",
    "                    if isinstance(token_len, torch.Tensor):\n",
    "                        token_len = token_len.to(device)\n",
    "                    txt_features = model.encode_text(tokens, token_len)\n",
    "                    txt_features = extractor.norm_features(txt_features)\n",
    "            \n",
    "            # Get query image embedding\n",
    "            query_embedding = image_embeddings[query_img_path].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            query_embedding = query_embedding.float()\n",
    "            txt_features = txt_features.float()\n",
    "            \n",
    "            similarity = (100.0 * query_embedding @ txt_features.transpose(-2, -1)).softmax(dim=1)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = similarity.argmax(dim=1).item()\n",
    "            \n",
    "            # Check if correct\n",
    "            is_correct = (pred_idx == correct_idx)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Store trial result\n",
    "            trial_results.append({\n",
    "                'trial': len(trial_results) + 1,\n",
    "                'query_class': class_name,\n",
    "                'query_color': query_color,\n",
    "                'query_size': query_size,\n",
    "                'query_texture': query_texture,\n",
    "                'correct_idx': correct_idx,\n",
    "                'predicted_idx': pred_idx,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_count / len(trial_results) if trial_results else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {model_name} - SCDCST Text-Vision Test:\")\n",
    "    print(f\"Total trials: {len(trial_results)}\")\n",
    "    print(f\"Correct: {correct_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_row = {\n",
    "        'Model': model_name,\n",
    "        'Test': 'SCDCST-TextVision-CVCLTraining',\n",
    "        'Dataset': 'SyntheticKonkle_224',\n",
    "        'Correct': correct_count,\n",
    "        'Trials': len(trial_results),\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    # Append to results file\n",
    "    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        results_df = pd.read_csv(RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([results_row])], ignore_index=True)\n",
    "    results_df.to_csv(RESULTS_PATH, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CVCL SCDCST Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCST Text-Vision Test with cvcl-resnext\n",
      "CVCL Training Classes Only\n",
      "(Same Class Different Color, Size & Texture)\n",
      "Text format: {size} {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading cvcl-resnext on cuda...\n",
      "Loading checkpoint from C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbats\\.cache\\huggingface\\hub\\models--wkvong--cvcl_s_dino_resnext50_embedding\\snapshots\\f50eaa0c50a6076a5190b1dd52aeeb6c3e747045\\cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "\n",
      "Found 24 CVCL training classes with 3+ color-size-texture combinations\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 176/176 [00:08<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 2800 images\n",
      "Skipped 14 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 24 classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 24/24 [00:36<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for cvcl-resnext - SCDCST Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 1162\n",
      "Accuracy: 0.2905 (29.05%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CVCL test\n",
    "cvcl_accuracy = run_scdcst_test('cvcl-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CLIP SCDCST Text-Vision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running SCDCST Text-Vision Test with clip-resnext\n",
      "CVCL Training Classes Only\n",
      "(Same Class Different Color, Size & Texture)\n",
      "Text format: {size} {color} {texture} {class} (natural English order)\n",
      "============================================================\n",
      "Using device: cuda\n",
      "[INFO] Loading clip-resnext on cuda...\n",
      "Adding missing classes from folders: {'ball'}\n",
      "Loaded 2832 images from 24 CVCL training classes\n",
      "Classes: ['apple', 'bagel', 'ball', 'basket', 'bell', 'bird', 'bread', 'butterfly', 'camera', 'cookie', 'fan', 'horse', 'pen', 'phone', 'pillow', 'pumpkin', 'rabbit', 'ring', 'seashell', 'stool', 'toothpaste', 'train', 'tree', 'tricycle']\n",
      "Unique colors: 12\n",
      "Unique sizes: 4\n",
      "Unique textures: 4\n",
      "\n",
      "Found 24 CVCL training classes with 3+ color-size-texture combinations\n",
      "\n",
      "Extracting image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/176 [00:00<?, ?it/s]c:\\Users\\jbats\\miniconda3\\envs\\ntu-synthetic\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Extracting embeddings: 100%|██████████| 176/176 [00:06<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 2800 images\n",
      "Skipped 14 corrupted/invalid images\n",
      "\n",
      "Running 4000 trials across 24 classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 24/24 [00:19<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results for clip-resnext - SCDCST Text-Vision Test:\n",
      "Total trials: 4000\n",
      "Correct: 3599\n",
      "Accuracy: 0.8998 (89.98%)\n",
      "============================================================\n",
      "\n",
      "Results saved to C:\\Users\\jbats\\Projects\\NTU-Synthetic\\PatrickProject\\Chart_Generation\\cvcl_training_text_vision_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run CLIP test\n",
    "clip_accuracy = run_scdcst_test('clip-resnext', seed=0, num_trials=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCDCST TEXT-VISION TEST COMPARISON - CVCL TRAINING CLASSES\n",
      "============================================================\n",
      "\n",
      "Results:\n",
      "  CVCL Accuracy: 0.2905 (29.05%)\n",
      "  CLIP Accuracy: 0.8998 (89.98%)\n",
      "\n",
      "Difference: 0.6093 (60.93%)\n",
      "CLIP performs better by 60.93% even on CVCL's training classes\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCDCST TEXT-VISION TEST COMPARISON - CVCL TRAINING CLASSES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  CVCL Accuracy: {cvcl_accuracy:.4f} ({cvcl_accuracy*100:.2f}%)\")\n",
    "print(f\"  CLIP Accuracy: {clip_accuracy:.4f} ({clip_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(cvcl_accuracy - clip_accuracy):.4f} ({abs(cvcl_accuracy - clip_accuracy)*100:.2f}%)\")\n",
    "if cvcl_accuracy > clip_accuracy:\n",
    "    print(f\"CVCL performs better by {(cvcl_accuracy - clip_accuracy)*100:.2f}% on its training classes\")\n",
    "elif clip_accuracy > cvcl_accuracy:\n",
    "    print(f\"CLIP performs better by {(clip_accuracy - cvcl_accuracy)*100:.2f}% even on CVCL's training classes\")\n",
    "else:\n",
    "    print(\"Both models perform equally\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
